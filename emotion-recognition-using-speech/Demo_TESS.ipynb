{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0815 14:50:51.188915 140736091374464 deprecation_wrapper.py:119] From /Users/m.elabd/Desktop/loudness/Emote-X/emotion-recognition-using-speech/deep_emotion_recognition.py:15: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import scipy\n",
    "# from scipy.optimize import curve_fit\n",
    "# from scipy.io import wavfile\n",
    "# from scipy.signal import butter, sosfiltfilt\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from scipy import signal\n",
    "# import osascript\n",
    "# from gtts import gTTS \n",
    "# from matplotlib import pylab\n",
    "# import os \n",
    "# import pyaudio\n",
    "# import wave\n",
    "# import keyboard  # using module keyboard\n",
    "# import soundfile as sf\n",
    "# import math\n",
    "# import pyloudnorm as pyln\n",
    "# from sys import byteorder\n",
    "# from array import array\n",
    "# from struct import pack\n",
    "# import import_ipynb\n",
    "# import concat_project7\n",
    "# from joblib import dump, load\n",
    "from emotion_recognition import EmotionRecognizer\n",
    "from sklearn.svm import SVC\n",
    "from deep_emotion_recognition import DeepEmotionRecognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THRESHOLD = 100\n",
    "# CHUNK_SIZE = 2048\n",
    "# FORMAT = pyaudio.paInt16\n",
    "# RATE = 16000\n",
    "# AVG_STEP = 75\n",
    "# PERCENTILE = 70\n",
    "# BANDPASS_FREQ = [300, 3400]\n",
    "# CALIBRATION_VOLUME = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record from microphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record():\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=FORMAT, channels=1, rate=RATE,\n",
    "        input=True, output=True,\n",
    "        frames_per_buffer=CHUNK_SIZE)\n",
    "\n",
    "    num_silent = 0\n",
    "    snd_started = False\n",
    "\n",
    "    r = array('h')\n",
    "\n",
    "    while 1:\n",
    "        # little endian, signed short\n",
    "        snd_data = array('h', stream.read(CHUNK_SIZE))\n",
    "        if byteorder == 'big':\n",
    "            snd_data.byteswap()\n",
    "        r.extend(snd_data)\n",
    "\n",
    "        silent = is_silent(snd_data)\n",
    "\n",
    "        if silent and snd_started:\n",
    "            num_silent += 1\n",
    "        elif not silent and not snd_started:\n",
    "            snd_started = True\n",
    "\n",
    "        if snd_started and num_silent > 60:\n",
    "            break\n",
    "\n",
    "    sample_width = p.get_sample_size(FORMAT)\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "#     r = trim(r)\n",
    "#     r = add_silence(r, 0.5)\n",
    "    return sample_width, r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to wav file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_to_file(path):\n",
    "    sample_width, data = record()\n",
    "    data = pack('<' + ('h'*len(data)), *data)\n",
    "\n",
    "    wf = wave.open(path, 'wb')\n",
    "    wf.setnchannels(1)\n",
    "    wf.setsampwidth(sample_width)\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(data)\n",
    "    wf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks if input is silent based on threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_silent(snd_data):\n",
    "    \"Returns 'True' if below the 'silent' threshold\"\n",
    "    return max(snd_data) < THRESHOLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trim input to remove silence at beginning and end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(snd_data):\n",
    "    def _trim(snd_data):\n",
    "        snd_started = False\n",
    "        r = array('h')\n",
    "\n",
    "        for i in snd_data:\n",
    "            if not snd_started and abs(i)>THRESHOLD:\n",
    "                snd_started = True\n",
    "                r.append(i)\n",
    "            elif snd_started:\n",
    "                r.append(i)\n",
    "        return r\n",
    "\n",
    "    # Trim to the left\n",
    "    snd_data = _trim(snd_data)\n",
    "\n",
    "    # Trim to the right\n",
    "    snd_data.reverse()\n",
    "    snd_data = _trim(snd_data)\n",
    "    snd_data.reverse()\n",
    "    return snd_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pad with silence at beginning and end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_silence(snd_data, seconds):\n",
    "    \"\"\n",
    "    r = array('h', [0 for i in range(int(seconds*RATE))])\n",
    "    r.extend(snd_data)\n",
    "    r.extend([0 for i in range(int(seconds*RATE))])\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process wav input with highpass filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voice_input(voice_data):\n",
    "    fs, data = wavfile.read(voice_data)\n",
    "    data = butter_bandpass_filter(data, BANDPASS_FREQ[0], BANDPASS_FREQ[1], fs)\n",
    "    return data, fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement butter bandpass filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "#     nyq = 0.5 * fs\n",
    "#     low = lowcut / nyq\n",
    "#     high = highcut / nyq\n",
    "#     sos = butter(order, [low, high], btype='band', analog=False, output='sos')\n",
    "#     return sos\n",
    "\n",
    "# def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "#     sos = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "#     y = sosfiltfilt(sos, data)\n",
    "#     return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get amplitude curve and apply regression on ITU-R BS.1770-4 loudness algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getting_emotion(data, rate):\n",
    "\n",
    "    \n",
    "# #     lol_data, fs = concat_project7.clean_sound(data, rate)\n",
    "\n",
    "#     #Convert raw data into features\n",
    "#     print(\"Data acquired\")\n",
    "\n",
    "#     feature_data = concat_project7.get_all_features(data, fs);\n",
    "    \n",
    "#     #return the exponential regression\n",
    "#     return (feature_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove silence before, after, and in-between voice data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_silence_from(amplitudes, threshold):\n",
    "    silenced = []\n",
    "    for x in amplitudes:\n",
    "        if x >= threshold:\n",
    "            silenced.append(x)\n",
    "    return silenced# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression converts loudness to scale 0 to 100 and adjusts for calibration offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_offset(calib_loudness):\n",
    "#     return calib_loudness - CALIBRATION_VOLUME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sets a new volume that is spoken at for the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reply_volume(user_volume):\n",
    "    speaker_volume = int(osascript.osascript('output volume of (get volume settings)')[1])\n",
    "    print(\"speaker volume is \" + str(speaker_volume))\n",
    "    print(\"user volume is \"+ str(user_volume))\n",
    "    if(speaker_volume < user_volume):\n",
    "        new_volume = speaker_volume + (((speaker_volume - user_volume)**2) /100)\n",
    "    else:\n",
    "        new_volume = speaker_volume - (((speaker_volume - user_volume)**2) /100)\n",
    "    print(\"speaker new volume is \" + str(new_volume))\n",
    "    osascript.osascript(\"set volume output volume \"+ str(new_volume)) \n",
    "    thing_to_say = 'hello your volume is at '+ str(int(new_volume))\n",
    "    print(thing_to_say)\n",
    "    os.system(\"say \"+ thing_to_say)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reply_emotion(user_emotion):\n",
    "    os.system(\"say \"+\" are you feeling \"+user_emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeprec = DeepEmotionRecognizer(emotions=['sad', 'neutral', 'happy', 'angry', 'disgust', 'fear'],\n",
    "                                n_rnn_layers=2, n_dense_layers=3, rnn_units=256, dense_units=128,\n",
    "                                dropout=0.35,epochs=500)\n",
    "# train the model\n",
    "deeprec.train(override=False)\n",
    "# get the accuracy\n",
    "print(deeprec.test_score())\n",
    "print(deeprec.confusion_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.system(\"say \"+ \"Please speak into the microphone\")\n",
    "# print(\"Please speak into the microphone\")\n",
    "# record_to_file('emotion.wav')\n",
    "# print(\"done\")\n",
    "# prediction = deeprec.predict('emotion.wav')\n",
    "# print(f\"Prediction: {prediction}\")\n",
    "# reply_emotion(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emotion_recognition import EmotionRecognizer\n",
    "from sklearn.svm import SVC\n",
    "# init a model, let's use SVC\n",
    "my_model = SVC()\n",
    "# pass my model to EmotionRecognizer instance\n",
    "# and balance the dataset\n",
    "rec = EmotionRecognizer(model=my_model, emotions=['angry', 'happy', 'sad'], balance=True, verbose=0)\n",
    "# train the model\n",
    "rec.train()\n",
    "# check the test accuracy for that model\n",
    "print(\"Test score:\", rec.test_score())\n",
    "# check the train accuracy for that model\n",
    "print(\"Train score:\", rec.train_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the best estimators from `grid` folder that was searched by GridSearchCV in `grid_search.py`,\n",
    "# and set the model to the best in terms of test score, and then train it\n",
    "rec.determine_best_model(train=True)\n",
    "# get the determined sklearn model name\n",
    "print(rec.model.__class__.__name__, \"is the best\")\n",
    "# get the test accuracy score for the best estimator\n",
    "print(\"Test score:\", rec.test_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.system(\"say \"+ \"Please speak into the microphone\")\n",
    "# print(\"Please speak into the microphone\")\n",
    "# record_to_file('emotion.wav')\n",
    "# print(\"done\")\n",
    "# prediction = rec.predict('emotion.wav')\n",
    "# print(f\"Prediction: {prediction}\")\n",
    "# reply_emotion(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emotion_recognition import EmotionRecognizer\n",
    "\n",
    "import pyaudio\n",
    "import os\n",
    "import wave\n",
    "from sys import byteorder\n",
    "from array import array\n",
    "from struct import pack\n",
    "from sklearn.ensemble import GradientBoostingClassifier, BaggingClassifier\n",
    "\n",
    "from utils import get_best_estimators\n",
    "\n",
    "THRESHOLD = 500\n",
    "CHUNK_SIZE = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "RATE = 16000\n",
    "\n",
    "SILENCE = 30\n",
    "\n",
    "def is_silent(snd_data):\n",
    "    \"Returns 'True' if below the 'silent' threshold\"\n",
    "    return max(snd_data) < THRESHOLD\n",
    "\n",
    "def normalize(snd_data):\n",
    "    \"Average the volume out\"\n",
    "    MAXIMUM = 16384\n",
    "    times = float(MAXIMUM)/max(abs(i) for i in snd_data)\n",
    "\n",
    "    r = array('h')\n",
    "    for i in snd_data:\n",
    "        r.append(int(i*times))\n",
    "    return r\n",
    "\n",
    "def trim(snd_data):\n",
    "    \"Trim the blank spots at the start and end\"\n",
    "    def _trim(snd_data):\n",
    "        snd_started = False\n",
    "        r = array('h')\n",
    "\n",
    "        for i in snd_data:\n",
    "            if not snd_started and abs(i)>THRESHOLD:\n",
    "                snd_started = True\n",
    "                r.append(i)\n",
    "\n",
    "            elif snd_started:\n",
    "                r.append(i)\n",
    "        return r\n",
    "\n",
    "    # Trim to the left\n",
    "    snd_data = _trim(snd_data)\n",
    "\n",
    "    # Trim to the right\n",
    "    snd_data.reverse()\n",
    "    snd_data = _trim(snd_data)\n",
    "    snd_data.reverse()\n",
    "    return snd_data\n",
    "\n",
    "def add_silence(snd_data, seconds):\n",
    "    \"Add silence to the start and end of 'snd_data' of length 'seconds' (float)\"\n",
    "    r = array('h', [0 for i in range(int(seconds*RATE))])\n",
    "    r.extend(snd_data)\n",
    "    r.extend([0 for i in range(int(seconds*RATE))])\n",
    "    return r\n",
    "\n",
    "def record():\n",
    "    \"\"\"\n",
    "    Record a word or words from the microphone and \n",
    "    return the data as an array of signed shorts.\n",
    "\n",
    "    Normalizes the audio, trims silence from the \n",
    "    start and end, and pads with 0.5 seconds of \n",
    "    blank sound to make sure VLC et al can play \n",
    "    it without getting chopped off.\n",
    "    \"\"\"\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=FORMAT, channels=1, rate=RATE,\n",
    "        input=True, output=True,\n",
    "        frames_per_buffer=CHUNK_SIZE)\n",
    "\n",
    "    num_silent = 0\n",
    "    snd_started = False\n",
    "\n",
    "    r = array('h')\n",
    "\n",
    "    while 1:\n",
    "        # little endian, signed short\n",
    "        snd_data = array('h', stream.read(CHUNK_SIZE))\n",
    "        if byteorder == 'big':\n",
    "            snd_data.byteswap()\n",
    "        r.extend(snd_data)\n",
    "\n",
    "        silent = is_silent(snd_data)\n",
    "\n",
    "        if silent and snd_started:\n",
    "            num_silent += 1\n",
    "        elif not silent and not snd_started:\n",
    "            snd_started = True\n",
    "\n",
    "        if snd_started and num_silent > SILENCE:\n",
    "            break\n",
    "\n",
    "    sample_width = p.get_sample_size(FORMAT)\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    r = normalize(r)\n",
    "    r = trim(r)\n",
    "    r = add_silence(r, 0.5)\n",
    "    return sample_width, r\n",
    "\n",
    "def record_to_file(path):\n",
    "    \"Records from the microphone and outputs the resulting data to 'path'\"\n",
    "    sample_width, data = record()\n",
    "    data = pack('<' + ('h'*len(data)), *data)\n",
    "\n",
    "    wf = wave.open(path, 'wb')\n",
    "    wf.setnchannels(1)\n",
    "    wf.setsampwidth(sample_width)\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(data)\n",
    "    wf.close()\n",
    "\n",
    "\n",
    "def get_estimators_name(estimators):\n",
    "    result = [ '\"{}\"'.format(estimator.__class__.__name__) for estimator, _, _ in estimators ]\n",
    "    return ','.join(result), {estimator_name.strip('\"'): estimator for estimator_name, (estimator, _, _) in zip(result, estimators)}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP REC NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Extracting features for train:   0%|          | 0/3837 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TESS&RAVDESS] There are 812 training audio files for category:sad\n",
      "[TESS&RAVDESS] There are 146 testing audio files for category:sad\n",
      "[TESS&RAVDESS] There are 805 training audio files for category:happy\n",
      "[TESS&RAVDESS] There are 147 testing audio files for category:happy\n",
      "[+] Writed TESS & RAVDESS DB CSV File\n",
      "[EMO-DB] Total files to write: 133\n",
      "[EMO-DB] Training samples: 106\n",
      "[EMO-DB] Testing samples: 26\n",
      "[+] Writed EMO-DB CSV File\n",
      "[+] Writed Custom DB CSV File\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'chroma_cens' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3ecb668ffaef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     deeprec = DeepEmotionRecognizer(emotions=['sad', 'happy'],\n\u001b[1;32m      3\u001b[0m                                     \u001b[0mn_rnn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_dense_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                     dropout=0.35,epochs=500)\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdeeprec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/loudness/Emote-X/emotion-recognition-using-speech/deep_emotion_recognition.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# compute the input length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_input_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# boolean attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/loudness/Emote-X/emotion-recognition-using-speech/deep_emotion_recognition.py\u001b[0m in \u001b[0;36m_compute_input_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compute_input_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/loudness/Emote-X/emotion-recognition-using-speech/deep_emotion_recognition.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;31m# reshape to 3 dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mX_train_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/loudness/Emote-X/emotion-recognition-using-speech/emotion_recognition.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             result = load_data(self.train_desc_files, self.test_desc_files, self.audio_config, self.classification,\n\u001b[0;32m--> 139\u001b[0;31m                                 emotions=self.emotions, balance=self.balance)\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/loudness/Emote-X/emotion-recognition-using-speech/data_extractor.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(train_desc_files, test_desc_files, audio_config, classification, shuffle, balance, emotions)\u001b[0m\n\u001b[1;32m    230\u001b[0m                                 balance=balance, verbose=0)\n\u001b[1;32m    231\u001b[0m     \u001b[0;31m# Loads training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0maudiogen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_desc_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0;31m# Loads testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0maudiogen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_desc_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/loudness/Emote-X/emotion-recognition-using-speech/data_extractor.py\u001b[0m in \u001b[0;36mload_train_data\u001b[0;34m(self, desc_files, shuffle)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_speech.csv\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;34m\"\"\"Loads training data from the metadata files `desc_files`\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_speech.csv\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/loudness/Emote-X/emotion-recognition-using-speech/data_extractor.py\u001b[0m in \u001b[0;36m_load_data\u001b[0;34m(self, desc_files, partition, shuffle)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_metadata_from_desc_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;31m# balancing the datasets ( both training or testing )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpartition\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbalance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/loudness/Emote-X/emotion-recognition-using-speech/data_extractor.py\u001b[0m in \u001b[0;36mload_metadata_from_desc_file\u001b[0;34m(self, desc_files, partition)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mappend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0maudio_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Extracting features for {partition}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                 \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dimension\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dimension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/loudness/Emote-X/emotion-recognition-using-speech/utils.py\u001b[0m in \u001b[0;36mextract_feature\u001b[0;34m(file_name, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mderiv_rmse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mchroma_cens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0mchroma_cens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchroma_cens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchroma_cens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'chroma_cens' referenced before assignment"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    deeprec = DeepEmotionRecognizer(emotions=['sad', 'happy'],\n",
    "                                    n_rnn_layers=2, n_dense_layers=3, rnn_units=256, dense_units=128,\n",
    "                                    dropout=0.35,epochs=500)\n",
    "    # train the model\n",
    "    deeprec.train(override=False)\n",
    "    # get the accuracy\n",
    "    print(deeprec.test_score())\n",
    "\n",
    "    print(\"Test accuracy score: {:.3f}%\".format(deeprec.test_score()*100))\n",
    "#     print(\"Please talk\")\n",
    "    \n",
    "    \n",
    "#     filename = \"test.wav\"\n",
    "#     record_to_file(filename)\n",
    "#     result = deeprec.predict(filename)\n",
    "#     print(result)\n",
    "# NOTES\n",
    "# look for happy vs anger\n",
    "# api to remove other people voice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## os.system(\"say \"+ \"Please speak into the microphone\")\n",
    "print(\"Please speak into the microphone\")\n",
    "record_to_file('emotion.wav')\n",
    "print(\"done\")\n",
    "prediction, confidence = deeprec.predict('emotion.wav')\n",
    "print(f\"Prediction: {prediction}\")\n",
    "reply_emotion(prediction)\n",
    "print(\"happy confidence: \" + str(confidence[0][0][1]*100) + \"%\")\n",
    "print(\"sad confidence: \" + str(confidence[0][0][0]*100) + \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    deeprec_angry = DeepEmotionRecognizer(emotions=['sad', 'happy', 'angry'],\n",
    "                                    n_rnn_layers=2, n_dense_layers=3, rnn_units=256, dense_units=128,\n",
    "                                    dropout=0.35,epochs=500)\n",
    "    # train the model\n",
    "    deeprec_angry.train(override=False)\n",
    "    # get the accuracy\n",
    "    print(deeprec_angry.test_score())\n",
    "\n",
    "    print(\"Test accuracy score: {:.3f}%\".format(deeprec_angry.test_score()*100))\n",
    "#     print(\"Please talk\")\n",
    "    \n",
    "    \n",
    "#     filename = \"test.wav\"\n",
    "#     record_to_file(filename)\n",
    "#     result = deeprec.predict(filename)\n",
    "#     print(result)\n",
    "# NOTES\n",
    "# look for happy vs anger\n",
    "# api to remove other people voice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## os.system(\"say \"+ \"Please speak into the microphone\")\n",
    "print(\"Please speak into the microphone\")\n",
    "record_to_file('emotion.wav')\n",
    "print(\"done\")\n",
    "prediction, confidence = deeprec_angry.predict('emotion.wav')\n",
    "print(f\"Prediction: {prediction}\")\n",
    "reply_emotion(prediction)\n",
    "print(\"happy confidence: \" + str(confidence[0][0][1]*100) + \"%\")\n",
    "print(\"sad confidence: \" + str(confidence[0][0][0]*100) + \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Extracting features for train:   0%|          | 0/3837 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TESS&RAVDESS] There are 805 training audio files for category:happy\n",
      "[TESS&RAVDESS] There are 147 testing audio files for category:happy\n",
      "[TESS&RAVDESS] There are 812 training audio files for category:sad\n",
      "[TESS&RAVDESS] There are 146 testing audio files for category:sad\n",
      "[+] Writed TESS & RAVDESS DB CSV File\n",
      "[EMO-DB] Total files to write: 133\n",
      "[EMO-DB] Training samples: 106\n",
      "[EMO-DB] Testing samples: 26\n",
      "[+] Writed EMO-DB CSV File\n",
      "[+] Writed Custom DB CSV File\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features for train:   2%|â–         | 65/3837 [00:44<40:23,  1.56it/s] "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    deeprec = DeepEmotionRecognizer(emotions=['happy', 'sad'],\n",
    "                        features=['mfcc', 'chroma','mel','contrast','tonnetz', 'rmse', 'poly_feature','chroma_cens','deriv'],\n",
    "                                    balance=True, verbose=1,\n",
    "                                    n_rnn_layers=2, n_dense_layers=3, rnn_units=128, dense_units=256,\n",
    "                                    dropout=0.3,epochs=300)\n",
    "    # train the model\n",
    "    deeprec.train(override=False)\n",
    "    # get the accuracy\n",
    "    print(deeprec.test_score())\n",
    "\n",
    "    print(\"Test accuracy score: {:.3f}%\".format(deeprec.test_score()*100))\n",
    "#     print(\"Please talk\")\n",
    "    \n",
    "    \n",
    "#     filename = \"test.wav\"\n",
    "#     record_to_file(filename)\n",
    "#     result = deeprec.predict(filename)\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"say \"+ \"Please speak into the microphone\")\n",
    "print(\"Please speak into the microphone\")\n",
    "record_to_file('emotion.wav')\n",
    "print(\"done\")\n",
    "prediction, confidence = deeprec.predict('emotion.wav')\n",
    "print(f\"Prediction: {prediction}\")\n",
    "reply_emotion(prediction)\n",
    "print(\"netrual confidence: \" + str(confidence[0][0][1]*100) + \"%\")\n",
    "print(\"sad confidence: \" + str(confidence[0][0][0]*100) + \"%\")\n",
    "print(\"happy confidence: \" + str(confidence[0][0][2]*100) + \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emotion_recognition import EmotionRecognizer\n",
    "from sklearn.svm import SVC\n",
    "# init a model, let's use SVC\n",
    "my_model = SVC()\n",
    "# pass my model to EmotionRecognizer instance\n",
    "# and balance the dataset\n",
    "#     pitch_cepstrum = kwargs.get('pitch_cepstrum')\n",
    "#     formants = kwargs.get('formants')\n",
    "#     rmse = kwargs.get('rmse')\n",
    "#     poly_features = kwargs.get('poly_features')\n",
    "#     deriv = kwargs.get('deriv')\n",
    "rec = EmotionRecognizer(model=my_model, emotions=['happy', 'sad'],\n",
    "                        features=['mfcc', 'chroma','mel','contrast','tonnetz'],\n",
    "                        balance=True, verbose=1)\n",
    "# train the model\n",
    "rec.train()\n",
    "# check the test accuracy for that model\n",
    "print(\"Test score:\", rec.test_score())\n",
    "# check the train accuracy for that model\n",
    "print(\"Train score:\", rec.train_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the best estimators from `grid` folder that was searched by GridSearchCV in `grid_search.py`,\n",
    "# and set the model to the best in terms of test score, and then train it\n",
    "rec.determine_best_model(train=True)\n",
    "# get the determined sklearn model name\n",
    "print(rec.model.__class__.__name__, \"is the best\")\n",
    "# get the test accuracy score for the best estimator\n",
    "print(\"Test score:\", rec.test_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"say \"+ \"Please speak into the microphone\")\n",
    "print(\"Please speak into the microphone\")\n",
    "record_to_file('emotion.wav')\n",
    "print(\"done\")\n",
    "prediction = rec.predict('emotion.wav')\n",
    "print(f\"Prediction: {prediction}\")\n",
    "reply_emotion(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    deeprec = DeepEmotionRecognizer(emotions=['sad', 'happy'],\n",
    "                                    n_rnn_layers=2, n_dense_layers=3, rnn_units=256, dense_units=128,\n",
    "                                    dropout=0.35,epochs=500)\n",
    "    # train the model\n",
    "    deeprec.train(override=False)\n",
    "    # get the accuracy\n",
    "    print(deeprec.test_score())\n",
    "\n",
    "    print(\"Test accuracy score: {:.3f}%\".format(deeprec.test_score()*100))\n",
    "#     print(\"Please talk\")\n",
    "    \n",
    "    \n",
    "#     filename = \"test.wav\"\n",
    "#     record_to_file(filename)\n",
    "#     result = deeprec.predict(filename)\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
