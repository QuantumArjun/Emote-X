{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy\n",
    "# from scipy.optimize import curve_fit\n",
    "# from scipy.io import wavfile\n",
    "# from scipy.signal import butter, sosfiltfilt\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from scipy import signal\n",
    "# import osascript\n",
    "# from gtts import gTTS \n",
    "# from matplotlib import pylab\n",
    "# import os \n",
    "# import pyaudio\n",
    "# import wave\n",
    "# import keyboard  # using module keyboard\n",
    "# import soundfile as sf\n",
    "# import math\n",
    "# import pyloudnorm as pyln\n",
    "# from sys import byteorder\n",
    "# from array import array\n",
    "# from struct import pack\n",
    "# import import_ipynb\n",
    "# import concat_project7\n",
    "# from joblib import dump, load\n",
    "from emotion_recognition import EmotionRecognizer\n",
    "from sklearn.svm import SVC\n",
    "from deep_emotion_recognition import DeepEmotionRecognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 100\n",
    "CHUNK_SIZE = 2048\n",
    "FORMAT = pyaudio.paInt16\n",
    "RATE = 16000\n",
    "AVG_STEP = 75\n",
    "PERCENTILE = 70\n",
    "BANDPASS_FREQ = [300, 3400]\n",
    "CALIBRATION_VOLUME = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record from microphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record():\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=FORMAT, channels=1, rate=RATE,\n",
    "        input=True, output=True,\n",
    "        frames_per_buffer=CHUNK_SIZE)\n",
    "\n",
    "    num_silent = 0\n",
    "    snd_started = False\n",
    "\n",
    "    r = array('h')\n",
    "\n",
    "    while 1:\n",
    "        # little endian, signed short\n",
    "        snd_data = array('h', stream.read(CHUNK_SIZE))\n",
    "        if byteorder == 'big':\n",
    "            snd_data.byteswap()\n",
    "        r.extend(snd_data)\n",
    "\n",
    "        silent = is_silent(snd_data)\n",
    "\n",
    "        if silent and snd_started:\n",
    "            num_silent += 1\n",
    "        elif not silent and not snd_started:\n",
    "            snd_started = True\n",
    "\n",
    "        if snd_started and num_silent > 60:\n",
    "            break\n",
    "\n",
    "    sample_width = p.get_sample_size(FORMAT)\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "#     r = trim(r)\n",
    "#     r = add_silence(r, 0.5)\n",
    "    return sample_width, r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to wav file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_to_file(path):\n",
    "    sample_width, data = record()\n",
    "    data = pack('<' + ('h'*len(data)), *data)\n",
    "\n",
    "    wf = wave.open(path, 'wb')\n",
    "    wf.setnchannels(1)\n",
    "    wf.setsampwidth(sample_width)\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(data)\n",
    "    wf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks if input is silent based on threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_silent(snd_data):\n",
    "    \"Returns 'True' if below the 'silent' threshold\"\n",
    "    return max(snd_data) < THRESHOLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trim input to remove silence at beginning and end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(snd_data):\n",
    "    def _trim(snd_data):\n",
    "        snd_started = False\n",
    "        r = array('h')\n",
    "\n",
    "        for i in snd_data:\n",
    "            if not snd_started and abs(i)>THRESHOLD:\n",
    "                snd_started = True\n",
    "                r.append(i)\n",
    "            elif snd_started:\n",
    "                r.append(i)\n",
    "        return r\n",
    "\n",
    "    # Trim to the left\n",
    "    snd_data = _trim(snd_data)\n",
    "\n",
    "    # Trim to the right\n",
    "    snd_data.reverse()\n",
    "    snd_data = _trim(snd_data)\n",
    "    snd_data.reverse()\n",
    "    return snd_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pad with silence at beginning and end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_silence(snd_data, seconds):\n",
    "    \"\"\n",
    "    r = array('h', [0 for i in range(int(seconds*RATE))])\n",
    "    r.extend(snd_data)\n",
    "    r.extend([0 for i in range(int(seconds*RATE))])\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process wav input with highpass filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voice_input(voice_data):\n",
    "    fs, data = wavfile.read(voice_data)\n",
    "    data = butter_bandpass_filter(data, BANDPASS_FREQ[0], BANDPASS_FREQ[1], fs)\n",
    "    return data, fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement butter bandpass filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "#     nyq = 0.5 * fs\n",
    "#     low = lowcut / nyq\n",
    "#     high = highcut / nyq\n",
    "#     sos = butter(order, [low, high], btype='band', analog=False, output='sos')\n",
    "#     return sos\n",
    "\n",
    "# def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "#     sos = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "#     y = sosfiltfilt(sos, data)\n",
    "#     return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get amplitude curve and apply regression on ITU-R BS.1770-4 loudness algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getting_emotion(data, rate):\n",
    "\n",
    "    \n",
    "# #     lol_data, fs = concat_project7.clean_sound(data, rate)\n",
    "\n",
    "#     #Convert raw data into features\n",
    "#     print(\"Data acquired\")\n",
    "\n",
    "#     feature_data = concat_project7.get_all_features(data, fs);\n",
    "    \n",
    "#     #return the exponential regression\n",
    "#     return (feature_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove silence before, after, and in-between voice data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_silence_from(amplitudes, threshold):\n",
    "    silenced = []\n",
    "    for x in amplitudes:\n",
    "        if x >= threshold:\n",
    "            silenced.append(x)\n",
    "    return silenced# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression converts loudness to scale 0 to 100 and adjusts for calibration offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_offset(calib_loudness):\n",
    "#     return calib_loudness - CALIBRATION_VOLUME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sets a new volume that is spoken at for the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reply_volume(user_volume):\n",
    "    speaker_volume = int(osascript.osascript('output volume of (get volume settings)')[1])\n",
    "    print(\"speaker volume is \" + str(speaker_volume))\n",
    "    print(\"user volume is \"+ str(user_volume))\n",
    "    if(speaker_volume < user_volume):\n",
    "        new_volume = speaker_volume + (((speaker_volume - user_volume)**2) /100)\n",
    "    else:\n",
    "        new_volume = speaker_volume - (((speaker_volume - user_volume)**2) /100)\n",
    "    print(\"speaker new volume is \" + str(new_volume))\n",
    "    osascript.osascript(\"set volume output volume \"+ str(new_volume)) \n",
    "    thing_to_say = 'hello your volume is at '+ str(int(new_volume))\n",
    "    print(thing_to_say)\n",
    "    os.system(\"say \"+ thing_to_say)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reply_emotion(user_emotion):\n",
    "    os.system(\"say \"+\" are you feeling \"+user_emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0814 13:14:02.562827 140735684912000 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0814 13:14:02.575743 140735684912000 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0814 13:14:02.577932 140735684912000 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TESS&RAVDESS] There are 812 training audio files for category:sad\n",
      "[TESS&RAVDESS] There are 146 testing audio files for category:sad\n",
      "[TESS&RAVDESS] There are 585 training audio files for category:neutral\n",
      "[TESS&RAVDESS] There are 93 testing audio files for category:neutral\n",
      "[TESS&RAVDESS] There are 805 training audio files for category:happy\n",
      "[TESS&RAVDESS] There are 147 testing audio files for category:happy\n",
      "[TESS&RAVDESS] There are 808 training audio files for category:angry\n",
      "[TESS&RAVDESS] There are 147 testing audio files for category:angry\n",
      "[TESS&RAVDESS] There are 511 training audio files for category:disgust\n",
      "[TESS&RAVDESS] There are 79 testing audio files for category:disgust\n",
      "[TESS&RAVDESS] There are 817 training audio files for category:fear\n",
      "[TESS&RAVDESS] There are 141 testing audio files for category:fear\n",
      "[+] Writed TESS & RAVDESS DB CSV File\n",
      "[EMO-DB] Total files to write: 454\n",
      "[EMO-DB] Training samples: 363\n",
      "[EMO-DB] Testing samples: 90\n",
      "[+] Writed EMO-DB CSV File\n",
      "[+] Writed Custom DB CSV File\n",
      "[+] Data loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0814 13:14:02.824571 140735684912000 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0814 13:14:02.831129 140735684912000 deprecation.py:506] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0814 13:14:03.280225 140735684912000 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Model created\n",
      "[*] Model weights loaded\n",
      "0.7619047619047619\n",
      "              predicted_sad  predicted_neutral  predicted_happy  \\\n",
      "true_sad          83.516487           1.098901         1.098901   \n",
      "true_neutral      23.076923          65.934067         0.000000   \n",
      "true_happy         6.593407           1.098901        71.428574   \n",
      "true_angry         0.000000           0.000000         2.197802   \n",
      "true_disgust       8.791209           1.098901         1.098901   \n",
      "true_fear         16.483517           0.000000        10.989011   \n",
      "\n",
      "              predicted_angry  predicted_disgust  predicted_fear  \n",
      "true_sad             0.000000           7.692308        6.593407  \n",
      "true_neutral         3.296704           5.494505        2.197802  \n",
      "true_happy           3.296704           6.593407       10.989011  \n",
      "true_angry          87.912086           6.593407        3.296704  \n",
      "true_disgust         0.000000          85.714287        3.296704  \n",
      "true_fear            4.395605           5.494505       62.637363  \n"
     ]
    }
   ],
   "source": [
    "deeprec = DeepEmotionRecognizer(emotions=['sad', 'neutral', 'happy', 'angry', 'disgust', 'fear'],\n",
    "                                n_rnn_layers=2, n_dense_layers=3, rnn_units=256, dense_units=128,\n",
    "                                dropout=0.35,epochs=500)\n",
    "# train the model\n",
    "deeprec.train(override=False)\n",
    "# get the accuracy\n",
    "print(deeprec.test_score())\n",
    "print(deeprec.confusion_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.system(\"say \"+ \"Please speak into the microphone\")\n",
    "# print(\"Please speak into the microphone\")\n",
    "# record_to_file('emotion.wav')\n",
    "# print(\"done\")\n",
    "# prediction = deeprec.predict('emotion.wav')\n",
    "# print(f\"Prediction: {prediction}\")\n",
    "# reply_emotion(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Model trained\n",
      "Test score: 0.7967479674796748\n",
      "Train score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from emotion_recognition import EmotionRecognizer\n",
    "from sklearn.svm import SVC\n",
    "# init a model, let's use SVC\n",
    "my_model = SVC()\n",
    "# pass my model to EmotionRecognizer instance\n",
    "# and balance the dataset\n",
    "rec = EmotionRecognizer(model=my_model, emotions=['angry', 'happy', 'sad'], balance=True, verbose=0)\n",
    "# train the model\n",
    "rec.train()\n",
    "# check the test accuracy for that model\n",
    "print(\"Test score:\", rec.test_score())\n",
    "# check the train accuracy for that model\n",
    "print(\"Train score:\", rec.train_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier is the best\n",
      "Test score: 0.8943089430894309\n"
     ]
    }
   ],
   "source": [
    "# loads the best estimators from `grid` folder that was searched by GridSearchCV in `grid_search.py`,\n",
    "# and set the model to the best in terms of test score, and then train it\n",
    "rec.determine_best_model(train=True)\n",
    "# get the determined sklearn model name\n",
    "print(rec.model.__class__.__name__, \"is the best\")\n",
    "# get the test accuracy score for the best estimator\n",
    "print(\"Test score:\", rec.test_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.system(\"say \"+ \"Please speak into the microphone\")\n",
    "# print(\"Please speak into the microphone\")\n",
    "# record_to_file('emotion.wav')\n",
    "# print(\"done\")\n",
    "# prediction = rec.predict('emotion.wav')\n",
    "# print(f\"Prediction: {prediction}\")\n",
    "# reply_emotion(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emotion_recognition import EmotionRecognizer\n",
    "\n",
    "import pyaudio\n",
    "import os\n",
    "import wave\n",
    "from sys import byteorder\n",
    "from array import array\n",
    "from struct import pack\n",
    "from sklearn.ensemble import GradientBoostingClassifier, BaggingClassifier\n",
    "\n",
    "from utils import get_best_estimators\n",
    "\n",
    "THRESHOLD = 500\n",
    "CHUNK_SIZE = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "RATE = 16000\n",
    "\n",
    "SILENCE = 30\n",
    "\n",
    "def is_silent(snd_data):\n",
    "    \"Returns 'True' if below the 'silent' threshold\"\n",
    "    return max(snd_data) < THRESHOLD\n",
    "\n",
    "def normalize(snd_data):\n",
    "    \"Average the volume out\"\n",
    "    MAXIMUM = 16384\n",
    "    times = float(MAXIMUM)/max(abs(i) for i in snd_data)\n",
    "\n",
    "    r = array('h')\n",
    "    for i in snd_data:\n",
    "        r.append(int(i*times))\n",
    "    return r\n",
    "\n",
    "def trim(snd_data):\n",
    "    \"Trim the blank spots at the start and end\"\n",
    "    def _trim(snd_data):\n",
    "        snd_started = False\n",
    "        r = array('h')\n",
    "\n",
    "        for i in snd_data:\n",
    "            if not snd_started and abs(i)>THRESHOLD:\n",
    "                snd_started = True\n",
    "                r.append(i)\n",
    "\n",
    "            elif snd_started:\n",
    "                r.append(i)\n",
    "        return r\n",
    "\n",
    "    # Trim to the left\n",
    "    snd_data = _trim(snd_data)\n",
    "\n",
    "    # Trim to the right\n",
    "    snd_data.reverse()\n",
    "    snd_data = _trim(snd_data)\n",
    "    snd_data.reverse()\n",
    "    return snd_data\n",
    "\n",
    "def add_silence(snd_data, seconds):\n",
    "    \"Add silence to the start and end of 'snd_data' of length 'seconds' (float)\"\n",
    "    r = array('h', [0 for i in range(int(seconds*RATE))])\n",
    "    r.extend(snd_data)\n",
    "    r.extend([0 for i in range(int(seconds*RATE))])\n",
    "    return r\n",
    "\n",
    "def record():\n",
    "    \"\"\"\n",
    "    Record a word or words from the microphone and \n",
    "    return the data as an array of signed shorts.\n",
    "\n",
    "    Normalizes the audio, trims silence from the \n",
    "    start and end, and pads with 0.5 seconds of \n",
    "    blank sound to make sure VLC et al can play \n",
    "    it without getting chopped off.\n",
    "    \"\"\"\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=FORMAT, channels=1, rate=RATE,\n",
    "        input=True, output=True,\n",
    "        frames_per_buffer=CHUNK_SIZE)\n",
    "\n",
    "    num_silent = 0\n",
    "    snd_started = False\n",
    "\n",
    "    r = array('h')\n",
    "\n",
    "    while 1:\n",
    "        # little endian, signed short\n",
    "        snd_data = array('h', stream.read(CHUNK_SIZE))\n",
    "        if byteorder == 'big':\n",
    "            snd_data.byteswap()\n",
    "        r.extend(snd_data)\n",
    "\n",
    "        silent = is_silent(snd_data)\n",
    "\n",
    "        if silent and snd_started:\n",
    "            num_silent += 1\n",
    "        elif not silent and not snd_started:\n",
    "            snd_started = True\n",
    "\n",
    "        if snd_started and num_silent > SILENCE:\n",
    "            break\n",
    "\n",
    "    sample_width = p.get_sample_size(FORMAT)\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    r = normalize(r)\n",
    "    r = trim(r)\n",
    "    r = add_silence(r, 0.5)\n",
    "    return sample_width, r\n",
    "\n",
    "def record_to_file(path):\n",
    "    \"Records from the microphone and outputs the resulting data to 'path'\"\n",
    "    sample_width, data = record()\n",
    "    data = pack('<' + ('h'*len(data)), *data)\n",
    "\n",
    "    wf = wave.open(path, 'wb')\n",
    "    wf.setnchannels(1)\n",
    "    wf.setsampwidth(sample_width)\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(data)\n",
    "    wf.close()\n",
    "\n",
    "\n",
    "def get_estimators_name(estimators):\n",
    "    result = [ '\"{}\"'.format(estimator.__class__.__name__) for estimator, _, _ in estimators ]\n",
    "    return ','.join(result), {estimator_name.strip('\"'): estimator for estimator_name, (estimator, _, _) in zip(result, estimators)}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP REC NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TESS&RAVDESS] There are 812 training audio files for category:sad\n",
      "[TESS&RAVDESS] There are 146 testing audio files for category:sad\n",
      "[TESS&RAVDESS] There are 805 training audio files for category:happy\n",
      "[TESS&RAVDESS] There are 147 testing audio files for category:happy\n",
      "[+] Writed TESS & RAVDESS DB CSV File\n",
      "[EMO-DB] Total files to write: 133\n",
      "[EMO-DB] Training samples: 106\n",
      "[EMO-DB] Testing samples: 26\n",
      "[+] Writed EMO-DB CSV File\n",
      "[+] Writed Custom DB CSV File\n",
      "[+] Data loaded\n",
      "[+] Model created\n",
      "[*] Model weights loaded\n",
      "0.9342857142857143\n",
      "Test accuracy score: 93.429%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    deeprec = DeepEmotionRecognizer(emotions=['sad', 'happy'],\n",
    "                                    n_rnn_layers=2, n_dense_layers=3, rnn_units=256, dense_units=128,\n",
    "                                    dropout=0.35,epochs=500)\n",
    "    # train the model\n",
    "    deeprec.train(override=False)\n",
    "    # get the accuracy\n",
    "    print(deeprec.test_score())\n",
    "\n",
    "    print(\"Test accuracy score: {:.3f}%\".format(deeprec.test_score()*100))\n",
    "#     print(\"Please talk\")\n",
    "    \n",
    "    \n",
    "#     filename = \"test.wav\"\n",
    "#     record_to_file(filename)\n",
    "#     result = deeprec.predict(filename)\n",
    "#     print(result)\n",
    "# NOTES\n",
    "# look for happy vs anger\n",
    "# api to remove other people voice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please speak into the microphone\n",
      "done\n",
      "Prediction: happy\n",
      "happy confidence: 98.06010127067566%\n",
      "sad confidence: 1.9398996606469154%\n"
     ]
    }
   ],
   "source": [
    "## os.system(\"say \"+ \"Please speak into the microphone\")\n",
    "print(\"Please speak into the microphone\")\n",
    "record_to_file('emotion.wav')\n",
    "print(\"done\")\n",
    "prediction, confidence = deeprec.predict('emotion.wav')\n",
    "print(f\"Prediction: {prediction}\")\n",
    "reply_emotion(prediction)\n",
    "print(\"happy confidence: \" + str(confidence[0][0][1]*100) + \"%\")\n",
    "print(\"sad confidence: \" + str(confidence[0][0][0]*100) + \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TESS&RAVDESS] There are 812 training audio files for category:sad\n",
      "[TESS&RAVDESS] There are 146 testing audio files for category:sad\n",
      "[TESS&RAVDESS] There are 805 training audio files for category:happy\n",
      "[TESS&RAVDESS] There are 147 testing audio files for category:happy\n",
      "[TESS&RAVDESS] There are 808 training audio files for category:angry\n",
      "[TESS&RAVDESS] There are 147 testing audio files for category:angry\n",
      "[+] Writed TESS & RAVDESS DB CSV File\n",
      "[EMO-DB] Total files to write: 260\n",
      "[EMO-DB] Training samples: 208\n",
      "[EMO-DB] Testing samples: 51\n",
      "[+] Writed EMO-DB CSV File\n",
      "[+] Writed Custom DB CSV File\n",
      "[+] Data loaded\n",
      "[+] Model created\n",
      "[*] Model weights loaded\n",
      "0.8780487804878049\n",
      "Test accuracy score: 87.805%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    deeprec_angry = DeepEmotionRecognizer(emotions=['sad', 'happy', 'angry'],\n",
    "                                    n_rnn_layers=2, n_dense_layers=3, rnn_units=256, dense_units=128,\n",
    "                                    dropout=0.35,epochs=500)\n",
    "    # train the model\n",
    "    deeprec_angry.train(override=False)\n",
    "    # get the accuracy\n",
    "    print(deeprec_angry.test_score())\n",
    "\n",
    "    print(\"Test accuracy score: {:.3f}%\".format(deeprec_angry.test_score()*100))\n",
    "#     print(\"Please talk\")\n",
    "    \n",
    "    \n",
    "#     filename = \"test.wav\"\n",
    "#     record_to_file(filename)\n",
    "#     result = deeprec.predict(filename)\n",
    "#     print(result)\n",
    "# NOTES\n",
    "# look for happy vs anger\n",
    "# api to remove other people voice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please speak into the microphone\n",
      "done\n",
      "Prediction: happy\n",
      "happy confidence: 79.44456338882446%\n",
      "sad confidence: 20.539531111717224%\n"
     ]
    }
   ],
   "source": [
    "## os.system(\"say \"+ \"Please speak into the microphone\")\n",
    "print(\"Please speak into the microphone\")\n",
    "record_to_file('emotion.wav')\n",
    "print(\"done\")\n",
    "prediction, confidence = deeprec_angry.predict('emotion.wav')\n",
    "print(f\"Prediction: {prediction}\")\n",
    "reply_emotion(prediction)\n",
    "print(\"happy confidence: \" + str(confidence[0][0][1]*100) + \"%\")\n",
    "print(\"sad confidence: \" + str(confidence[0][0][0]*100) + \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TESS&RAVDESS] There are 805 training audio files for category:happy\n",
      "[TESS&RAVDESS] There are 147 testing audio files for category:happy\n",
      "[TESS&RAVDESS] There are 812 training audio files for category:sad\n",
      "[TESS&RAVDESS] There are 146 testing audio files for category:sad\n",
      "[+] Writed TESS & RAVDESS DB CSV File\n",
      "[EMO-DB] Total files to write: 133\n",
      "[EMO-DB] Training samples: 106\n",
      "[EMO-DB] Testing samples: 26\n",
      "[+] Writed EMO-DB CSV File\n",
      "[+] Writed Custom DB CSV File\n",
      "[+] Data loaded\n",
      "[+] Model created\n",
      "[*] Model weights loaded\n",
      "0.9119318181818182\n",
      "Test accuracy score: 91.193%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    deeprec = DeepEmotionRecognizer(emotions=['happy', 'sad'],\n",
    "                        features=['mfcc', 'chroma','mel','contrast','tonnetz'], balance=True, verbose=1,\n",
    "                                    n_rnn_layers=2, n_dense_layers=3, rnn_units=128, dense_units=256,\n",
    "                                    dropout=0.3,epochs=500)\n",
    "    # train the model\n",
    "    deeprec.train(override=False)\n",
    "    # get the accuracy\n",
    "    print(deeprec.test_score())\n",
    "\n",
    "    print(\"Test accuracy score: {:.3f}%\".format(deeprec.test_score()*100))\n",
    "#     print(\"Please talk\")\n",
    "    \n",
    "    \n",
    "#     filename = \"test.wav\"\n",
    "#     record_to_file(filename)\n",
    "#     result = deeprec.predict(filename)\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please speak into the microphone\n",
      "done\n",
      "Prediction: neutral\n",
      "netrual confidence: 83.63749980926514%\n",
      "sad confidence: 3.679598867893219%\n",
      "happy confidence: 12.682893872261047%\n"
     ]
    }
   ],
   "source": [
    "os.system(\"say \"+ \"Please speak into the microphone\")\n",
    "print(\"Please speak into the microphone\")\n",
    "record_to_file('emotion.wav')\n",
    "print(\"done\")\n",
    "prediction, confidence = deeprec.predict('emotion.wav')\n",
    "print(f\"Prediction: {prediction}\")\n",
    "reply_emotion(prediction)\n",
    "print(\"netrual confidence: \" + str(confidence[0][0][1]*100) + \"%\")\n",
    "print(\"sad confidence: \" + str(confidence[0][0][0]*100) + \"%\")\n",
    "print(\"happy confidence: \" + str(confidence[0][0][2]*100) + \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TESS&RAVDESS] There are 805 training audio files for category:happy\n",
      "[TESS&RAVDESS] There are 147 testing audio files for category:happy\n",
      "[TESS&RAVDESS] There are 812 training audio files for category:sad\n",
      "[TESS&RAVDESS] There are 146 testing audio files for category:sad\n",
      "[+] Writed TESS & RAVDESS DB CSV File\n",
      "[EMO-DB] Total files to write: 133\n",
      "[EMO-DB] Training samples: 106\n",
      "[EMO-DB] Testing samples: 26\n",
      "[+] Writed EMO-DB CSV File\n",
      "[+] Writed Custom DB CSV File\n",
      "[+] Data loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emotex/.local/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Model trained\n",
      "Test score: 0.8380681818181818\n",
      "Train score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from emotion_recognition import EmotionRecognizer\n",
    "from sklearn.svm import SVC\n",
    "# init a model, let's use SVC\n",
    "my_model = SVC()\n",
    "# pass my model to EmotionRecognizer instance\n",
    "# and balance the dataset\n",
    "rec = EmotionRecognizer(model=my_model, emotions=['happy', 'sad'],\n",
    "                        features=['mfcc', 'chroma','mel','contrast','tonnetz'],\n",
    "                        balance=True, verbose=1)\n",
    "# train the model\n",
    "rec.train()\n",
    "# check the test accuracy for that model\n",
    "print(\"Test score:\", rec.test_score())\n",
    "# check the train accuracy for that model\n",
    "print(\"Train score:\", rec.train_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emotex/.local/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator SVC from version 0.21.2 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/emotex/.local/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.21.2 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/emotex/.local/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.21.2 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/emotex/.local/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator DummyClassifier from version 0.21.2 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/emotex/.local/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 0.21.2 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/emotex/.local/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator GradientBoostingClassifier from version 0.21.2 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/emotex/.local/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator KNeighborsClassifier from version 0.21.2 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/emotex/.local/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator LabelBinarizer from version 0.21.2 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/emotex/.local/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator MLPClassifier from version 0.21.2 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/emotex/.local/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator BaggingClassifier from version 0.21.2 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "Evaluating BaggingClassifier: 100%|██████████| 6/6 [00:15<00:00,  2.39s/it]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Best model determined: MLPClassifier with 94.886% test accuracy\n",
      "MLPClassifier is the best\n",
      "Test score: 0.9488636363636364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# loads the best estimators from `grid` folder that was searched by GridSearchCV in `grid_search.py`,\n",
    "# and set the model to the best in terms of test score, and then train it\n",
    "rec.determine_best_model(train=True)\n",
    "# get the determined sklearn model name\n",
    "print(rec.model.__class__.__name__, \"is the best\")\n",
    "# get the test accuracy score for the best estimator\n",
    "print(\"Test score:\", rec.test_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please speak into the microphone\n",
      "done\n",
      "Prediction: happy\n"
     ]
    }
   ],
   "source": [
    "os.system(\"say \"+ \"Please speak into the microphone\")\n",
    "print(\"Please speak into the microphone\")\n",
    "record_to_file('emotion.wav')\n",
    "print(\"done\")\n",
    "prediction = rec.predict('emotion.wav')\n",
    "print(f\"Prediction: {prediction}\")\n",
    "reply_emotion(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    deeprec = DeepEmotionRecognizer(emotions=['sad', 'happy'],\n",
    "                                    n_rnn_layers=2, n_dense_layers=3, rnn_units=256, dense_units=128,\n",
    "                                    dropout=0.35,epochs=500)\n",
    "    # train the model\n",
    "    deeprec.train(override=False)\n",
    "    # get the accuracy\n",
    "    print(deeprec.test_score())\n",
    "\n",
    "    print(\"Test accuracy score: {:.3f}%\".format(deeprec.test_score()*100))\n",
    "#     print(\"Please talk\")\n",
    "    \n",
    "    \n",
    "#     filename = \"test.wav\"\n",
    "#     record_to_file(filename)\n",
    "#     result = deeprec.predict(filename)\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
