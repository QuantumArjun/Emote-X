{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from concat_project7.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import butter, sosfiltfilt\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import osascript\n",
    "from gtts import gTTS \n",
    "from matplotlib import pylab\n",
    "import os \n",
    "import pyaudio\n",
    "import wave\n",
    "import keyboard  # using module keyboard\n",
    "import soundfile as sf\n",
    "import math\n",
    "import pyloudnorm as pyln\n",
    "from sys import byteorder\n",
    "from array import array\n",
    "from struct import pack\n",
    "import import_ipynb\n",
    "import concat_project7\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 100\n",
    "CHUNK_SIZE = 2048\n",
    "FORMAT = pyaudio.paInt16\n",
    "RATE = 44100\n",
    "AVG_STEP = 75\n",
    "PERCENTILE = 70\n",
    "BANDPASS_FREQ = [300, 3400]\n",
    "CALIBRATION_VOLUME = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record from microphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record():\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=FORMAT, channels=1, rate=RATE,\n",
    "        input=True, output=True,\n",
    "        frames_per_buffer=CHUNK_SIZE)\n",
    "\n",
    "    num_silent = 0\n",
    "    snd_started = False\n",
    "\n",
    "    r = array('h')\n",
    "\n",
    "    while 1:\n",
    "        # little endian, signed short\n",
    "        snd_data = array('h', stream.read(CHUNK_SIZE))\n",
    "        if byteorder == 'big':\n",
    "            snd_data.byteswap()\n",
    "        r.extend(snd_data)\n",
    "\n",
    "        silent = is_silent(snd_data)\n",
    "\n",
    "        if silent and snd_started:\n",
    "            num_silent += 1\n",
    "        elif not silent and not snd_started:\n",
    "            snd_started = True\n",
    "\n",
    "        if snd_started and num_silent > 60:\n",
    "            break\n",
    "\n",
    "    sample_width = p.get_sample_size(FORMAT)\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "#     r = trim(r)\n",
    "#     r = add_silence(r, 0.5)\n",
    "    return sample_width, r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to wav file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_to_file(path):\n",
    "    sample_width, data = record()\n",
    "    data = pack('<' + ('h'*len(data)), *data)\n",
    "\n",
    "    wf = wave.open(path, 'wb')\n",
    "    wf.setnchannels(1)\n",
    "    wf.setsampwidth(sample_width)\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(data)\n",
    "    wf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks if input is silent based on threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_silent(snd_data):\n",
    "    \"Returns 'True' if below the 'silent' threshold\"\n",
    "    return max(snd_data) < THRESHOLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trim input to remove silence at beginning and end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(snd_data):\n",
    "    def _trim(snd_data):\n",
    "        snd_started = False\n",
    "        r = array('h')\n",
    "\n",
    "        for i in snd_data:\n",
    "            if not snd_started and abs(i)>THRESHOLD:\n",
    "                snd_started = True\n",
    "                r.append(i)\n",
    "            elif snd_started:\n",
    "                r.append(i)\n",
    "        return r\n",
    "\n",
    "    # Trim to the left\n",
    "    snd_data = _trim(snd_data)\n",
    "\n",
    "    # Trim to the right\n",
    "    snd_data.reverse()\n",
    "    snd_data = _trim(snd_data)\n",
    "    snd_data.reverse()\n",
    "    return snd_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pad with silence at beginning and end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_silence(snd_data, seconds):\n",
    "    \"\"\n",
    "    r = array('h', [0 for i in range(int(seconds*RATE))])\n",
    "    r.extend(snd_data)\n",
    "    r.extend([0 for i in range(int(seconds*RATE))])\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process wav input with highpass filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voice_input(voice_data):\n",
    "    fs, data = wavfile.read(voice_data)\n",
    "    data = butter_bandpass_filter(data, BANDPASS_FREQ[0], BANDPASS_FREQ[1], fs)\n",
    "    return data, fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement butter bandpass filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    sos = butter(order, [low, high], btype='band', analog=False, output='sos')\n",
    "    return sos\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    sos = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = sosfiltfilt(sos, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get amplitude curve and apply regression on ITU-R BS.1770-4 loudness algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_emotion(data, rate):\n",
    "\n",
    "    \n",
    "#     lol_data, fs = concat_project7.clean_sound(data, rate)\n",
    "\n",
    "    #Convert raw data into features\n",
    "    print(\"Data acquired\")\n",
    "\n",
    "    feature_data = concat_project7.get_all_features(data, fs);\n",
    "    \n",
    "    #return the exponential regression\n",
    "    return (feature_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove silence before, after, and in-between voice data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_silence_from(amplitudes, threshold):\n",
    "    silenced = []\n",
    "    for x in amplitudes:\n",
    "        if x >= threshold:\n",
    "            silenced.append(x)\n",
    "    return silenced# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression converts loudness to scale 0 to 100 and adjusts for calibration offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(loudness, offset=0):\n",
    "    return 2.1256619652039683 * loudness + 52.66548772500079 - offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offset(calib_loudness):\n",
    "    return calib_loudness - CALIBRATION_VOLUME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sets a new volume that is spoken at for the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reply(user_volume):\n",
    "    speaker_volume = int(osascript.osascript('output volume of (get volume settings)')[1])\n",
    "    print(\"speaker volume is \" + str(speaker_volume))\n",
    "    print(\"user volume is \"+ str(user_volume))\n",
    "    if(speaker_volume < user_volume):\n",
    "        new_volume = speaker_volume + (((speaker_volume - user_volume)**2) /100)\n",
    "    else:\n",
    "        new_volume = speaker_volume - (((speaker_volume - user_volume)**2) /100)\n",
    "    print(\"speaker new volume is \" + str(new_volume))\n",
    "    osascript.osascript(\"set volume output volume \"+ str(new_volume)) \n",
    "    thing_to_say = 'hello your volume is at '+ str(int(new_volume))\n",
    "    print(thing_to_say)\n",
    "    os.system(\"say \"+ thing_to_say)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_to_emotion(array):\n",
    "    emotion = ''\n",
    "    if (array[0] == [1]):\n",
    "        emotion = 'Angry \\n'\n",
    "    if (array[1] == [1]):\n",
    "        emotion = 'Disgust \\n'\n",
    "    if (array[2] == [1]):\n",
    "        emotion += 'Fear \\n'\n",
    "\n",
    "    if (array[3] == [1]):\n",
    "        emotion = 'Happy \\n'\n",
    "\n",
    "    if (array[4] == [1]):\n",
    "        emotion = 'Neutral \\n'\n",
    "\n",
    "    if (array[5] == [1]):\n",
    "        emotion = 'Surprised'\n",
    "    \n",
    "    if (array[6] == [1]):\n",
    "        emotion = 'Sad'\n",
    "    if (array[1] == [1]):\n",
    "        emotion = 'Disgust \\n'\n",
    "\n",
    "    return emotion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = load('mlp_for_demo.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done inputing voice\n",
      "Data acquired\n",
      "MFCC DONE dimensions are [1, 91]-----------------------------] 0.0% ...Calculating MFCC's\n",
      "MFCC dimensions:[1, 91]--------------------------------------] 0.0% ...Calculating stats\n",
      "Pitch dimensions:[1, 8]\n",
      "Spectral dimensions:[1, 8]\n",
      "RMS dimensions:[1, 8]\n",
      "SR dimensions:[1, 8]\n",
      "Zero dimensions:[1, 8]\n",
      "Disgust \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "if __name__ == '__main__':\n",
    "    #calibration\n",
    "#     os.system(\"say \"+ \"Please speak into the microphone\")\n",
    "#     print(\"Please speak into the microphone\")\n",
    "#     record_to_file('emotion.wav')\n",
    "#     print(\"done\")\n",
    "    fs, data = wavfile.read('../../TESS/YAF_food_disgust.wav')\n",
    "#     data, rate = voice_input('emotion.wav')\n",
    "    print('done inputing voice')\n",
    "    \n",
    "    voice_data = getting_emotion(np.asarray([data]), fs)\n",
    "    \n",
    "    predictions = clf.predict(voice_data)[0]\n",
    "    print(one_hot_to_emotion(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done inputing voice\n",
      "Data acquired\n",
      "MFCC DONE dimensions are [1, 91]-----------------------------] 0.0% ...Calculating MFCC's\n",
      "MFCC dimensions:[1, 91]--------------------------------------] 0.0% ...Calculating stats\n",
      "Pitch dimensions:[1, 8]\n",
      "Spectral dimensions:[1, 8]\n",
      "RMS dimensions:[1, 8]\n",
      "SR dimensions:[1, 8]\n",
      "Zero dimensions:[1, 8]\n",
      "Disgust \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "if __name__ == '__main__':\n",
    "    #calibration\n",
    "#     os.system(\"say \"+ \"Please speak into the microphone\")\n",
    "#     print(\"Please speak into the microphone\")\n",
    "#     record_to_file('emotion.wav')\n",
    "#     print(\"done\")\n",
    "    fs, data = wavfile.read('../../TESS/YAF_ring_.wav')\n",
    "#     data, rate = voice_input('emotion.wav')\n",
    "    print('done inputing voice')\n",
    "    \n",
    "    voice_data = getting_emotion(np.asarray([data]), fs)\n",
    "    \n",
    "    predictions = clf.predict(voice_data)[0]\n",
    "    print(one_hot_to_emotion(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disgust \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(one_hot_to_emotion(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'?\n",
    "?\n",
    ".'\"\"''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
