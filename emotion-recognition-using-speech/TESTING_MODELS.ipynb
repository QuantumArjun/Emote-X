{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import butter, sosfiltfilt\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import osascript\n",
    "from gtts import gTTS \n",
    "from matplotlib import pylab\n",
    "import os \n",
    "import pyaudio\n",
    "import wave\n",
    "import keyboard  # using module keyboard\n",
    "import soundfile as sf\n",
    "import math\n",
    "import pyloudnorm as pyln\n",
    "from sys import byteorder\n",
    "from array import array\n",
    "from struct import pack\n",
    "import import_ipynb\n",
    "import concat_project7\n",
    "from joblib import dump, load\n",
    "from emotion_recognition import EmotionRecognizer\n",
    "from sklearn.svm import SVC\n",
    "from deep_emotion_recognition import DeepEmotionRecognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 100\n",
    "CHUNK_SIZE = 2048\n",
    "FORMAT = pyaudio.paInt16\n",
    "RATE = 16000\n",
    "AVG_STEP = 75\n",
    "PERCENTILE = 70\n",
    "BANDPASS_FREQ = [300, 3400]\n",
    "CALIBRATION_VOLUME = 40\n",
    "ALL_FEATURES_DERIV = ['mfcc', 'chroma', 'mel', 'contrast', 'tonnetz', 'rmse', 'deriv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record from microphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record():\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=FORMAT, channels=1, rate=RATE,\n",
    "        input=True, output=True,\n",
    "        frames_per_buffer=CHUNK_SIZE)\n",
    "\n",
    "    num_silent = 0\n",
    "    snd_started = False\n",
    "\n",
    "    r = array('h')\n",
    "\n",
    "    while 1:\n",
    "        # little endian, signed short\n",
    "        snd_data = array('h', stream.read(CHUNK_SIZE))\n",
    "        if byteorder == 'big':\n",
    "            snd_data.byteswap()\n",
    "        r.extend(snd_data)\n",
    "\n",
    "        silent = is_silent(snd_data)\n",
    "\n",
    "        if silent and snd_started:\n",
    "            num_silent += 1\n",
    "        elif not silent and not snd_started:\n",
    "            snd_started = True\n",
    "\n",
    "        if snd_started and num_silent > 60:\n",
    "            break\n",
    "\n",
    "    sample_width = p.get_sample_size(FORMAT)\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "#     r = trim(r)\n",
    "#     r = add_silence(r, 0.5)\n",
    "    return sample_width, r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to wav file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_to_file(path):\n",
    "    sample_width, data = record()\n",
    "    data = pack('<' + ('h'*len(data)), *data)\n",
    "\n",
    "    wf = wave.open(path, 'wb')\n",
    "    wf.setnchannels(1)\n",
    "    wf.setsampwidth(sample_width)\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(data)\n",
    "    wf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks if input is silent based on threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_silent(snd_data):\n",
    "    \"Returns 'True' if below the 'silent' threshold\"\n",
    "    return max(snd_data) < THRESHOLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trim input to remove silence at beginning and end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(snd_data):\n",
    "    def _trim(snd_data):\n",
    "        snd_started = False\n",
    "        r = array('h')\n",
    "\n",
    "        for i in snd_data:\n",
    "            if not snd_started and abs(i)>THRESHOLD:\n",
    "                snd_started = True\n",
    "                r.append(i)\n",
    "            elif snd_started:\n",
    "                r.append(i)\n",
    "        return r\n",
    "\n",
    "    # Trim to the left\n",
    "    snd_data = _trim(snd_data)\n",
    "\n",
    "    # Trim to the right\n",
    "    snd_data.reverse()\n",
    "    snd_data = _trim(snd_data)\n",
    "    snd_data.reverse()\n",
    "    return snd_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pad with silence at beginning and end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_silence(snd_data, seconds):\n",
    "    \"\"\n",
    "    r = array('h', [0 for i in range(int(seconds*RATE))])\n",
    "    r.extend(snd_data)\n",
    "    r.extend([0 for i in range(int(seconds*RATE))])\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process wav input with highpass filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voice_input(voice_data):\n",
    "    fs, data = wavfile.read(voice_data)\n",
    "    data = butter_bandpass_filter(data, BANDPASS_FREQ[0], BANDPASS_FREQ[1], fs)\n",
    "    return data, fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement butter bandpass filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "#     nyq = 0.5 * fs\n",
    "#     low = lowcut / nyq\n",
    "#     high = highcut / nyq\n",
    "#     sos = butter(order, [low, high], btype='band', analog=False, output='sos')\n",
    "#     return sos\n",
    "\n",
    "# def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "#     sos = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "#     y = sosfiltfilt(sos, data)\n",
    "#     return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get amplitude curve and apply regression on ITU-R BS.1770-4 loudness algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getting_emotion(data, rate):\n",
    "\n",
    "    \n",
    "# #     lol_data, fs = concat_project7.clean_sound(data, rate)\n",
    "\n",
    "#     #Convert raw data into features\n",
    "#     print(\"Data acquired\")\n",
    "\n",
    "#     feature_data = concat_project7.get_all_features(data, fs);\n",
    "    \n",
    "#     #return the exponential regression\n",
    "#     return (feature_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove silence before, after, and in-between voice data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_silence_from(amplitudes, threshold):\n",
    "    silenced = []\n",
    "    for x in amplitudes:\n",
    "        if x >= threshold:\n",
    "            silenced.append(x)\n",
    "    return silenced# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression converts loudness to scale 0 to 100 and adjusts for calibration offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_offset(calib_loudness):\n",
    "#     return calib_loudness - CALIBRATION_VOLUME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sets a new volume that is spoken at for the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reply_volume(user_volume):\n",
    "    speaker_volume = int(osascript.osascript('output volume of (get volume settings)')[1])\n",
    "    print(\"speaker volume is \" + str(speaker_volume))\n",
    "    print(\"user volume is \"+ str(user_volume))\n",
    "    if(speaker_volume < user_volume):\n",
    "        new_volume = speaker_volume + (((speaker_volume - user_volume)**2) /100)\n",
    "    else:\n",
    "        new_volume = speaker_volume - (((speaker_volume - user_volume)**2) /100)\n",
    "    print(\"speaker new volume is \" + str(new_volume))\n",
    "    osascript.osascript(\"set volume output volume \"+ str(new_volume)) \n",
    "    thing_to_say = 'hello your volume is at '+ str(int(new_volume))\n",
    "    print(thing_to_say)\n",
    "    os.system(\"say \"+ thing_to_say)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reply_emotion(user_emotion):\n",
    "    os.system(\"say \"+\" are you feeling \"+user_emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "## ALL EMOTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0814 13:14:02.562827 140735684912000 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0814 13:14:02.575743 140735684912000 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0814 13:14:02.577932 140735684912000 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TESS&RAVDESS] There are 812 training audio files for category:sad\n",
      "[TESS&RAVDESS] There are 146 testing audio files for category:sad\n",
      "[TESS&RAVDESS] There are 585 training audio files for category:neutral\n",
      "[TESS&RAVDESS] There are 93 testing audio files for category:neutral\n",
      "[TESS&RAVDESS] There are 805 training audio files for category:happy\n",
      "[TESS&RAVDESS] There are 147 testing audio files for category:happy\n",
      "[TESS&RAVDESS] There are 808 training audio files for category:angry\n",
      "[TESS&RAVDESS] There are 147 testing audio files for category:angry\n",
      "[TESS&RAVDESS] There are 511 training audio files for category:disgust\n",
      "[TESS&RAVDESS] There are 79 testing audio files for category:disgust\n",
      "[TESS&RAVDESS] There are 817 training audio files for category:fear\n",
      "[TESS&RAVDESS] There are 141 testing audio files for category:fear\n",
      "[+] Writed TESS & RAVDESS DB CSV File\n",
      "[EMO-DB] Total files to write: 454\n",
      "[EMO-DB] Training samples: 363\n",
      "[EMO-DB] Testing samples: 90\n",
      "[+] Writed EMO-DB CSV File\n",
      "[+] Writed Custom DB CSV File\n",
      "[+] Data loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0814 13:14:02.824571 140735684912000 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0814 13:14:02.831129 140735684912000 deprecation.py:506] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0814 13:14:03.280225 140735684912000 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Model created\n",
      "[*] Model weights loaded\n",
      "0.7619047619047619\n",
      "              predicted_sad  predicted_neutral  predicted_happy  \\\n",
      "true_sad          83.516487           1.098901         1.098901   \n",
      "true_neutral      23.076923          65.934067         0.000000   \n",
      "true_happy         6.593407           1.098901        71.428574   \n",
      "true_angry         0.000000           0.000000         2.197802   \n",
      "true_disgust       8.791209           1.098901         1.098901   \n",
      "true_fear         16.483517           0.000000        10.989011   \n",
      "\n",
      "              predicted_angry  predicted_disgust  predicted_fear  \n",
      "true_sad             0.000000           7.692308        6.593407  \n",
      "true_neutral         3.296704           5.494505        2.197802  \n",
      "true_happy           3.296704           6.593407       10.989011  \n",
      "true_angry          87.912086           6.593407        3.296704  \n",
      "true_disgust         0.000000          85.714287        3.296704  \n",
      "true_fear            4.395605           5.494505       62.637363  \n"
     ]
    }
   ],
   "source": [
    "deeprec = DeepEmotionRecognizer(emotions=['sad', 'neutral', 'happy', 'angry', 'disgust', 'fear'],\n",
    "                                n_rnn_layers=2, n_dense_layers=3, rnn_units=256, dense_units=128,\n",
    "                                dropout=0.35,epochs=500)\n",
    "# train the model\n",
    "deeprec.train(override=False)\n",
    "# get the accuracy\n",
    "print(deeprec.test_score())\n",
    "print(deeprec.confusion_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"say \"+ \"Please speak into the microphone\")\n",
    "print(\"Please speak into the microphone\")\n",
    "record_to_file('emotion.wav')\n",
    "print(\"done\")\n",
    "prediction = deeprec.predict('emotion.wav')\n",
    "print(f\"Prediction: {prediction}\")\n",
    "reply_emotion(prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HAPPY ANGRY SAD CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Model trained\n",
      "Test score: 0.7967479674796748\n",
      "Train score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from emotion_recognition import EmotionRecognizer\n",
    "from sklearn.svm import SVC\n",
    "# init a model, let's use SVC\n",
    "my_model = SVC()\n",
    "# pass my model to EmotionRecognizer instance\n",
    "# and balance the dataset\n",
    "rec = EmotionRecognizer(model=my_model, emotions=['angry', 'happy', 'sad'], balance=True, verbose=0)\n",
    "# train the model\n",
    "rec.train()\n",
    "# check the test accuracy for that model\n",
    "print(\"Test score:\", rec.test_score())\n",
    "# check the train accuracy for that model\n",
    "print(\"Train score:\", rec.train_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier is the best\n",
      "Test score: 0.8943089430894309\n"
     ]
    }
   ],
   "source": [
    "# loads the best estimators from `grid` folder that was searched by GridSearchCV in `grid_search.py`,\n",
    "# and set the model to the best in terms of test score, and then train it\n",
    "rec.determine_best_model(train=True)\n",
    "# get the determined sklearn model name\n",
    "print(rec.model.__class__.__name__, \"is the best\")\n",
    "# get the test accuracy score for the best estimator\n",
    "print(\"Test score:\", rec.test_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.system(\"say \"+ \"Please speak into the microphone\")\n",
    "# print(\"Please speak into the microphone\")\n",
    "# record_to_file('emotion.wav')\n",
    "# print(\"done\")\n",
    "# prediction = rec.predict('emotion.wav')\n",
    "# print(f\"Prediction: {prediction}\")\n",
    "# reply_emotion(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emotion_recognition import EmotionRecognizer\n",
    "\n",
    "import pyaudio\n",
    "import os\n",
    "import wave\n",
    "from sys import byteorder\n",
    "from array import array\n",
    "from struct import pack\n",
    "from sklearn.ensemble import GradientBoostingClassifier, BaggingClassifier\n",
    "\n",
    "from utils import get_best_estimators\n",
    "\n",
    "THRESHOLD = 500\n",
    "CHUNK_SIZE = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "RATE = 16000\n",
    "\n",
    "SILENCE = 30\n",
    "\n",
    "def is_silent(snd_data):\n",
    "    \"Returns 'True' if below the 'silent' threshold\"\n",
    "    return max(snd_data) < THRESHOLD\n",
    "\n",
    "def normalize(snd_data):\n",
    "    \"Average the volume out\"\n",
    "    MAXIMUM = 16384\n",
    "    times = float(MAXIMUM)/max(abs(i) for i in snd_data)\n",
    "\n",
    "    r = array('h')\n",
    "    for i in snd_data:\n",
    "        r.append(int(i*times))\n",
    "    return r\n",
    "\n",
    "def trim(snd_data):\n",
    "    \"Trim the blank spots at the start and end\"\n",
    "    def _trim(snd_data):\n",
    "        snd_started = False\n",
    "        r = array('h')\n",
    "\n",
    "        for i in snd_data:\n",
    "            if not snd_started and abs(i)>THRESHOLD:\n",
    "                snd_started = True\n",
    "                r.append(i)\n",
    "\n",
    "            elif snd_started:\n",
    "                r.append(i)\n",
    "        return r\n",
    "\n",
    "    # Trim to the left\n",
    "    snd_data = _trim(snd_data)\n",
    "\n",
    "    # Trim to the right\n",
    "    snd_data.reverse()\n",
    "    snd_data = _trim(snd_data)\n",
    "    snd_data.reverse()\n",
    "    return snd_data\n",
    "\n",
    "def add_silence(snd_data, seconds):\n",
    "    \"Add silence to the start and end of 'snd_data' of length 'seconds' (float)\"\n",
    "    r = array('h', [0 for i in range(int(seconds*RATE))])\n",
    "    r.extend(snd_data)\n",
    "    r.extend([0 for i in range(int(seconds*RATE))])\n",
    "    return r\n",
    "\n",
    "def record():\n",
    "    \"\"\"\n",
    "    Record a word or words from the microphone and \n",
    "    return the data as an array of signed shorts.\n",
    "\n",
    "    Normalizes the audio, trims silence from the \n",
    "    start and end, and pads with 0.5 seconds of \n",
    "    blank sound to make sure VLC et al can play \n",
    "    it without getting chopped off.\n",
    "    \"\"\"\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=FORMAT, channels=1, rate=RATE,\n",
    "        input=True, output=True,\n",
    "        frames_per_buffer=CHUNK_SIZE)\n",
    "\n",
    "    num_silent = 0\n",
    "    snd_started = False\n",
    "\n",
    "    r = array('h')\n",
    "\n",
    "    while 1:\n",
    "        # little endian, signed short\n",
    "        snd_data = array('h', stream.read(CHUNK_SIZE))\n",
    "        if byteorder == 'big':\n",
    "            snd_data.byteswap()\n",
    "        r.extend(snd_data)\n",
    "\n",
    "        silent = is_silent(snd_data)\n",
    "\n",
    "        if silent and snd_started:\n",
    "            num_silent += 1\n",
    "        elif not silent and not snd_started:\n",
    "            snd_started = True\n",
    "\n",
    "        if snd_started and num_silent > SILENCE:\n",
    "            break\n",
    "\n",
    "    sample_width = p.get_sample_size(FORMAT)\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    r = normalize(r)\n",
    "    r = trim(r)\n",
    "    r = add_silence(r, 0.5)\n",
    "    return sample_width, r\n",
    "\n",
    "def record_to_file(path):\n",
    "    \"Records from the microphone and outputs the resulting data to 'path'\"\n",
    "    sample_width, data = record()\n",
    "    data = pack('<' + ('h'*len(data)), *data)\n",
    "\n",
    "    wf = wave.open(path, 'wb')\n",
    "    wf.setnchannels(1)\n",
    "    wf.setsampwidth(sample_width)\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(data)\n",
    "    wf.close()\n",
    "\n",
    "\n",
    "def get_estimators_name(estimators):\n",
    "    result = [ '\"{}\"'.format(estimator.__class__.__name__) for estimator, _, _ in estimators ]\n",
    "    return ','.join(result), {estimator_name.strip('\"'): estimator for estimator_name, (estimator, _, _) in zip(result, estimators)}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFAULT FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP REC NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TESS&RAVDESS] There are 508 training audio files for category:sad\n",
      "[TESS&RAVDESS] There are 82 testing audio files for category:sad\n",
      "[TESS&RAVDESS] There are 501 training audio files for category:happy\n",
      "[TESS&RAVDESS] There are 83 testing audio files for category:happy\n",
      "[+] Writed TESS & RAVDESS DB CSV File\n",
      "[EMO-DB] Total files to write: 133\n",
      "[EMO-DB] Training samples: 106\n",
      "[EMO-DB] Testing samples: 26\n",
      "[+] Writed EMO-DB CSV File\n",
      "[+] Writed Custom DB CSV File\n",
      "[+] Data loaded\n",
      "[+] Model created\n",
      "[*] Model weights loaded\n",
      "0.8528645833333334\n",
      "Test accuracy score: 85.286%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    HS_deep = DeepEmotionRecognizer(emotions=['sad', 'happy'],\n",
    "                                    n_rnn_layers=2, n_dense_layers=3, rnn_units=128, dense_units=128,\n",
    "                                    dropout=0.35,epochs=500)\n",
    "    # train the model\n",
    "    HS_deep.train(override=False)\n",
    "    # get the accuracy\n",
    "    print(HS_deep.test_score())\n",
    "\n",
    "    print(\"Test accuracy score: {:.3f}%\".format(HS_deep.test_score()*100))\n",
    "#     print(\"Please talk\")\n",
    "    \n",
    "    \n",
    "#     filename = \"test.wav\"\n",
    "#     record_to_file(filename)\n",
    "#     result = deeprec.predict(filename)\n",
    "#     print(result)\n",
    "# NOTES\n",
    "# look for happy vs anger\n",
    "# api to remove other people voice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please speak into the microphone\n",
      "done\n",
      "Prediction: happy\n",
      "happy confidence: 98.06010127067566%\n",
      "sad confidence: 1.9398996606469154%\n"
     ]
    }
   ],
   "source": [
    "## os.system(\"say \"+ \"Please speak into the microphone\")\n",
    "print(\"Please speak into the microphone\")\n",
    "record_to_file('emotion.wav')\n",
    "print(\"done\")\n",
    "prediction, confidence = HS_deep.predict('emotion.wav')\n",
    "print(f\"Prediction: {prediction}\")\n",
    "reply_emotion(prediction)\n",
    "print(\"happy confidence: \" + str(confidence[0][0][1]*100) + \"%\")\n",
    "print(\"sad confidence: \" + str(confidence[0][0][0]*100) + \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANGRY NEUTRAL CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features for train:   0%|          | 2/3078 [00:00<02:33, 19.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TESS&RAVDESS] There are 433 training audio files for category:neutral\n",
      "[TESS&RAVDESS] There are 61 testing audio files for category:neutral\n",
      "[TESS&RAVDESS] There are 504 training audio files for category:angry\n",
      "[TESS&RAVDESS] There are 83 testing audio files for category:angry\n",
      "[+] Writed TESS & RAVDESS DB CSV File\n",
      "[EMO-DB] Total files to write: 206\n",
      "[EMO-DB] Training samples: 164\n",
      "[EMO-DB] Testing samples: 41\n",
      "[+] Writed EMO-DB CSV File\n",
      "[+] Writed Custom DB CSV File\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features for train: 100%|██████████| 3078/3078 [01:16<00:00, 40.19it/s]\n",
      "Extracting features for test: 100%|██████████| 697/697 [00:17<00:00, 44.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Data loaded\n",
      "[+] Model created\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/500\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.8452 - acc: 0.5052 - val_loss: 0.8316 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.83160, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.8729 - acc: 0.4990 - val_loss: 0.7223 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.83160 to 0.72229, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.7551 - acc: 0.5190 - val_loss: 0.6863 - val_acc: 0.5616\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.72229 to 0.68627, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.7255 - acc: 0.5121 - val_loss: 0.7006 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.68627\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.7376 - acc: 0.5059 - val_loss: 0.7034 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.68627\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.7233 - acc: 0.5163 - val_loss: 0.6930 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.68627\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.7229 - acc: 0.5028 - val_loss: 0.6820 - val_acc: 0.5029\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.68627 to 0.68205, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.7102 - acc: 0.5235 - val_loss: 0.6767 - val_acc: 0.6730\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.68205 to 0.67673, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6948 - acc: 0.5335 - val_loss: 0.6760 - val_acc: 0.5792\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.67673 to 0.67604, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6951 - acc: 0.5432 - val_loss: 0.6754 - val_acc: 0.5499\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.67604 to 0.67544, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6972 - acc: 0.5304 - val_loss: 0.6719 - val_acc: 0.5616\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.67544 to 0.67194, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6858 - acc: 0.5567 - val_loss: 0.6657 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.67194 to 0.66569, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6782 - acc: 0.5643 - val_loss: 0.6585 - val_acc: 0.6833\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.66569 to 0.65846, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6719 - acc: 0.5906 - val_loss: 0.6522 - val_acc: 0.6877\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.65846 to 0.65223, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6641 - acc: 0.5930 - val_loss: 0.6468 - val_acc: 0.6554\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.65223 to 0.64682, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6474 - acc: 0.6262 - val_loss: 0.6398 - val_acc: 0.6408\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.64682 to 0.63980, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6446 - acc: 0.6131 - val_loss: 0.6291 - val_acc: 0.6598\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.63980 to 0.62911, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6353 - acc: 0.6383 - val_loss: 0.6150 - val_acc: 0.6686\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.62911 to 0.61505, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6149 - acc: 0.6698 - val_loss: 0.5990 - val_acc: 0.7214\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.61505 to 0.59896, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6055 - acc: 0.6826 - val_loss: 0.5829 - val_acc: 0.7331\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.59896 to 0.58286, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.5858 - acc: 0.7216 - val_loss: 0.5668 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.58286 to 0.56682, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.5764 - acc: 0.7023 - val_loss: 0.5498 - val_acc: 0.7419\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.56682 to 0.54977, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.5503 - acc: 0.7410 - val_loss: 0.5335 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.54977 to 0.53348, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.5347 - acc: 0.7431 - val_loss: 0.5213 - val_acc: 0.7507\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.53348 to 0.52127, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.5081 - acc: 0.7583 - val_loss: 0.5061 - val_acc: 0.7581\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.52127 to 0.50609, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.4905 - acc: 0.7860 - val_loss: 0.4898 - val_acc: 0.7742\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.50609 to 0.48977, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.4696 - acc: 0.7960 - val_loss: 0.4775 - val_acc: 0.7874\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.48977 to 0.47748, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.4540 - acc: 0.7998 - val_loss: 0.4683 - val_acc: 0.7889\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.47748 to 0.46834, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.4386 - acc: 0.8071 - val_loss: 0.4620 - val_acc: 0.7874\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.46834 to 0.46197, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.4145 - acc: 0.8233 - val_loss: 0.4556 - val_acc: 0.7962\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.46197 to 0.45559, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 31/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step - loss: 0.4060 - acc: 0.8257 - val_loss: 0.4471 - val_acc: 0.8109\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.45559 to 0.44715, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.3988 - acc: 0.8340 - val_loss: 0.4375 - val_acc: 0.8152\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.44715 to 0.43753, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.3765 - acc: 0.8454 - val_loss: 0.4302 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.43753 to 0.43018, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.3624 - acc: 0.8548 - val_loss: 0.4193 - val_acc: 0.8255\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.43018 to 0.41934, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.3537 - acc: 0.8558 - val_loss: 0.4109 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.41934 to 0.41086, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.3468 - acc: 0.8696 - val_loss: 0.4022 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.41086 to 0.40217, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.3247 - acc: 0.8696 - val_loss: 0.3953 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.40217 to 0.39533, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.3162 - acc: 0.8797 - val_loss: 0.3894 - val_acc: 0.8255\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.39533 to 0.38940, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2936 - acc: 0.8862 - val_loss: 0.3836 - val_acc: 0.8284\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.38940 to 0.38359, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2905 - acc: 0.8835 - val_loss: 0.3790 - val_acc: 0.8328\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.38359 to 0.37896, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2989 - acc: 0.8897 - val_loss: 0.3751 - val_acc: 0.8299\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.37896 to 0.37514, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2875 - acc: 0.8866 - val_loss: 0.3749 - val_acc: 0.8387\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.37514 to 0.37494, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2792 - acc: 0.8935 - val_loss: 0.3736 - val_acc: 0.8372\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.37494 to 0.37362, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2749 - acc: 0.8963 - val_loss: 0.3695 - val_acc: 0.8314\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.37362 to 0.36949, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2817 - acc: 0.8876 - val_loss: 0.3637 - val_acc: 0.8314\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.36949 to 0.36372, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2746 - acc: 0.8959 - val_loss: 0.3567 - val_acc: 0.8328\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.36372 to 0.35666, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2679 - acc: 0.8911 - val_loss: 0.3497 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.35666 to 0.34967, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2618 - acc: 0.8966 - val_loss: 0.3428 - val_acc: 0.8460\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.34967 to 0.34282, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2587 - acc: 0.8973 - val_loss: 0.3359 - val_acc: 0.8460\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.34282 to 0.33591, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2500 - acc: 0.8976 - val_loss: 0.3307 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.33591 to 0.33068, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2588 - acc: 0.8932 - val_loss: 0.3261 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.33068 to 0.32613, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2473 - acc: 0.9053 - val_loss: 0.3229 - val_acc: 0.8475\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.32613 to 0.32287, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2417 - acc: 0.9028 - val_loss: 0.3217 - val_acc: 0.8519\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.32287 to 0.32169, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2399 - acc: 0.9004 - val_loss: 0.3213 - val_acc: 0.8548\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.32169 to 0.32131, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2350 - acc: 0.9046 - val_loss: 0.3213 - val_acc: 0.8563\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.32131 to 0.32128, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2351 - acc: 0.9073 - val_loss: 0.3218 - val_acc: 0.8563\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.32128\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2260 - acc: 0.9101 - val_loss: 0.3209 - val_acc: 0.8578\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.32128 to 0.32092, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2265 - acc: 0.9077 - val_loss: 0.3182 - val_acc: 0.8592\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.32092 to 0.31823, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2234 - acc: 0.9087 - val_loss: 0.3170 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.31823 to 0.31702, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2239 - acc: 0.9094 - val_loss: 0.3144 - val_acc: 0.8651\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.31702 to 0.31441, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 61/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step - loss: 0.2264 - acc: 0.9115 - val_loss: 0.3117 - val_acc: 0.8680\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.31441 to 0.31170, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2176 - acc: 0.9160 - val_loss: 0.3080 - val_acc: 0.8695\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.31170 to 0.30804, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2286 - acc: 0.9073 - val_loss: 0.3029 - val_acc: 0.8724\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.30804 to 0.30293, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2087 - acc: 0.9170 - val_loss: 0.2964 - val_acc: 0.8724\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.30293 to 0.29635, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2086 - acc: 0.9174 - val_loss: 0.2925 - val_acc: 0.8739\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.29635 to 0.29251, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2188 - acc: 0.9108 - val_loss: 0.2931 - val_acc: 0.8783\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.29251\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2066 - acc: 0.9201 - val_loss: 0.2948 - val_acc: 0.8827\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.29251\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2040 - acc: 0.9201 - val_loss: 0.2946 - val_acc: 0.8798\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.29251\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.1959 - acc: 0.9253 - val_loss: 0.2971 - val_acc: 0.8754\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.29251\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.1967 - acc: 0.9212 - val_loss: 0.2995 - val_acc: 0.8724\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.29251\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.1935 - acc: 0.9187 - val_loss: 0.3029 - val_acc: 0.8724\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.29251\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.1965 - acc: 0.9219 - val_loss: 0.3068 - val_acc: 0.8695\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.29251\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.1886 - acc: 0.9267 - val_loss: 0.3095 - val_acc: 0.8754\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.29251\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.1850 - acc: 0.9263 - val_loss: 0.3078 - val_acc: 0.8724\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.29251\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.1765 - acc: 0.9312 - val_loss: 0.3057 - val_acc: 0.8724\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.29251\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.1828 - acc: 0.9267 - val_loss: 0.3049 - val_acc: 0.8768\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.29251\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.1804 - acc: 0.9298 - val_loss: 0.3029 - val_acc: 0.8827\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.29251\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.1728 - acc: 0.9371 - val_loss: 0.3045 - val_acc: 0.8856\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.29251\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.1728 - acc: 0.9253 - val_loss: 0.3034 - val_acc: 0.8856\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.29251\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.1823 - acc: 0.9315 - val_loss: 0.2980 - val_acc: 0.8856\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.29251\n",
      "Epoch 81/500\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    NA_deep = DeepEmotionRecognizer(emotions=['neutral', 'angry'],\n",
    "                                    n_rnn_layers=2, n_dense_layers=3, rnn_units=128, dense_units=128,\n",
    "                                    dropout=0.35,epochs=500)\n",
    "    # train the model\n",
    "    NA_deep.train(override=True)\n",
    "    # get the accuracy\n",
    "    print(NA_deep.test_score())\n",
    "\n",
    "    print(\"Test accuracy score: {:.3f}%\".format(NA_deep.test_score()*100))\n",
    "#     print(\"Please talk\")\n",
    "    \n",
    "    \n",
    "#     filename = \"test.wav\"\n",
    "#     record_to_file(filename)\n",
    "#     result = deeprec.predict(filename)\n",
    "#     print(result)\n",
    "# NOTES\n",
    "# look for happy vs anger\n",
    "# api to remove other people voice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please speak into the microphone\n",
      "done\n",
      "Prediction: happy\n",
      "happy confidence: 79.44456338882446%\n",
      "sad confidence: 20.539531111717224%\n"
     ]
    }
   ],
   "source": [
    "## os.system(\"say \"+ \"Please speak into the microphone\")\n",
    "print(\"Please speak into the microphone\")\n",
    "record_to_file('emotion.wav')\n",
    "print(\"done\")\n",
    "prediction, confidence = NA_deep.predict('emotion.wav')\n",
    "print(f\"Prediction: {prediction}\")\n",
    "reply_emotion(prediction)\n",
    "print(\"happy confidence: \" + str(confidence[0][0][1]*100) + \"%\")\n",
    "print(\"sad confidence: \" + str(confidence[0][0][0]*100) + \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TESS&RAVDESS] There are 812 training audio files for category:sad\n",
      "[TESS&RAVDESS] There are 146 testing audio files for category:sad\n",
      "[TESS&RAVDESS] There are 585 training audio files for category:neutral\n",
      "[TESS&RAVDESS] There are 93 testing audio files for category:neutral\n",
      "[TESS&RAVDESS] There are 805 training audio files for category:happy\n",
      "[TESS&RAVDESS] There are 147 testing audio files for category:happy\n",
      "[+] Writed TESS & RAVDESS DB CSV File\n",
      "[EMO-DB] Total files to write: 212\n",
      "[EMO-DB] Training samples: 169\n",
      "[EMO-DB] Testing samples: 42\n",
      "[+] Writed EMO-DB CSV File\n",
      "[+] Writed Custom DB CSV File\n",
      "[+] Data loaded\n",
      "[+] Model created\n",
      "[*] Model weights loaded\n",
      "0.7552742616033755\n",
      "Test accuracy score: 75.527%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    deeprec = DeepEmotionRecognizer(emotions=['sad', 'neutral', 'happy'],\n",
    "                                    n_rnn_layers=2, n_dense_layers=3, rnn_units=128, dense_units=256,\n",
    "                                    dropout=0.3,epochs=500)\n",
    "    # train the model\n",
    "    deeprec.train(override=False)\n",
    "    # get the accuracy\n",
    "    print(deeprec.test_score())\n",
    "\n",
    "    print(\"Test accuracy score: {:.3f}%\".format(deeprec.test_score()*100))\n",
    "#     print(\"Please talk\")\n",
    "    \n",
    "    \n",
    "#     filename = \"test.wav\"\n",
    "#     record_to_file(filename)\n",
    "#     result = deeprec.predict(filename)\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please speak into the microphone\n",
      "done\n",
      "Prediction: neutral\n",
      "netrual confidence: 83.63749980926514%\n",
      "sad confidence: 3.679598867893219%\n",
      "happy confidence: 12.682893872261047%\n"
     ]
    }
   ],
   "source": [
    "os.system(\"say \"+ \"Please speak into the microphone\")\n",
    "print(\"Please speak into the microphone\")\n",
    "record_to_file('emotion.wav')\n",
    "print(\"done\")\n",
    "prediction, confidence = deeprec.predict('emotion.wav')\n",
    "print(f\"Prediction: {prediction}\")\n",
    "reply_emotion(prediction)\n",
    "print(\"netrual confidence: \" + str(confidence[0][0][1]*100) + \"%\")\n",
    "print(\"sad confidence: \" + str(confidence[0][0][0]*100) + \"%\")\n",
    "print(\"happy confidence: \" + str(confidence[0][0][2]*100) + \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRA FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TESS&RAVDESS] There are 508 training audio files for category:sad\n",
      "[TESS&RAVDESS] There are 82 testing audio files for category:sad\n",
      "[TESS&RAVDESS] There are 501 training audio files for category:happy\n",
      "[TESS&RAVDESS] There are 83 testing audio files for category:happy\n",
      "[+] Writed TESS & RAVDESS DB CSV File\n",
      "[EMO-DB] Total files to write: 133\n",
      "[EMO-DB] Training samples: 106\n",
      "[EMO-DB] Testing samples: 26\n",
      "[+] Writed EMO-DB CSV File\n",
      "[+] Writed Custom DB CSV File\n",
      "[+] Data loaded\n",
      "[+] Model created\n",
      "[*] Model weights loaded\n",
      "0.8528645833333334\n",
      "Test accuracy score: 85.286%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    HS_deep = DeepEmotionRecognizer(emotions=['sad', 'happy'],\n",
    "                                    n_rnn_layers=2, n_dense_layers=3, rnn_units=128, dense_units=128,\n",
    "                                    features = ALL_FEATURES_DERIV,\n",
    "                                    dropout=0.35,epochs=500)\n",
    "    # train the model\n",
    "    HS_deep.train(override=False)\n",
    "    # get the accuracy\n",
    "    print(HS_deep.test_score())\n",
    "\n",
    "    print(\"Test accuracy score: {:.3f}%\".format(HS_deep.test_score()*100))\n",
    "#     print(\"Please talk\")\n",
    "    \n",
    "    \n",
    "#     filename = \"test.wav\"\n",
    "#     record_to_file(filename)\n",
    "#     result = deeprec.predict(filename)\n",
    "#     print(result)\n",
    "# NOTES\n",
    "# look for happy vs anger\n",
    "# api to remove other people voice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please speak into the microphone\n",
      "done\n",
      "Prediction: happy\n",
      "happy confidence: 98.06010127067566%\n",
      "sad confidence: 1.9398996606469154%\n"
     ]
    }
   ],
   "source": [
    "## os.system(\"say \"+ \"Please speak into the microphone\")\n",
    "print(\"Please speak into the microphone\")\n",
    "record_to_file('emotion.wav')\n",
    "print(\"done\")\n",
    "prediction, confidence = HS_deep.predict('emotion.wav')\n",
    "print(f\"Prediction: {prediction}\")\n",
    "reply_emotion(prediction)\n",
    "print(\"happy confidence: \" + str(confidence[0][0][1]*100) + \"%\")\n",
    "print(\"sad confidence: \" + str(confidence[0][0][0]*100) + \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANGRY NEUTRAL CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features for train:   0%|          | 2/3078 [00:00<02:33, 19.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TESS&RAVDESS] There are 433 training audio files for category:neutral\n",
      "[TESS&RAVDESS] There are 61 testing audio files for category:neutral\n",
      "[TESS&RAVDESS] There are 504 training audio files for category:angry\n",
      "[TESS&RAVDESS] There are 83 testing audio files for category:angry\n",
      "[+] Writed TESS & RAVDESS DB CSV File\n",
      "[EMO-DB] Total files to write: 206\n",
      "[EMO-DB] Training samples: 164\n",
      "[EMO-DB] Testing samples: 41\n",
      "[+] Writed EMO-DB CSV File\n",
      "[+] Writed Custom DB CSV File\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features for train: 100%|██████████| 3078/3078 [01:16<00:00, 40.19it/s]\n",
      "Extracting features for test: 100%|██████████| 697/697 [00:17<00:00, 44.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Data loaded\n",
      "[+] Model created\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/500\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.8452 - acc: 0.5052 - val_loss: 0.8316 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.83160, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.8729 - acc: 0.4990 - val_loss: 0.7223 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.83160 to 0.72229, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.7551 - acc: 0.5190 - val_loss: 0.6863 - val_acc: 0.5616\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.72229 to 0.68627, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.7255 - acc: 0.5121 - val_loss: 0.7006 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.68627\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.7376 - acc: 0.5059 - val_loss: 0.7034 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.68627\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.7233 - acc: 0.5163 - val_loss: 0.6930 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.68627\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.7229 - acc: 0.5028 - val_loss: 0.6820 - val_acc: 0.5029\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.68627 to 0.68205, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.7102 - acc: 0.5235 - val_loss: 0.6767 - val_acc: 0.6730\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.68205 to 0.67673, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6948 - acc: 0.5335 - val_loss: 0.6760 - val_acc: 0.5792\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.67673 to 0.67604, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6951 - acc: 0.5432 - val_loss: 0.6754 - val_acc: 0.5499\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.67604 to 0.67544, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6972 - acc: 0.5304 - val_loss: 0.6719 - val_acc: 0.5616\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.67544 to 0.67194, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6858 - acc: 0.5567 - val_loss: 0.6657 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.67194 to 0.66569, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6782 - acc: 0.5643 - val_loss: 0.6585 - val_acc: 0.6833\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.66569 to 0.65846, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6719 - acc: 0.5906 - val_loss: 0.6522 - val_acc: 0.6877\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.65846 to 0.65223, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6641 - acc: 0.5930 - val_loss: 0.6468 - val_acc: 0.6554\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.65223 to 0.64682, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6474 - acc: 0.6262 - val_loss: 0.6398 - val_acc: 0.6408\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.64682 to 0.63980, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6446 - acc: 0.6131 - val_loss: 0.6291 - val_acc: 0.6598\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.63980 to 0.62911, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6353 - acc: 0.6383 - val_loss: 0.6150 - val_acc: 0.6686\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.62911 to 0.61505, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6149 - acc: 0.6698 - val_loss: 0.5990 - val_acc: 0.7214\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.61505 to 0.59896, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6055 - acc: 0.6826 - val_loss: 0.5829 - val_acc: 0.7331\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.59896 to 0.58286, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.5858 - acc: 0.7216 - val_loss: 0.5668 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.58286 to 0.56682, saving model to results/AN-c-LSTM-layers-2-3-units-128-128-dropout-0.35_0.35_0.35_0.35_0.35.h5\n",
      "Epoch 22/500\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    NA_deep = DeepEmotionRecognizer(emotions=['neutral', 'angry'],\n",
    "                                    n_rnn_layers=2, n_dense_layers=3, rnn_units=128, dense_units=128,\n",
    "                                    features = ALL_FEATURES_DERIV,\n",
    "                                    dropout=0.35,epochs=500)\n",
    "    # train the model\n",
    "    NA_deep.train(override=True)\n",
    "    # get the accuracy\n",
    "    print(NA_deep.test_score())\n",
    "\n",
    "    print(\"Test accuracy score: {:.3f}%\".format(NA_deep.test_score()*100))\n",
    "#     print(\"Please talk\")\n",
    "    \n",
    "    \n",
    "#     filename = \"test.wav\"\n",
    "#     record_to_file(filename)\n",
    "#     result = deeprec.predict(filename)\n",
    "#     print(result)\n",
    "# NOTES\n",
    "# look for happy vs anger\n",
    "# api to remove other people voice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please speak into the microphone\n",
      "done\n",
      "Prediction: happy\n",
      "happy confidence: 79.44456338882446%\n",
      "sad confidence: 20.539531111717224%\n"
     ]
    }
   ],
   "source": [
    "## os.system(\"say \"+ \"Please speak into the microphone\")\n",
    "print(\"Please speak into the microphone\")\n",
    "record_to_file('emotion.wav')\n",
    "print(\"done\")\n",
    "prediction, confidence = NA_deep.predict('emotion.wav')\n",
    "print(f\"Prediction: {prediction}\")\n",
    "reply_emotion(prediction)\n",
    "print(\"happy confidence: \" + str(confidence[0][0][1]*100) + \"%\")\n",
    "print(\"sad confidence: \" + str(confidence[0][0][0]*100) + \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Model trained\n",
      "Test score: 0.8028455284552846\n",
      "Train score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from emotion_recognition import EmotionRecognizer\n",
    "from sklearn.svm import SVC\n",
    "# init a model, let's use SVC\n",
    "my_model = SVC()\n",
    "# pass my model to EmotionRecognizer instance\n",
    "# and balance the dataset\n",
    "rec = EmotionRecognizer(model=my_model, emotions=['sad', 'happy', 'angry'], balance=True, verbose=0)\n",
    "# train the model\n",
    "rec.train()\n",
    "# check the test accuracy for that model\n",
    "print(\"Test score:\", rec.test_score())\n",
    "# check the train accuracy for that model\n",
    "print(\"Train score:\", rec.train_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier is the best\n",
      "Test score: 0.9065040650406504\n"
     ]
    }
   ],
   "source": [
    "# loads the best estimators from `grid` folder that was searched by GridSearchCV in `grid_search.py`,\n",
    "# and set the model to the best in terms of test score, and then train it\n",
    "rec.determine_best_model(train=True)\n",
    "# get the determined sklearn model name\n",
    "print(rec.model.__class__.__name__, \"is the best\")\n",
    "# get the test accuracy score for the best estimator\n",
    "print(\"Test score:\", rec.test_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please speak into the microphone\n",
      "done\n",
      "Prediction: happy\n"
     ]
    }
   ],
   "source": [
    "os.system(\"say \"+ \"Please speak into the microphone\")\n",
    "print(\"Please speak into the microphone\")\n",
    "record_to_file('emotion.wav')\n",
    "print(\"done\")\n",
    "prediction = rec.predict('emotion.wav')\n",
    "print(f\"Prediction: {prediction}\")\n",
    "reply_emotion(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    deeprec = DeepEmotionRecognizer(emotions=['sad', 'happy'],\n",
    "                                    n_rnn_layers=2, n_dense_layers=3, rnn_units=256, dense_units=128,\n",
    "                                    dropout=0.35,epochs=500)\n",
    "    # train the model\n",
    "    deeprec.train(override=False)\n",
    "    # get the accuracy\n",
    "    print(deeprec.test_score())\n",
    "\n",
    "    print(\"Test accuracy score: {:.3f}%\".format(deeprec.test_score()*100))\n",
    "#     print(\"Please talk\")\n",
    "    \n",
    "    \n",
    "#     filename = \"test.wav\"\n",
    "#     record_to_file(filename)\n",
    "#     result = deeprec.predict(filename)\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
