{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import emotion_recognition\n",
    "from utils import get_audio_config\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense, Conv2D, Conv1D, Convolution2D, concatenate, LSTM, Reshape\n",
    "from keras.layers import Dropout, Flatten\n",
    "from keras.models import Sequential, Model\n",
    "import numpy as np\n",
    "from keras.utils import plot_model\n",
    "import create_csv\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURES TO BE USED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_desc_files = ['train_custom.csv', 'train_emo.csv', 'train_tess_ravdess.csv']\n",
    "test_desc_files = ['test_custom.csv', 'test_emo.csv', 'test_tess_ravdess.csv']\n",
    "ALL_FEATURES = ['mfcc', 'chroma', 'mel', 'contrast', 'tonnetz', 'rmse']\n",
    "image_features = ['mel']\n",
    "non_image_features = ['contrast', 'rmse', 'mfcc', 'tonnetz']\n",
    "emotions = ['happy', 'angry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EMO-DB] Total files to write: 251\n",
      "[EMO-DB] Training samples: 200\n",
      "[EMO-DB] Testing samples: 50\n",
      "[TESS&RAVDESS] There are 655 training audio files for category:happy\n",
      "[TESS&RAVDESS] There are 115 testing audio files for category:happy\n",
      "[TESS&RAVDESS] There are 661 training audio files for category:angry\n",
      "[TESS&RAVDESS] There are 115 testing audio files for category:angry\n"
     ]
    }
   ],
   "source": [
    "create_csv.write_emodb_csv(emotions=emotions, train_name=\"train_emo.csv\",\n",
    "                    test_name=\"test_emo.csv\", train_size=0.8, verbose=1)\n",
    "create_csv.write_tess_ravdess_csv(emotions=emotions, train_name=\"train_tess_ravdess.csv\",\n",
    "                            test_name=\"test_tess_ravdess.csv\", verbose=1)\n",
    "create_csv.write_custom_csv(emotions=emotions, train_name=\"train_custom.csv\", test_name=\"test_custom.csv\",\n",
    "                    verbose=1)\n",
    "int2emotions = {i: e for i, e in enumerate(emotions)}\n",
    "emotions2int = {v: k for k, v in int2emotions.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACTING FEATURES (IMAGE AND NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_config = get_audio_config(non_image_features)\n",
    "data_flat = emotion_recognition.load_data(train_desc_files, test_desc_files, audio_config, classification=True,\n",
    "                                emotions=emotions, balance=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_config = get_audio_config(['image'])\n",
    "data_image = emotion_recognition.load_data(train_desc_files, test_desc_files, audio_config, classification=True,\n",
    "                                emotions=emotions, balance=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3802, 128, 1412), (3802, 54))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_image[\"X_train\"].shape, data_flat[\"X_train\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3802, 1, 54, 1), (3802, 128, 1412, 1), (3802, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_flat = data_flat[\"X_train\"].reshape(data_flat[\"X_train\"].shape[0], 1,  54, 1)\n",
    "X_train_image =  data_image[\"X_train\"].reshape(data_image[\"X_train\"].shape[0], 128, 1412, 1)\n",
    "X_test_flat = data_flat[\"X_test\"].reshape(data_flat[\"X_test\"].shape[0], 1,  54, 1)\n",
    "X_test_image =  data_image[\"X_test\"].reshape(data_image[\"X_test\"].shape[0], 128, 1412, 1)\n",
    "y_train = to_categorical([emotions2int[str(e)] for e in data_image['y_train'].squeeze() ])\n",
    "y_test =  to_categorical([emotions2int[str(e)] for e in data_image['y_test'].squeeze() ])\n",
    "X_train_flat.shape, X_train_image.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([431., 431.], dtype=float32), array([1901., 1901.], dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_test, axis =0), np.sum(y_train, axis =0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL HYPER PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model constants\n",
    "n_rnn_layers = 2\n",
    "n_rnn_layers -=1\n",
    "rnn_units = 128\n",
    "dropout = 0.35\n",
    "n_dense_layers = 2\n",
    "dense_units = 64\n",
    "output_dims = len(emotions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv2d_10/Relu:0\", shape=(?, 1, 54, 64), dtype=float32)\n",
      "Tensor(\"input_4:0\", shape=(?, 1, 54, 1), dtype=float32)\n",
      "here\n",
      "Tensor(\"concatenate_2/concat:0\", shape=(?, 1, 54, 65), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0828 17:45:02.843023 140736179348352 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0828 17:45:02.849240 140736179348352 deprecation.py:506] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0828 17:45:03.154213 140736179348352 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0828 17:45:03.178338 140736179348352 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RAMS MODEL\n",
    "input_cnn = Input(shape = (128, 1412, 1))\n",
    "CNN = Conv2D(8, (12, 13), activation='relu', strides=(1,13),\n",
    "             input_shape = (128, 1412, 1))(input_cnn)\n",
    "CNN = Conv2D(16, (16, 2), activation='relu', strides=(1,2))(CNN)\n",
    "CNN = Conv2D(32, (24, 1), activation='relu', strides=(1,1) )(CNN)\n",
    "CNN = Conv2D(48, (32, 1), activation='relu', strides=(1,1))(CNN)\n",
    "CNN = Conv2D(64, (48, 1), activation='relu', strides=(1,1))(CNN)\n",
    "\n",
    "\n",
    "CNN = Model(inputs=input_cnn, outputs=CNN)\n",
    "\n",
    "input_rnn = Input(shape = (1, 54, 1))\n",
    "RNN = Model(inputs=input_rnn, outputs=input_rnn)\n",
    "\n",
    "print(CNN.output)\n",
    "print(RNN.output)\n",
    "RAMS = concatenate([CNN.output, RNN.output])\n",
    "print(\"here\")\n",
    "print(RAMS)\n",
    "\n",
    "\n",
    "RAMS = Reshape((54, 65))(RAMS)\n",
    "RAMS = (LSTM(rnn_units, return_sequences=True, input_shape=(48, 129)))(RAMS)\n",
    "RAMS = (Dropout(dropout))(RAMS)\n",
    "\n",
    "# rnn layers\n",
    "for i in range(n_rnn_layers):\n",
    "    RAMS = LSTM(rnn_units, return_sequences=True)(RAMS)\n",
    "    RAMS = (Dropout(dropout))(RAMS)\n",
    "RAMS = Flatten()(RAMS)\n",
    "# dense layers\n",
    "RAMS = (Dense(dense_units, activation=\"relu\"))(RAMS)\n",
    "RAMS = (Dropout(dropout))(RAMS)\n",
    "for j in range(n_dense_layers):\n",
    "    RAMS = (Dense(dense_units, activation=\"relu\"))(RAMS)\n",
    "    RAMS = (Dropout(dropout))(RAMS)\n",
    "RAMS = (Dense(output_dims, activation=\"softmax\"))(RAMS)\n",
    "\n",
    "\n",
    "model = Model(inputs = [input_cnn, input_rnn], outputs = RAMS)\n",
    "model.compile(loss='categorical_crossentropy', metrics=[\"accuracy\"], optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 128, 1412, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 117, 108, 8)  1256        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 102, 54, 16)  4112        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 79, 54, 32)   12320       conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 48, 54, 48)   49200       conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 1, 54, 64)    147520      conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1, 54, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1, 54, 65)    0           conv2d_10[0][0]                  \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 54, 65)       0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 54, 128)      99328       reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 54, 128)      0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 54, 128)      131584      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 54, 128)      0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 6912)         0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           442432      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 64)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           4160        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 64)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           4160        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 64)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2)            130         dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 896,202\n",
      "Trainable params: 896,202\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "plot_model(model, to_file='model.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "epochs = 500\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0828 17:48:01.965946 140736179348352 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3802 samples, validate on 862 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0828 17:48:04.575776 140736179348352 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "W0828 17:48:04.576835 140736179348352 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "3802/3802 [==============================] - 116s 30ms/step - loss: 0.6985 - acc: 0.5092 - val_loss: 0.6744 - val_acc: 0.6265\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.67443, saving model to RAMS_trial_1\n",
      "Epoch 2/500\n",
      "3802/3802 [==============================] - 110s 29ms/step - loss: 0.6729 - acc: 0.5776 - val_loss: 0.6367 - val_acc: 0.6636\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.67443 to 0.63671, saving model to RAMS_trial_1\n",
      "Epoch 3/500\n",
      "3802/3802 [==============================] - 116s 30ms/step - loss: 0.6410 - acc: 0.6260 - val_loss: 0.6055 - val_acc: 0.6914\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63671 to 0.60551, saving model to RAMS_trial_1\n",
      "Epoch 4/500\n",
      "2048/3802 [===============>..............] - ETA: 51s - loss: 0.6159 - acc: 0.6655 "
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(\"RAMS_trial_1\", save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=f\"logs/RAMS_trial_1\")\n",
    "\n",
    "history = model.fit([X_train_image, X_train_flat], y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                validation_data=([X_test_image, X_test_flat], y_test),\n",
    "                callbacks=[checkpointer, tensorboard],\n",
    "                verbose=verbose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(audio_path):    \n",
    "    image_audio_config = \n",
    "    flat_audio_config = \n",
    "    image_feature = extract_feature(audio_path, **image_audio_config).reshape(1, 1,  54, 1)\n",
    "    flat_feature = extract_feature(audio_path, **flat_audio_config).reshape(1,128, 1412, 1)\n",
    "    return self.int2emotions[self.model.predict_classes(feature)[0][0]], self.model.predict(feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
