{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "import scipy\n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import osascript\n",
    "from gtts import gTTS \n",
    "import os \n",
    "import pyaudio\n",
    "import wave\n",
    "import keyboard  # using module keyboard\n",
    "import soundfile as sf\n",
    "import math\n",
    "import pyloudnorm as pyln\n",
    "from sys import byteorder\n",
    "from array import array\n",
    "from struct import pack\n",
    "import librosa\n",
    "from scipy.signal import butter, sosfiltfilt\n",
    "import pysptk\n",
    "import python_speech_features\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "import time\n",
    "import sherpa\n",
    "import sherpa.algorithms.bayesian_optimization as bayesian_optimization\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "import keras as keras\n",
    "from  conch.analysis.formants import lpc\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras import regularizers\n",
    "import librosa.display \n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fs = 16000\n",
    "# dataset, y, useless_number = np.load(\"../../CREMA_chunked.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = MFCC_algorithm(dataset, fs)\n",
    "# X = X.reshape(X.shape[0], 50, 13, 1)\n",
    "# np.save(\"X_BIG_SPEC.npy\", X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"X_BIG_SPEC.npy\")\n",
    "y= np.load('Y_BOG_SPEC.npy')\n",
    "X = X.reshape(X.shape[0], 128,257,1)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train shape: (32499, 128, 257, 1)\n",
      "y train shape: (32499, 12)\n",
      "x test shape: (4063, 128, 257, 1)\n",
      "y test shape: (4063, 12)\n"
     ]
    }
   ],
   "source": [
    "print(\"x train shape: \" +str(X_train.shape))\n",
    "print(\"y train shape: \" +str(y_train.shape))\n",
    "print(\"x test shape: \" +str(X_test.shape))\n",
    "print(\"y test shape: \" +str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediumV4(X_train, X_test, y_train, y_test, regularization = 0.1,drop_likely = 0.1,\n",
    "              learning_rate = 0.001, beta1 = 0.9, beta2 = 0.999,\n",
    "             num_iterations = 100, size_batch=64,activation = 'relu'):\n",
    "    \n",
    "    target_class = y_train.shape[1]\n",
    "    print(target_class)\n",
    "    model = Sequential()\n",
    "    \n",
    "    \n",
    "    model.add(Conv2D(16, kernel_size = 3, activation=activation, input_shape = (128,257,1)))\n",
    "    print(model.input_shape)\n",
    "    model.add(Conv2D(16, kernel_size = 3, activation=activation))\n",
    "    model.add(MaxPooling2D(pool_size = 3, strides = (1,1)))\n",
    "    model.add(Dropout(0.1))\n",
    "        \n",
    "    model.add(Conv2D(32, kernel_size = 3, activation=activation))\n",
    "    model.add(Conv2D(32, kernel_size = 3, activation=activation))\n",
    "    model.add(MaxPooling2D(pool_size = (3,3), strides = (1,1)))\n",
    "    model.add(Dropout(0.1))\n",
    "        \n",
    "    model.add(Conv2D(128, kernel_size = (3,3), activation=activation))\n",
    "    model.add(keras.layers.GlobalMaxPool2D())\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    \n",
    "    model.add(Dense(128, activation=activation))\n",
    "    model.add(Dense(64, activation=activation))\n",
    "    \n",
    "    model.add(Dense(target_class, activation='softmax'))\n",
    "    \n",
    "    opt = keras.optimizers.Adam(lr=learning_rate, beta_1=beta1, beta_2=beta2, epsilon=10e-8)\n",
    "       \n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=opt,metrics=[keras.metrics.categorical_accuracy])\n",
    "    \n",
    "    print(model.summary())\n",
    "    nn_history = model.fit(X_train, y_train, batch_size=size_batch, epochs=num_iterations, \n",
    "                         validation_data=(X_test, y_test),\n",
    "                          )\n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mediumV4(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROMISING\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras import Input, layers\n",
    "from tensorflow.keras import backend as K\n",
    "def mediumV5Cond1d(X_train, X_test, y_train, y_test, regularization = 0.1,drop_likely = 0.1,\n",
    "              learning_rate = 0.001, beta1 = 0.9, beta2 = 0.999,\n",
    "             num_iterations = 100, size_batch=64,activation = 'relu'):\n",
    "    \n",
    "    target_class = y_train.shape[1]\n",
    "    input_shape = X_train.shape[1:]\n",
    "    input_tensor = layers.Input(shape=(input_shape))\n",
    "\n",
    "    x = layers.Conv1D(8, 11, padding='valid', activation='relu', strides=1)(input_tensor)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(drop_out_rate)(x)\n",
    "    x = layers.Conv1D(16, 7, padding='valid', activation='relu', strides=1)(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(drop_out_rate)(x)\n",
    "    x = layers.Conv1D(32, 5, padding='valid', activation='relu', strides=1)(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(drop_out_rate)(x)\n",
    "    x = layers.Conv1D(64, 5, padding='valid', activation='relu', strides=1)(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(drop_out_rate)(x)\n",
    "    x = layers.Conv1D(128, 3, padding='valid', activation='relu', strides=1)(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(drop_out_rate)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(drop_out_rate)(x)\n",
    "    output_tensor = layers.Dense(target_class, activation='softmax')(x)\n",
    "    model = tf.keras.Model(input_tensor, output_tensor)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                 optimizer=keras.optimizers.Adam(lr = lr),\n",
    "                 metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9fdf7e3a78c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmediumV5Cond1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "mediumV5Cond1d(X_train, X_test, y_train, y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with device('/cpu:0'):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40624, 128, 257, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0731 09:43:58.140693 140735549535104 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0731 09:43:58.143024 140735549535104 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0731 09:43:58.152976 140735549535104 deprecation.py:506] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0731 09:43:58.457551 140735549535104 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "(None, 128, 257, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0731 09:43:58.468502 140735549535104 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0731 09:43:58.642163 140735549535104 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 126, 251, 16)      352       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 124, 245, 16)      5392      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 122, 239, 16)      0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 122, 239, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 120, 237, 32)      4640      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 118, 235, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 116, 233, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 116, 233, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 114, 231, 128)     36992     \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 12)                1548      \n",
      "=================================================================\n",
      "Total params: 91,196\n",
      "Trainable params: 91,196\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 32499 samples, validate on 4062 samples\n",
      "Epoch 1/100\n",
      "32499/32499 [==============================] - 4076s 125ms/step - loss: 2.4387 - categorical_accuracy: 0.1229 - val_loss: 2.3655 - val_categorical_accuracy: 0.1465\n",
      "Epoch 2/100\n",
      "32499/32499 [==============================] - 3843s 118ms/step - loss: 2.3250 - categorical_accuracy: 0.1647 - val_loss: 2.3064 - val_categorical_accuracy: 0.1777\n",
      "Epoch 3/100\n",
      "32499/32499 [==============================] - 3828s 118ms/step - loss: 2.2823 - categorical_accuracy: 0.1800 - val_loss: 2.3222 - val_categorical_accuracy: 0.1699\n",
      "Epoch 4/100\n",
      "32499/32499 [==============================] - 3830s 118ms/step - loss: 2.2565 - categorical_accuracy: 0.1896 - val_loss: 2.2470 - val_categorical_accuracy: 0.1987\n",
      "Epoch 5/100\n",
      "32499/32499 [==============================] - 3800s 117ms/step - loss: 2.1878 - categorical_accuracy: 0.2138 - val_loss: 2.2268 - val_categorical_accuracy: 0.2043\n",
      "Epoch 6/100\n",
      "32499/32499 [==============================] - 3792s 117ms/step - loss: 2.1343 - categorical_accuracy: 0.2315 - val_loss: 2.2444 - val_categorical_accuracy: 0.1844\n",
      "Epoch 7/100\n",
      "32499/32499 [==============================] - 3906s 120ms/step - loss: 2.1075 - categorical_accuracy: 0.2380 - val_loss: 2.2556 - val_categorical_accuracy: 0.1928\n",
      "Epoch 8/100\n",
      "32499/32499 [==============================] - 3856s 119ms/step - loss: 2.0871 - categorical_accuracy: 0.2417 - val_loss: 2.2002 - val_categorical_accuracy: 0.2048\n",
      "Epoch 9/100\n",
      "32499/32499 [==============================] - 3861s 119ms/step - loss: 2.0653 - categorical_accuracy: 0.2495 - val_loss: 2.1978 - val_categorical_accuracy: 0.2043\n",
      "Epoch 10/100\n",
      "32499/32499 [==============================] - 3855s 119ms/step - loss: 2.0665 - categorical_accuracy: 0.2494 - val_loss: 2.1755 - val_categorical_accuracy: 0.2063\n",
      "Epoch 11/100\n",
      "32499/32499 [==============================] - 3824s 118ms/step - loss: 2.0617 - categorical_accuracy: 0.2499 - val_loss: 2.1596 - val_categorical_accuracy: 0.2208\n",
      "Epoch 12/100\n",
      "32499/32499 [==============================] - 3877s 119ms/step - loss: 2.0436 - categorical_accuracy: 0.2548 - val_loss: 2.1828 - val_categorical_accuracy: 0.2075\n",
      "Epoch 13/100\n",
      "32499/32499 [==============================] - 3864s 119ms/step - loss: 2.0436 - categorical_accuracy: 0.2571 - val_loss: 2.1615 - val_categorical_accuracy: 0.2053\n",
      "Epoch 14/100\n",
      "32499/32499 [==============================] - 3841s 118ms/step - loss: 2.0253 - categorical_accuracy: 0.2611 - val_loss: 2.1719 - val_categorical_accuracy: 0.2223\n",
      "Epoch 15/100\n",
      "32499/32499 [==============================] - 3846s 118ms/step - loss: 2.0309 - categorical_accuracy: 0.2630 - val_loss: 2.2480 - val_categorical_accuracy: 0.1881\n",
      "Epoch 16/100\n",
      "32499/32499 [==============================] - 3859s 119ms/step - loss: 2.0233 - categorical_accuracy: 0.2653 - val_loss: 2.1286 - val_categorical_accuracy: 0.2260\n",
      "Epoch 17/100\n",
      "32499/32499 [==============================] - 3852s 119ms/step - loss: 2.0177 - categorical_accuracy: 0.2702 - val_loss: 2.1027 - val_categorical_accuracy: 0.2366\n",
      "Epoch 18/100\n",
      "32499/32499 [==============================] - 3821s 118ms/step - loss: 2.0143 - categorical_accuracy: 0.2678 - val_loss: 2.1144 - val_categorical_accuracy: 0.2339\n",
      "Epoch 19/100\n",
      "32499/32499 [==============================] - 3821s 118ms/step - loss: 2.0089 - categorical_accuracy: 0.2693 - val_loss: 2.1619 - val_categorical_accuracy: 0.2120\n",
      "Epoch 20/100\n",
      "32499/32499 [==============================] - 3801s 117ms/step - loss: 2.0037 - categorical_accuracy: 0.2675 - val_loss: 2.1037 - val_categorical_accuracy: 0.2319\n",
      "Epoch 21/100\n",
      "32499/32499 [==============================] - 3843s 118ms/step - loss: 1.9905 - categorical_accuracy: 0.2714 - val_loss: 2.1252 - val_categorical_accuracy: 0.2307\n",
      "Epoch 22/100\n",
      "32499/32499 [==============================] - 3834s 118ms/step - loss: 1.9918 - categorical_accuracy: 0.2724 - val_loss: 2.1371 - val_categorical_accuracy: 0.2280\n",
      "Epoch 23/100\n",
      "32499/32499 [==============================] - 3947s 121ms/step - loss: 1.9944 - categorical_accuracy: 0.2710 - val_loss: 2.1246 - val_categorical_accuracy: 0.2277\n",
      "Epoch 24/100\n",
      "32499/32499 [==============================] - 4082s 126ms/step - loss: 1.9907 - categorical_accuracy: 0.2765 - val_loss: 2.2715 - val_categorical_accuracy: 0.1827\n",
      "Epoch 25/100\n",
      "32499/32499 [==============================] - 4152s 128ms/step - loss: 2.0008 - categorical_accuracy: 0.2722 - val_loss: 2.2467 - val_categorical_accuracy: 0.1945\n",
      "Epoch 26/100\n",
      "32499/32499 [==============================] - 4063s 125ms/step - loss: 1.9843 - categorical_accuracy: 0.2783 - val_loss: 2.0863 - val_categorical_accuracy: 0.2442\n",
      "Epoch 27/100\n",
      "32499/32499 [==============================] - 3966s 122ms/step - loss: 1.9860 - categorical_accuracy: 0.2787 - val_loss: 2.0867 - val_categorical_accuracy: 0.2366\n",
      "Epoch 28/100\n",
      "32499/32499 [==============================] - 3832s 118ms/step - loss: 1.9877 - categorical_accuracy: 0.2741 - val_loss: 2.0813 - val_categorical_accuracy: 0.2258\n",
      "Epoch 29/100\n",
      "30208/32499 [==========================>...] - ETA: 4:30 - loss: 1.9687 - categorical_accuracy: 0.2805"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8d5c9eaa65cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmediumV4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-f01c367d7ca7>\u001b[0m in \u001b[0;36mmediumV4\u001b[0;34m(X_train, X_test, y_train, y_test, regularization, drop_likely, learning_rate, beta1, beta2, num_iterations, size_batch, activation)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     nn_history = model.fit(X_train, y_train, batch_size=size_batch, epochs=num_iterations, \n\u001b[0;32m---> 38\u001b[0;31m                          \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m                           )\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mediumV4(X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vggish_google_transfer_learning(X_train, X_test, y_train, y_test, regularization = 0.1,drop_likely = 0.1,\n",
    "              learning_rate = 0.001, beta1 = 0.9, beta2 = 0.999,\n",
    "             num_iterations = 40, size_batch=64,activation = 'relu'):\n",
    "    \n",
    "    target_class = y_train.shape[1]\n",
    "    # Model \n",
    "    model = Sequential()\n",
    "    \n",
    "    \n",
    "        \n",
    "    model.add(Conv2D(16, kernel_size = (3,7), activation=activation, input_shape = (157,320,1)))\n",
    "    model.add(Conv2D(16, kernel_size = (3,7), activation=activation))\n",
    "    model.add(MaxPooling2D(pool_size = (3,7), strides = (1,1)))\n",
    "    model.add(Dropout(0.1))\n",
    "        \n",
    "    model.add(Conv2D(32, kernel_size = 3, activation=activation))\n",
    "    model.add(Conv2D(32, kernel_size = 3, activation=activation))\n",
    "    model.add(MaxPooling2D(pool_size = (3,3), strides = (1,1)))\n",
    "    model.add(Dropout(0.1))\n",
    "        \n",
    "    model.add(Conv2D(128, kernel_size = (3,3), activation=activation))\n",
    "    model.add(GlobalMaxPool2D())\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    \n",
    "    model.add(Dense(128, activation=activation))\n",
    "    model.add(Dense(128, activation=activation))\n",
    "    \n",
    "    model.add(Dense(target_class, activation='softnax'))\n",
    "    \n",
    "    opt = keras.optimizers.Adam(lr=learning_rate, beta_1=beta1, beta_2=beta2, epsilon=10e-8)\n",
    "       \n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=opt,metrics=[keras.metrics.categorical_accuracy])\n",
    "    \n",
    "    nn_history = model.fit(X_train, y_train, batch_size=size_batch, epochs=num_iterations, \n",
    "                         validation_data=(X_test, y_test),\n",
    "                          callbacks=[es])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_MFCC_LSTM(X_train, X_test, y_train, y_test, regularization = 0.1,drop_likely = 0.1,\n",
    "              learning_rate = 0.001, beta1 = 0.9, beta2 = 0.999,\n",
    "             num_iterations = 40, size_batch=64,activation = 'relu'):\n",
    "    # Set the target class number\n",
    "    \n",
    "    # VGG 19\n",
    "    \n",
    "    target_class = y_train.shape[1]\n",
    "    # Model \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(keras.layers.BatchNormalization())    \n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size = (3,3), activation=activation, input_shape = (50,13,1)))\n",
    "    model.add(Conv2D(64, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (1,1)))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(Conv2D(128, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (1,1)))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(256, kernel_size = (3,3), activation=activation))\n",
    "    model.add(Conv2D(256, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(Conv2D(256, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(Conv2D(256, kernel_size = (3,3), activation=activation))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (1,1)))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    \n",
    "    model.add(Conv2D(512, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(Conv2D(512, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(Conv2D(512, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(Conv2D(512, kernel_size = (3,3), activation=activation))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(Flatten())\n",
    "\n",
    "    \n",
    "    \n",
    "    model.add(Dense(4096, activation=activation))\n",
    "    model.add(Dense(4096, activation=activation))\n",
    "    model.add(Dense(256, activation=activation))\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(Dense(target_class, activation='softmax'))\n",
    "\n",
    "    \n",
    "    \n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_acc', mode='max',  patience=10, min_delta=0)\n",
    "    \n",
    "    opt = keras.optimizers.Adam(lr=learning_rate, beta_1=beta1, beta_2=beta2, epsilon=10e-8)\n",
    "       \n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=opt,metrics=[keras.metrics.categorical_accuracy])\n",
    "    \n",
    "    nn_history = model.fit(X_train, y_train, batch_size=size_batch, epochs=num_iterations, \n",
    "                         validation_data=(X_test, y_test),\n",
    "                          callbacks=[es])\n",
    "    \n",
    "    a = model.predict(X_train)\n",
    "    predictions = np.zeros_like(a)\n",
    "    predictions[np.arange(len(a)), a.argmax(1)] = 1\n",
    "    print(predictions)\n",
    "    print(y_train)\n",
    "    print(classification_report(y_train,predictions))\n",
    "    a = model.predict(X_test)\n",
    "    predictions = np.zeros_like(a)\n",
    "    predictions[np.arange(len(a)), a.argmax(1)] = 1\n",
    "    print(classification_report(y_test,predictions))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_cnn(X_train, X_test, y_train, y_test, regularization = 0.1,drop_likely = 0.1,\n",
    "              learning_rate = 0.001, beta1 = 0.9, beta2 = 0.999,\n",
    "             num_iterations = 40, size_batch=128,activation = 'tanh'):\n",
    "    target_class = y_train.shape[1]\n",
    "    # Model \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Conv2D(64, kernel_size = (3,3), activation=activation, input_shape = (50,13,1)))\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size = (3,3), activation=activation))\n",
    "\n",
    "    model.add(Conv2D(128, kernel_size = (3,3), activation=activation))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (1,1)))\n",
    "    \n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(2048, activation=activation))\n",
    "    model.add(Dense(1024, activation=activation))\n",
    "    model.add(Dense(512, activation=activation))\n",
    "    \n",
    "    model.add(Dense(target_class, activation='softmax'))\n",
    "\n",
    "    \n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', mode='max',  patience=10, min_delta=0)\n",
    "    \n",
    "    opt = keras.optimizers.Adam(lr=learning_rate, beta_1=beta1, beta_2=beta2, epsilon=10e-8)\n",
    "       \n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=opt,metrics=[keras.metrics.categorical_accuracy])\n",
    "    \n",
    "    nn_history = model.fit(X_train, y_train, batch_size=size_batch, epochs=num_iterations, \n",
    "                         validation_data=(X_test, y_test),\n",
    "                          callbacks=[es])\n",
    "    \n",
    "    a = model.predict(X_train)\n",
    "    predictions = np.zeros_like(a)\n",
    "    predictions[np.arange(len(a)), a.argmax(1)] = 1\n",
    "    print(predictions)\n",
    "    print(y_train)\n",
    "    print(classification_report(y_train,predictions))\n",
    "    a = model.predict(X_test)\n",
    "    predictions = np.zeros_like(a)\n",
    "    predictions[np.arange(len(a)), a.argmax(1)] = 1\n",
    "    print(classification_report(y_test,predictions))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32499 samples, validate on 4063 samples\n",
      "Epoch 1/40\n",
      "32499/32499 [==============================] - 721s 22ms/step - loss: 2.5986 - categorical_accuracy: 0.0875 - val_loss: 2.5069 - val_categorical_accuracy: 0.0925\n",
      "Epoch 2/40\n",
      "32499/32499 [==============================] - 981s 30ms/step - loss: 2.5065 - categorical_accuracy: 0.0893 - val_loss: 2.5363 - val_categorical_accuracy: 0.0817\n",
      "Epoch 3/40\n",
      "24960/32499 [======================>.......] - ETA: 3:37 - loss: 2.5147 - categorical_accuracy: 0.0875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0729 11:00:10.212630 140735522395008 ultratb.py:149] Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-07b4934d80f8>\", line 1, in <module>\n",
      "    simple_cnn(X_train, X_test, y_train, y_test)\n",
      "  File \"<ipython-input-6-d18395cd049d>\", line 40, in simple_cnn\n",
      "    callbacks=[es])\n",
      "  File \"/usr/local/lib/python3.7/site-packages/keras/engine/training.py\", line 1039, in fit\n",
      "    validation_steps=validation_steps)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/keras/engine/training_arrays.py\", line 199, in fit_loop\n",
      "    outs = f(ins_batch)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\", line 2715, in __call__\n",
      "    return self._call(inputs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\", line 2675, in _call\n",
      "    fetched = self._callable_fn(*array_vals)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1458, in __call__\n",
      "    run_metadata_ptr)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/posixpath.py\", line 415, in _joinrealpath\n",
      "    name, _, rest = rest.partition(sep)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0729 11:00:10.812683 140735522395008 alias.py:221] Invalid alias: The name clear can't be aliased because it is another magic command.\n",
      "E0729 11:00:10.813905 140735522395008 alias.py:221] Invalid alias: The name more can't be aliased because it is another magic command.\n",
      "E0729 11:00:10.814826 140735522395008 alias.py:221] Invalid alias: The name less can't be aliased because it is another magic command.\n",
      "E0729 11:00:10.815836 140735522395008 alias.py:221] Invalid alias: The name man can't be aliased because it is another magic command.\n"
     ]
    }
   ],
   "source": [
    "simple_cnn(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_MFCC_LSTM(X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.04027690e+01, -2.29216742e+00, -2.45621438e+01, ...,\n",
       "         4.88963861e+00,  1.48930418e+01,  9.20361442e+00],\n",
       "       [ 1.13726075e+01,  1.64783184e-01, -2.58792595e+01, ...,\n",
       "        -2.06376344e+00,  1.46754733e+01,  7.15964668e+00],\n",
       "       [ 1.36984766e+01, -3.23827023e+00, -2.46365405e+01, ...,\n",
       "        -9.64290945e-01,  4.47336320e+00,  8.45182667e+00],\n",
       "       ...,\n",
       "       [ 1.89949429e+01,  2.19963632e+01, -2.23273079e+01, ...,\n",
       "        -6.88303927e+00,  2.70539077e+01,  4.76705165e+00],\n",
       "       [ 1.49043370e+01,  9.61019133e+00, -3.04150089e+01, ...,\n",
       "        -2.10617805e-02, -5.44090491e+00, -1.51162092e+00],\n",
       "       [ 1.03970799e+01, -2.06840049e+00, -1.48779458e+01, ...,\n",
       "         4.57743321e+00,  2.12141676e+01,  1.02466831e+01]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(X_train, (X_train.shape[0], 50*13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_1dconv(X_train, X_test, y_train, y_test, regularization = 0.1,drop_likely = 0.1,\n",
    "              learning_rate = 0.001, beta1 = 0.9, beta2 = 0.999,\n",
    "             num_iterations = 40, size_batch=128,activation = 'relu'):\n",
    "    target_class = y_train.shape[1]\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], 50*13,1))\n",
    "    X_test =np.reshape(X_test, (X_test.shape[0], 50*13,1))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(256, 5, padding='same',input_shape=(X_train.shape[1],1))) #1\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Conv1D(256, 5, padding='same')) #2\n",
    "    model.add(keras.layers.BatchNormalization())    \n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(MaxPooling1D(pool_size=(8)))\n",
    "    model.add(Conv1D(128, 5, padding='same')) #3\n",
    "    model.add(Activation(activation)) \n",
    "    model.add(Conv1D(128, 5, padding='same')) #4\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Conv1D(128, 5, padding='same')) #5\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Conv1D(128, 5, padding='same')) #6\n",
    "    model.add(keras.layers.BatchNormalization())    \n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(MaxPooling1D(pool_size=(8)))\n",
    "    model.add(Conv1D(64, 5, padding='same')) #7\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Conv1D(64, 5, padding='same')) #8\n",
    "    model.add(Activation(activation))\n",
    "\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    \n",
    "    model.add(Dense(target_class)) #9\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    \n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', mode='max',  patience=10, min_delta=0)\n",
    "    \n",
    "    opt = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    \n",
    "#     opt = keras.optimizers.Adam(lr=learning_rate, beta_1=beta1, beta_2=beta2, epsilon=10e-8)\n",
    "       \n",
    "    lr_reduce = keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.9, patience=20, min_lr=0.000001)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=opt)\n",
    "    cnnhistory=model.fit(X_train, y_train, batch_size=16, epochs=700, validation_data=(X_test, y_test), \n",
    "                             callbacks=[lr_reduce])\n",
    "\n",
    "    \n",
    "    a = model.predict(X_train)\n",
    "    predictions = np.zeros_like(a)\n",
    "    predictions[np.arange(len(a)), a.argmax(1)] = 1\n",
    "    print(predictions)\n",
    "    print(y_train)\n",
    "    print(classification_report(y_train,predictions))\n",
    "    a = model.predict(X_test)\n",
    "    predictions = np.zeros_like(a)\n",
    "    predictions[np.arange(len(a)), a.argmax(1)] = 1\n",
    "    print(classification_report(y_test,predictions))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32499 samples, validate on 4063 samples\n",
      "Epoch 1/700\n",
      "32499/32499 [==============================] - 946s 29ms/step - loss: 2.4967 - val_loss: 2.4478\n",
      "Epoch 2/700\n",
      "32499/32499 [==============================] - 646s 20ms/step - loss: 2.3753 - val_loss: 2.4057\n",
      "Epoch 3/700\n",
      "32499/32499 [==============================] - 647s 20ms/step - loss: 2.2933 - val_loss: 2.3426\n",
      "Epoch 4/700\n",
      "32499/32499 [==============================] - 649s 20ms/step - loss: 2.2295 - val_loss: 2.3211\n",
      "Epoch 5/700\n",
      "32499/32499 [==============================] - 674s 21ms/step - loss: 2.1756 - val_loss: 2.2523\n",
      "Epoch 6/700\n",
      "32499/32499 [==============================] - 651s 20ms/step - loss: 2.1436 - val_loss: 2.2837\n",
      "Epoch 7/700\n",
      "32499/32499 [==============================] - 655s 20ms/step - loss: 2.1164 - val_loss: 2.1921\n",
      "Epoch 8/700\n",
      "32499/32499 [==============================] - 615s 19ms/step - loss: 2.0928 - val_loss: 2.2812\n",
      "Epoch 9/700\n",
      "32499/32499 [==============================] - 582s 18ms/step - loss: 2.0835 - val_loss: 2.1872\n",
      "Epoch 10/700\n",
      "32499/32499 [==============================] - 601s 19ms/step - loss: 2.0644 - val_loss: 2.2163\n",
      "Epoch 11/700\n",
      "32499/32499 [==============================] - 585s 18ms/step - loss: 2.0546 - val_loss: 2.2091\n",
      "Epoch 12/700\n",
      "32499/32499 [==============================] - 552s 17ms/step - loss: 2.0418 - val_loss: 2.2289\n",
      "Epoch 13/700\n",
      "32499/32499 [==============================] - 566s 17ms/step - loss: 2.0316 - val_loss: 2.1414\n",
      "Epoch 14/700\n",
      "32499/32499 [==============================] - 578s 18ms/step - loss: 2.0261 - val_loss: 2.1532\n",
      "Epoch 15/700\n",
      "32499/32499 [==============================] - 574s 18ms/step - loss: 2.0207 - val_loss: 2.1237\n",
      "Epoch 16/700\n",
      "32499/32499 [==============================] - 564s 17ms/step - loss: 2.0125 - val_loss: 2.1772\n",
      "Epoch 17/700\n",
      "32499/32499 [==============================] - 575s 18ms/step - loss: 2.0021 - val_loss: 2.1547\n",
      "Epoch 18/700\n",
      "32499/32499 [==============================] - 577s 18ms/step - loss: 2.0010 - val_loss: 2.1187\n",
      "Epoch 19/700\n",
      "32499/32499 [==============================] - 563s 17ms/step - loss: 1.9882 - val_loss: 2.1437\n",
      "Epoch 20/700\n",
      "32499/32499 [==============================] - 544s 17ms/step - loss: 1.9868 - val_loss: 2.1731\n",
      "Epoch 21/700\n",
      "32499/32499 [==============================] - 542s 17ms/step - loss: 1.9782 - val_loss: 2.2544\n",
      "Epoch 22/700\n",
      "32499/32499 [==============================] - 545s 17ms/step - loss: 1.9766 - val_loss: 2.1779\n",
      "Epoch 23/700\n",
      "32499/32499 [==============================] - 573s 18ms/step - loss: 1.9699 - val_loss: 2.1440\n",
      "Epoch 24/700\n",
      "32499/32499 [==============================] - 582s 18ms/step - loss: 1.9658 - val_loss: 2.0631\n",
      "Epoch 25/700\n",
      "32499/32499 [==============================] - 562s 17ms/step - loss: 1.9628 - val_loss: 2.1408\n",
      "Epoch 26/700\n",
      "32499/32499 [==============================] - 551s 17ms/step - loss: 1.9532 - val_loss: 2.1449\n",
      "Epoch 27/700\n",
      "32499/32499 [==============================] - 579s 18ms/step - loss: 1.9521 - val_loss: 2.0623\n",
      "Epoch 28/700\n",
      "32499/32499 [==============================] - 598s 18ms/step - loss: 1.9533 - val_loss: 2.0853\n",
      "Epoch 29/700\n",
      "32499/32499 [==============================] - 593s 18ms/step - loss: 1.9481 - val_loss: 2.1002\n",
      "Epoch 30/700\n",
      "32499/32499 [==============================] - 595s 18ms/step - loss: 1.9448 - val_loss: 2.1100\n",
      "Epoch 31/700\n",
      "32499/32499 [==============================] - 592s 18ms/step - loss: 1.9433 - val_loss: 2.1294\n",
      "Epoch 32/700\n",
      "32499/32499 [==============================] - 596s 18ms/step - loss: 1.9375 - val_loss: 2.1462\n",
      "Epoch 33/700\n",
      "32499/32499 [==============================] - 596s 18ms/step - loss: 1.9337 - val_loss: 2.1047\n",
      "Epoch 34/700\n",
      "32499/32499 [==============================] - 599s 18ms/step - loss: 1.9332 - val_loss: 2.0802\n",
      "Epoch 35/700\n",
      "32499/32499 [==============================] - 596s 18ms/step - loss: 1.9314 - val_loss: 2.1183\n",
      "Epoch 36/700\n",
      "32499/32499 [==============================] - 602s 19ms/step - loss: 1.9263 - val_loss: 2.1284\n",
      "Epoch 37/700\n",
      "32499/32499 [==============================] - 599s 18ms/step - loss: 1.9182 - val_loss: 2.0820\n",
      "Epoch 38/700\n",
      "32499/32499 [==============================] - 596s 18ms/step - loss: 1.9204 - val_loss: 2.0766\n",
      "Epoch 39/700\n",
      "32499/32499 [==============================] - 598s 18ms/step - loss: 1.9151 - val_loss: 2.0631\n",
      "Epoch 40/700\n",
      "32499/32499 [==============================] - 597s 18ms/step - loss: 1.9163 - val_loss: 2.1221\n",
      "Epoch 41/700\n",
      "32499/32499 [==============================] - 595s 18ms/step - loss: 1.9124 - val_loss: 2.0774\n",
      "Epoch 42/700\n",
      "32499/32499 [==============================] - 597s 18ms/step - loss: 1.9106 - val_loss: 2.0393\n",
      "Epoch 43/700\n",
      "32499/32499 [==============================] - 595s 18ms/step - loss: 1.9092 - val_loss: 2.1221\n",
      "Epoch 44/700\n",
      "32499/32499 [==============================] - 597s 18ms/step - loss: 1.9075 - val_loss: 2.0675\n",
      "Epoch 45/700\n",
      "32499/32499 [==============================] - 597s 18ms/step - loss: 1.9005 - val_loss: 2.0739\n",
      "Epoch 46/700\n",
      "32499/32499 [==============================] - 596s 18ms/step - loss: 1.8995 - val_loss: 2.0804\n",
      "Epoch 47/700\n",
      "32499/32499 [==============================] - 600s 18ms/step - loss: 1.8968 - val_loss: 2.1634\n",
      "Epoch 48/700\n",
      "32499/32499 [==============================] - 604s 19ms/step - loss: 1.8949 - val_loss: 2.1127\n",
      "Epoch 49/700\n",
      "32499/32499 [==============================] - 597s 18ms/step - loss: 1.8944 - val_loss: 2.2121\n",
      "Epoch 50/700\n",
      "32499/32499 [==============================] - 599s 18ms/step - loss: 1.8894 - val_loss: 2.3022\n",
      "Epoch 51/700\n",
      "32499/32499 [==============================] - 597s 18ms/step - loss: 1.8899 - val_loss: 2.0645\n",
      "Epoch 52/700\n",
      "32499/32499 [==============================] - 599s 18ms/step - loss: 1.8923 - val_loss: 2.1814\n",
      "Epoch 53/700\n",
      "32499/32499 [==============================] - 597s 18ms/step - loss: 1.8846 - val_loss: 2.0686\n",
      "Epoch 54/700\n",
      "32499/32499 [==============================] - 591s 18ms/step - loss: 1.8809 - val_loss: 2.0831\n",
      "Epoch 55/700\n",
      "32499/32499 [==============================] - 584s 18ms/step - loss: 1.8829 - val_loss: 2.0626\n",
      "Epoch 56/700\n",
      "32499/32499 [==============================] - 590s 18ms/step - loss: 1.8785 - val_loss: 2.1056\n",
      "Epoch 57/700\n",
      "32499/32499 [==============================] - 593s 18ms/step - loss: 1.8783 - val_loss: 2.1388\n",
      "Epoch 58/700\n",
      "32499/32499 [==============================] - 589s 18ms/step - loss: 1.8753 - val_loss: 2.0087\n",
      "Epoch 59/700\n",
      "32499/32499 [==============================] - 584s 18ms/step - loss: 1.8729 - val_loss: 2.0268\n",
      "Epoch 60/700\n",
      "32499/32499 [==============================] - 580s 18ms/step - loss: 1.8735 - val_loss: 2.0639\n",
      "Epoch 61/700\n",
      "32499/32499 [==============================] - 586s 18ms/step - loss: 1.8719 - val_loss: 2.0547\n",
      "Epoch 62/700\n",
      "32499/32499 [==============================] - 588s 18ms/step - loss: 1.8682 - val_loss: 2.0858\n",
      "Epoch 63/700\n",
      "32499/32499 [==============================] - 590s 18ms/step - loss: 1.8645 - val_loss: 2.0791\n",
      "Epoch 64/700\n",
      "32499/32499 [==============================] - 589s 18ms/step - loss: 1.8655 - val_loss: 2.0746\n",
      "Epoch 65/700\n",
      "32499/32499 [==============================] - 583s 18ms/step - loss: 1.8616 - val_loss: 2.0477\n",
      "Epoch 66/700\n",
      "32499/32499 [==============================] - 591s 18ms/step - loss: 1.8589 - val_loss: 2.0860\n",
      "Epoch 67/700\n",
      "32499/32499 [==============================] - 598s 18ms/step - loss: 1.8584 - val_loss: 2.0119\n",
      "Epoch 68/700\n",
      "32499/32499 [==============================] - 595s 18ms/step - loss: 1.8567 - val_loss: 2.0625\n",
      "Epoch 69/700\n",
      "32499/32499 [==============================] - 589s 18ms/step - loss: 1.8562 - val_loss: 2.1338\n",
      "Epoch 70/700\n",
      "32499/32499 [==============================] - 593s 18ms/step - loss: 1.8510 - val_loss: 2.0830\n",
      "Epoch 71/700\n",
      "32499/32499 [==============================] - 586s 18ms/step - loss: 1.8511 - val_loss: 2.0546\n",
      "Epoch 72/700\n",
      "32499/32499 [==============================] - 591s 18ms/step - loss: 1.8520 - val_loss: 2.0510\n",
      "Epoch 73/700\n",
      "32499/32499 [==============================] - 556s 17ms/step - loss: 1.8463 - val_loss: 2.0302\n",
      "Epoch 74/700\n",
      "32499/32499 [==============================] - 573s 18ms/step - loss: 1.8475 - val_loss: 2.0549\n",
      "Epoch 75/700\n",
      "32499/32499 [==============================] - 587s 18ms/step - loss: 1.8418 - val_loss: 2.1145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/700\n",
      "32499/32499 [==============================] - 585s 18ms/step - loss: 1.8409 - val_loss: 2.1548\n",
      "Epoch 77/700\n",
      "32499/32499 [==============================] - 583s 18ms/step - loss: 1.8383 - val_loss: 1.9976\n",
      "Epoch 78/700\n",
      "32499/32499 [==============================] - 581s 18ms/step - loss: 1.8368 - val_loss: 2.0857\n",
      "Epoch 79/700\n",
      "32499/32499 [==============================] - 578s 18ms/step - loss: 1.8394 - val_loss: 2.1315\n",
      "Epoch 80/700\n",
      "32499/32499 [==============================] - 579s 18ms/step - loss: 1.8346 - val_loss: 1.9794\n",
      "Epoch 81/700\n",
      "32499/32499 [==============================] - 581s 18ms/step - loss: 1.8347 - val_loss: 2.1766\n",
      "Epoch 82/700\n",
      "32499/32499 [==============================] - 583s 18ms/step - loss: 1.8325 - val_loss: 1.9936\n",
      "Epoch 83/700\n",
      "32499/32499 [==============================] - 589s 18ms/step - loss: 1.8320 - val_loss: 2.1384\n",
      "Epoch 84/700\n",
      "32499/32499 [==============================] - 582s 18ms/step - loss: 1.8325 - val_loss: 2.1415\n",
      "Epoch 85/700\n",
      "32499/32499 [==============================] - 583s 18ms/step - loss: 1.8286 - val_loss: 2.0653\n",
      "Epoch 86/700\n",
      "32499/32499 [==============================] - 582s 18ms/step - loss: 1.8264 - val_loss: 2.1293\n",
      "Epoch 87/700\n",
      "32499/32499 [==============================] - 574s 18ms/step - loss: 1.8250 - val_loss: 2.0036\n",
      "Epoch 88/700\n",
      "32499/32499 [==============================] - 575s 18ms/step - loss: 1.8255 - val_loss: 2.0278\n",
      "Epoch 89/700\n",
      "32499/32499 [==============================] - 575s 18ms/step - loss: 1.8157 - val_loss: 2.1516\n",
      "Epoch 90/700\n",
      "32499/32499 [==============================] - 582s 18ms/step - loss: 1.8183 - val_loss: 1.9854\n",
      "Epoch 91/700\n",
      "32499/32499 [==============================] - 584s 18ms/step - loss: 1.8159 - val_loss: 2.1270\n",
      "Epoch 92/700\n",
      "32499/32499 [==============================] - 548s 17ms/step - loss: 1.8173 - val_loss: 2.1604\n",
      "Epoch 93/700\n",
      "32499/32499 [==============================] - 559s 17ms/step - loss: 1.8121 - val_loss: 2.0609\n",
      "Epoch 94/700\n",
      "32499/32499 [==============================] - 540s 17ms/step - loss: 1.8142 - val_loss: 2.1084\n",
      "Epoch 95/700\n",
      "32499/32499 [==============================] - 540s 17ms/step - loss: 1.8117 - val_loss: 2.0590\n",
      "Epoch 96/700\n",
      "32499/32499 [==============================] - 542s 17ms/step - loss: 1.8089 - val_loss: 2.0825\n",
      "Epoch 97/700\n",
      "32499/32499 [==============================] - 542s 17ms/step - loss: 1.8083 - val_loss: 2.0482\n",
      "Epoch 98/700\n",
      "32499/32499 [==============================] - 547s 17ms/step - loss: 1.8061 - val_loss: 2.2696\n",
      "Epoch 99/700\n",
      "32499/32499 [==============================] - 562s 17ms/step - loss: 1.8083 - val_loss: 2.0403\n",
      "Epoch 100/700\n",
      "32499/32499 [==============================] - 544s 17ms/step - loss: 1.8062 - val_loss: 2.1018\n",
      "Epoch 101/700\n",
      "32499/32499 [==============================] - 546s 17ms/step - loss: 1.8055 - val_loss: 2.0312\n",
      "Epoch 102/700\n",
      "32499/32499 [==============================] - 548s 17ms/step - loss: 1.7984 - val_loss: 2.0340\n",
      "Epoch 103/700\n",
      "32499/32499 [==============================] - 550s 17ms/step - loss: 1.8024 - val_loss: 2.1014\n",
      "Epoch 104/700\n",
      "32499/32499 [==============================] - 551s 17ms/step - loss: 1.7984 - val_loss: 2.0698\n",
      "Epoch 105/700\n",
      "32499/32499 [==============================] - 577s 18ms/step - loss: 1.7943 - val_loss: 2.1344\n",
      "Epoch 106/700\n",
      "32499/32499 [==============================] - 574s 18ms/step - loss: 1.7942 - val_loss: 2.1558\n",
      "Epoch 107/700\n",
      "32499/32499 [==============================] - 569s 18ms/step - loss: 1.7933 - val_loss: 2.2152\n",
      "Epoch 108/700\n",
      "32499/32499 [==============================] - 571s 18ms/step - loss: 1.7931 - val_loss: 1.9952\n",
      "Epoch 109/700\n",
      "32499/32499 [==============================] - 610s 19ms/step - loss: 1.7888 - val_loss: 2.2267\n",
      "Epoch 110/700\n",
      "32499/32499 [==============================] - 605s 19ms/step - loss: 1.7875 - val_loss: 2.0376\n",
      "Epoch 111/700\n",
      "32499/32499 [==============================] - 634s 20ms/step - loss: 1.7893 - val_loss: 2.1553\n",
      "Epoch 112/700\n",
      "32499/32499 [==============================] - 599s 18ms/step - loss: 1.7838 - val_loss: 2.0551\n",
      "Epoch 113/700\n",
      "32499/32499 [==============================] - 612s 19ms/step - loss: 1.7844 - val_loss: 2.1468\n",
      "Epoch 114/700\n",
      "32499/32499 [==============================] - 617s 19ms/step - loss: 1.7832 - val_loss: 2.0333\n",
      "Epoch 115/700\n",
      "32499/32499 [==============================] - 594s 18ms/step - loss: 1.7830 - val_loss: 1.9963\n",
      "Epoch 116/700\n",
      "32499/32499 [==============================] - 563s 17ms/step - loss: 1.7817 - val_loss: 1.9921\n",
      "Epoch 117/700\n",
      "32499/32499 [==============================] - 544s 17ms/step - loss: 1.7727 - val_loss: 2.0255\n",
      "Epoch 118/700\n",
      "32499/32499 [==============================] - 548s 17ms/step - loss: 1.7770 - val_loss: 2.1566\n",
      "Epoch 119/700\n",
      " 2464/32499 [=>............................] - ETA: 8:46 - loss: 1.7758"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-ec3c199bea46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimple_1dconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-a47fc9b5c00e>\u001b[0m in \u001b[0;36msimple_1dconv\u001b[0;34m(X_train, X_test, y_train, y_test, regularization, drop_likely, learning_rate, beta1, beta2, num_iterations, size_batch, activation)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     cnnhistory=model.fit(X_train, y_train, batch_size=16, epochs=700, validation_data=(X_test, y_test), \n\u001b[0;32m---> 49\u001b[0;31m                              callbacks=[lr_reduce])\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "simple_1dconv(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = [sherpa.Continuous(name='lr', range=[0.0001, 0.1], scale='log'),\n",
    "              sherpa.Continuous(name='beta1', range=[0.85, 1.0], scale='log'),\n",
    "              sherpa.Continuous(name=\"regularization\", range=[0.0001, 1], scale='log'),\n",
    "              sherpa.Continuous(name='dropout', range=[0.0, 0.5]),\n",
    "              sherpa.Ordinal(name='batch_size', range=[16, 32, 64,128,256,512]),\n",
    "              sherpa.Choice(name='activation', range=['relu', 'elu', 'prelu', 'tanh'])]\n",
    "\n",
    "alg =bayesian_optimization.GPyOpt()\n",
    "\n",
    "study = sherpa.Study(parameters=parameters,\n",
    "                     algorithm=alg,dashboard_port=9998,disable_dashboard=False,\n",
    "                     lower_is_better=False)\n",
    "\n",
    "for trial in study:\n",
    "    cnn_MFCC_LSTM(study, trial, X_train, X_val, y_train, y_val, regularization =trial.parameters['regularization'],\n",
    "              drop_likely = trial.parameters['dropout'],\n",
    "              learning_rate = trial.parameters['lr'],\n",
    "              beta1 = trial.parameters['beta1'], beta2 = 0.999,\n",
    "             num_iterations = 150, size_batch=trial.parameters[\"batch_size\"])\n",
    "    study.finalize(trial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_MFCC_LSTM(study, trial, X_train, X_test, y_train, y_test, regularization = 0.1,drop_likely = 0.1,\n",
    "              learning_rate = 0.001, beta1 = 0.9, beta2 = 0.999,\n",
    "             num_iterations = 40, size_batch=X_train.shape[0],activation = 'tanh'):\n",
    "    # Set the target class number\n",
    "    \n",
    "    # VGG 19\n",
    "    \n",
    "    target_class = y_train.shape[1]\n",
    "    # Model \n",
    "    model = Sequential()\n",
    "    \n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size = (3,3), activation=activation, input_shape = (50,13,1)))\n",
    "    model.add(Conv2D(64, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (1,1)))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(Conv2D(128, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (1,1)))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(256, kernel_size = (3,3), activation=activation))\n",
    "    model.add(Conv2D(256, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(Conv2D(256, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(Conv2D(256, kernel_size = (3,3), activation=activation))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (1,1)))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    \n",
    "    model.add(Conv2D(512, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(Conv2D(512, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(Conv2D(512, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(Conv2D(512, kernel_size = (3,3), activation=activation))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(Flatten())\n",
    "\n",
    "    \n",
    "    \n",
    "    model.add(Dense(4096, activation=activation))\n",
    "    model.add(Dense(4096, activation=activation))\n",
    "\n",
    "    model.add(Dense(target_class, activation='softmax'))\n",
    "\n",
    "    \n",
    "    \n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_acc', mode='max',  patience=10, min_delta=0)\n",
    "    \n",
    "    opt = keras.optimizers.Adam(lr=learning_rate, beta_1=beta1, beta_2=beta2, epsilon=10e-8)\n",
    "       \n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=opt,metrics=[keras.metrics.categorical_accuracy])\n",
    "    \n",
    "    nn_history = model.fit(X_train, y_train, batch_size=size_batch, epochs=num_iterations, \n",
    "                         validation_data=(X_test, y_test),\n",
    "                          callbacks=[es, study.keras_callback(trial, objective_name='val_categorical_accuracy')])\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_inception(X_train, X_test, y_train, y_test):\n",
    "    #inception architecture\n",
    "    \n",
    "    target_class = y_train.shape[1]\n",
    "    # Model \n",
    "    model = Sequential()\n",
    "    model.add(keras.layers. conv2d_bn(32, 3,3, stridea =(2,2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_inception(X_cache, X_cache, Y_cache, Y_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.argmax(y_train, axis = 1)\n",
    "Y_val = np.argmax(y_val, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.svm.NuSVC(gamma = 'auto' )\n",
    "model.fit(X_train, Y_train)\n",
    "predictions = model.predict(X_val)\n",
    "print(classification_report(Y_val, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.svm.SVC(gamma = 'scale')\n",
    "model.fit(X_train, Y_train)\n",
    "predictions = model.predict(X_val)\n",
    "print(classification_report(Y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.SGDClassifier()\n",
    "model.fit(X_train, Y_train)\n",
    "predictions = model.predict(X_val)\n",
    "print(classification_report(Y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization = 0.1\n",
    "drop_likely = 0.1\n",
    "learning_rate = 0.001\n",
    "beta1 = 0.9 \n",
    "beta2 = 0.999,\n",
    "num_iterations = 40\n",
    "size_batch=256,\n",
    "activation = 'relu'\n",
    "target_class = y_train.shape[1]\n",
    "# Model ALEX NEt\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(96, 22, padding='valid',strides=2,input_shape=(X_train.shape[1], 1))) \n",
    "print(model.input_shape)\n",
    "print(model.output_shape)\n",
    "model.add(MaxPooling1D(pool_size = (3), strides=(2)))\n",
    "print(model.output_shape)\n",
    "model.add(Conv1D(256, 5, padding='same')) \n",
    "\n",
    "print(model.output_shape)\n",
    "model.add(MaxPooling1D(pool_size = (3), strides=(2)))\n",
    "print(model.output_shape)\n",
    "\n",
    "model.add(Conv1D(384, 3, padding='same')) \n",
    "print(model.output_shape)\n",
    "\n",
    "model.add(Conv1D(384, 3, padding='same')) \n",
    "print(model.output_shape)\n",
    "\n",
    "model.add(Conv1D(256, 3, padding='same')) \n",
    "print(model.output_shape)\n",
    "\n",
    "model.add(MaxPooling1D(pool_size = (3), strides=(2)))\n",
    "print(model.output_shape)\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(4096, input_dim=9216,  activation=activation,\n",
    "               kernel_regularizer=regularizers.l2(regularization)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(Dense(4096, activation=activation,\n",
    "               kernel_regularizer=regularizers.l2(regularization)))\n",
    "model.add(Dropout(drop_likely))\n",
    "\n",
    "model.add(Dense(2048, activation=activation,\n",
    "               kernel_regularizer=regularizers.l2(regularization)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(Dropout(drop_likely))\n",
    "\n",
    "model.add(Dense(1024, activation=activation,\n",
    "               kernel_regularizer=regularizers.l2(regularization)))\n",
    "\n",
    "model.add(Dense(512, activation=activation,\n",
    "               kernel_regularizer=regularizers.l2(regularization)))\n",
    "\n",
    "model.add(Dense(target_class, activation='softmax'))\n",
    "\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_acc', mode='max',  patience=10, min_delta=0)\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=learning_rate, beta_1=beta1, beta_2=beta2, epsilon=10e-8)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "nn_history = model.fit(X_train, y_train, batch_size=size_batch, epochs=num_iterations, \n",
    "                     validation_data=(X_val, y_val),callbacks=[es])\n",
    "model, nn_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_cnn(X_train, X_test, y_train, y_test):\n",
    "    # Set the target class number\n",
    "    target_class = y_train.shape[1]\n",
    "    # Model \n",
    "    model = Sequential()\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1], 1))) #1\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Conv1D(256, 8, padding='same')) #2\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(128, 8, padding='same')) #3\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Activation('tanh')) \n",
    "    model.add(Conv1D(128, 8, padding='same')) #4\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Conv1D(128, 8, padding='same')) #5\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Conv1D(128, 8, padding='same')) #6\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(64, 8, padding='same')) #7\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Conv1D(64, 8, padding='same')) #8\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(target_class)) #9\n",
    "    model.add(Activation('softmax'))\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=10e-8, decay=0.0, amsgrad=False)\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=opt,metrics=[keras.metrics.categorical_accuracy])\n",
    "    cnnhistory = model.fit(X_train, y_train, batch_size=128, epochs=40, \n",
    "                         validation_data=(X_test, y_test))\n",
    "    a = model.predict(X_train)\n",
    "    predictions = np.zeros_like(a)\n",
    "    predictions[np.arange(len(a)), a.argmax(1)] = 1\n",
    "    print(predictions)\n",
    "    print(y_train)\n",
    "    print(classification_report(y_train,predictions))\n",
    "    a = model.predict(X_test)\n",
    "    predictions = np.zeros_like(a)\n",
    "    predictions[np.arange(len(a)), a.argmax(1)] = 1\n",
    "    print(classification_report(y_test,predictions))\n",
    "    return model, cnnhistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_silence_from(amplitudes, threshold):\n",
    "    silenced = []\n",
    "    for x in amplitudes:\n",
    "        if x >= threshold:\n",
    "            silenced.append(x)\n",
    "    return silenced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MFCC_algorithm_LIBROSA(np_data, fs):\n",
    "    MFCCs = []\n",
    "    print(\"running .....\")\n",
    "    #for progess bar\n",
    "    for one_sound in np_data:\n",
    "        \n",
    "        one_sound = one_sound.astype(float)\n",
    "        MFCCs.append(librosa.power_to_db(librosa.feature.melspectrogram(y=one_sound, sr=fs, \n",
    "                                                    n_mels=128,n_fft=2048,hop_length=32, fmax=8000)))\n",
    "    return np.array(MFCCs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MFCC_algorithm(np_data, fs):\n",
    "    MFCCs = []\n",
    "    print(\"running .....\")\n",
    "    #for progess bar\n",
    "    for one_sound in np_data:\n",
    "        \n",
    "        one_sound = np.asarray(one_sound).reshape(int((fs/1000)*512), 1)\n",
    "        \n",
    "        MFCCs.append(python_speech_features.base.mfcc(one_sound, samplerate=fs, \n",
    "                                     winlen=0.025, winstep=0.01, numcep=13, \n",
    "                                     nfilt=26, nfft=552))\n",
    "    return np.array(MFCCs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pitch_vector(data, fs):\n",
    "    data = np.float32(data)\n",
    "    pitch = pysptk.sptk.rapt(y=data, sr=fs, hopsize = 40)\n",
    "    silenced = remove_silence_from(pitch, np.mean(pitch))\n",
    "    return silenced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectral_vector(data, fs):\n",
    "    data = np.float32(data)\n",
    "    cent = librosa.feature.spectral_centroid(y=data, sr=fs, hop_length=165)\n",
    "    return cent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rms_vector(data):\n",
    "    temp_data = np.float32(data)\n",
    "    cent = librosa.feature.rms(y=temp_data, hop_length=165)\n",
    "    return cent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zero_vector(data):\n",
    "    temp_data = np.float32(data)\n",
    "    cent = librosa.feature.zero_crossing_rate(y=temp_data, hop_length=165)\n",
    "    return cent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sr_vector(data):\n",
    "    temp_data = np.float32(data)\n",
    "    cent = librosa.feature.spectral_rolloff(y=temp_data, hop_length=165)\n",
    "    return cent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def me_and_gradient(x):\n",
    "    return x, np.gradient(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
