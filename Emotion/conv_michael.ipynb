{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "import scipy\n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import osascript\n",
    "from gtts import gTTS \n",
    "import os \n",
    "import pyaudio\n",
    "import wave\n",
    "import keyboard  # using module keyboard\n",
    "import soundfile as sf\n",
    "import math\n",
    "import pyloudnorm as pyln\n",
    "from sys import byteorder\n",
    "from array import array\n",
    "from struct import pack\n",
    "import librosa\n",
    "from scipy.signal import butter, sosfiltfilt\n",
    "import pysptk\n",
    "import python_speech_features\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "import keras as keras\n",
    "from  conch.analysis.formants import lpc\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras import regularizers\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = np.load(\"../../splitdata.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, fs, y = np.load(\"../../CREMA/CREMAdatafsy.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8192,), 16000, 8192)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk = int(512*(fs/1000))\n",
    "one_point = dataset[0][0:chunk]\n",
    "one_point.shape, fs, chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_silence_from(amplitudes, threshold):\n",
    "    silenced = []\n",
    "    for x in amplitudes:\n",
    "        if x >= threshold:\n",
    "            silenced.append(x)\n",
    "    return silenced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MFCC_algorithm(np_data, fs):\n",
    "    MFCCs = []\n",
    "    print(\"running .....\")\n",
    "    #for progess bar\n",
    "    for one_sound in np_data:\n",
    "        \n",
    "        one_sound = np.asarray(one_sound).reshape(int((fs/1000)*512), 1)\n",
    "        \n",
    "        MFCCs.append(python_speech_features.base.mfcc(one_sound, samplerate=fs, \n",
    "                                     winlen=0.025, winstep=0.01, numcep=13, \n",
    "                                     nfilt=26, nfft=552))\n",
    "    return np.array(MFCCs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pitch_vector(data, fs):\n",
    "    data = np.float32(data)\n",
    "    pitch = pysptk.sptk.rapt(y=data, sr=fs, hopsize = 40)\n",
    "    silenced = remove_silence_from(pitch, np.mean(pitch))\n",
    "    return silenced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectral_vector(data, fs):\n",
    "    data = np.float32(data)\n",
    "    cent = librosa.feature.spectral_centroid(y=data, sr=fs, hop_length=165)\n",
    "    return cent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rms_vector(data):\n",
    "    temp_data = np.float32(data)\n",
    "    cent = librosa.feature.rms(y=temp_data, hop_length=165)\n",
    "    return cent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zero_vector(data):\n",
    "    temp_data = np.float32(data)\n",
    "    cent = librosa.feature.zero_crossing_rate(y=temp_data, hop_length=165)\n",
    "    return cent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sr_vector(data):\n",
    "    temp_data = np.float32(data)\n",
    "    cent = librosa.feature.spectral_rolloff(y=temp_data, hop_length=165)\n",
    "    return cent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def me_and_gradient(x):\n",
    "    return x, np.gradient(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len((get_spectral_vector(one_point, fs))[0]))\n",
    "\n",
    "print(len(get_spectral_vector(one_point, fs)[0]))\n",
    "\n",
    "print(len(get_zero_vector(one_point)[0]))\n",
    "\n",
    "print(len(get_rms_vector(one_point)[0]))\n",
    "\n",
    "print(len(get_sr_vector(one_point)[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 16000\n",
    "dataset, y, useless_number = np.load(\"../../CREMA_chunked.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running .....\n"
     ]
    }
   ],
   "source": [
    "X = MFCC_algorithm(dataset, fs)\n",
    "X = X.reshape(X.shape[0], 50, 13, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train shape: (32499, 50, 13, 1)\n",
      "y train shape: (32499, 12)\n",
      "x test shape: (4063, 50, 13, 1)\n",
      "y test shape: (4063, 12)\n"
     ]
    }
   ],
   "source": [
    "print(\"x train shape: \" +str(X_train.shape))\n",
    "print(\"y train shape: \" +str(y_train.shape))\n",
    "print(\"x test shape: \" +str(X_test.shape))\n",
    "print(\"y test shape: \" +str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_MFCC_LSTM(X_train, X_test, y_train, y_test):\n",
    "    # Set the target class number\n",
    "    \n",
    "    # VGG 19\n",
    "    \n",
    "    target_class = y_train.shape[1]\n",
    "    # Model \n",
    "    model = Sequential()\n",
    "    \n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size = (3,3), activation='relu', input_shape = (50,13,1)))\n",
    "    model.add(Conv2D(64, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (1,1)))\n",
    "    print(model.input_shape)\n",
    "    print(model.output_shape)\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(128, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (1,1)))\n",
    "    print(model.output_shape)\n",
    "    \n",
    "    model.add(Conv2D(256, kernel_size = (3,3), activation='relu'))\n",
    "    model.add(Conv2D(256, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(256, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(256, kernel_size = (3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (1,1)))\n",
    "    print(model.output_shape)\n",
    "    \n",
    "    \n",
    "    model.add(Conv2D(512, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(512, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(512, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(512, kernel_size = (3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "    print(model.output_shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(Flatten())\n",
    "    print(\"flattened: \")\n",
    "    print(model.output_shape)\n",
    "    \n",
    "    \n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    print(model.output_shape)\n",
    "    \n",
    "    model.add(Dense(target_class, activation='softmax'))\n",
    "    print(model.output_shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=10e-8, decay=0.0, amsgrad=False)\n",
    "    \n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=opt,metrics=[keras.metrics.categorical_accuracy])\n",
    "    cnnhistory = model.fit(X_train, y_train, batch_size=X_train.shape[0], epochs=40, \n",
    "                         validation_data=(X_test, y_test))\n",
    "    \n",
    "    \n",
    "    a = model.predict(X_train)\n",
    "    predictions = np.zeros_like(a)\n",
    "    predictions[np.arange(len(a)), a.argmax(1)] = 1\n",
    "    print(predictions)\n",
    "    print(y_train)\n",
    "    print(classification_report(y_train,predictions))\n",
    "    a = model.predict(X_test)\n",
    "    predictions = np.zeros_like(a)\n",
    "    predictions[np.arange(len(a)), a.argmax(1)] = 1\n",
    "    print(classification_report(y_test,predictions))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0726 18:34:22.253114 140735522395008 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0726 18:34:22.305541 140735522395008 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0726 18:34:22.310031 140735522395008 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0726 18:34:22.413871 140735522395008 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 50, 13, 1)\n",
      "(None, 47, 10, 64)\n",
      "(None, 46, 9, 128)\n",
      "(None, 41, 4, 256)\n",
      "(None, 19, 1, 512)\n",
      "flattened: \n",
      "(None, 9728)\n",
      "(None, 4096)\n",
      "(None, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0726 18:34:23.361099 140735522395008 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0726 18:34:23.404896 140735522395008 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0726 18:34:23.852771 140735522395008 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0726 18:34:24.455065 140735522395008 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32499 samples, validate on 4062 samples\n",
      "Epoch 1/40\n"
     ]
    }
   ],
   "source": [
    "cnn_MFCC_LSTM(X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [sherpa.Continuous(name='lr', range=[0.0001, 0.1], scale='log'),\n",
    "              sherpa.Continuous(name='beta1', range=[0.85, 1.0], scale='log'),\n",
    "              sherpa.Continuous(name=\"regularization\", range=[0.0001, 1], scale='log'),\n",
    "              sherpa.Continuous(name='dropout', range=[0.0, 0.5]),\n",
    "              sherpa.Ordinal(name='batch_size', range=[16, 32, 64,128,256,512,1024, X_train.shape[0]]),\n",
    "              sherpa.Choice(name='activation', range=['relu', 'elu', 'prelu', 'tanh'])]\n",
    "\n",
    "alg =bayesian_optimization.GPyOpt()\n",
    "\n",
    "study = sherpa.Study(parameters=parameters,\n",
    "                     algorithm=alg,dashboard_port=9998,disable_dashboard=False,\n",
    "                     lower_is_better=False)\n",
    "\n",
    "for trial in study:\n",
    "    cnn_MFCC_LSTM(study, trial, X_train, X_val, y_train, y_val, regularization =trial.parameters['regularization'],\n",
    "              drop_likely = trial.parameters['dropout'],\n",
    "              learning_rate = trial.parameters['lr'],\n",
    "              beta1 = trial.parameters['beta1'], beta2 = 0.999,\n",
    "             num_iterations = 150, size_batch=trial.parameters[\"batch_size\"])\n",
    "    study.finalize(trial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_MFCC_LSTM(study, trial, X_train, X_test, y_train, y_test, regularization = 0.1,drop_likely = 0.1,\n",
    "              learning_rate = 0.001, beta1 = 0.9, beta2 = 0.999,\n",
    "             num_iterations = 40, size_batch=X_train.shape[0],activation = 'tanh'):\n",
    "    # Set the target class number\n",
    "    \n",
    "    # VGG 19\n",
    "    \n",
    "    target_class = y_train.shape[1]\n",
    "    # Model \n",
    "    model = Sequential()\n",
    "    \n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size = (3,3), activation=activation, input_shape = (50,13,1)))\n",
    "    model.add(Conv2D(64, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (1,1)))\n",
    "\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(Conv2D(128, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (1,1)))\n",
    "\n",
    "    \n",
    "    model.add(Conv2D(256, kernel_size = (3,3), activation=activation))\n",
    "    model.add(Conv2D(256, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(Conv2D(256, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(Conv2D(256, kernel_size = (3,3), activation=activation))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (1,1)))\n",
    "\n",
    "    \n",
    "    \n",
    "    model.add(Conv2D(512, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(Conv2D(512, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(Conv2D(512, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(Conv2D(512, kernel_size = (3,3), activation=activation))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(Flatten())\n",
    "\n",
    "    \n",
    "    \n",
    "    model.add(Dense(4096, activation=activation))\n",
    "    model.add(Dense(4096, activation=activation))\n",
    "\n",
    "    model.add(Dense(target_class, activation='softmax'))\n",
    "\n",
    "    \n",
    "    \n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_acc', mode='max',  patience=10, min_delta=0)\n",
    "    \n",
    "    opt = keras.optimizers.Adam(lr=learning_rate, beta_1=beta1, beta_2=beta2, epsilon=10e-8)\n",
    "       \n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=opt,metrics=[keras.metrics.categorical_accuracy])\n",
    "    \n",
    "    nn_history = model.fit(X_train, y_train, batch_size=size_batch, epochs=num_iterations, \n",
    "                         validation_data=(X_test, y_test),\n",
    "                          callbacks=[es, study.keras_callback(trial, objective_name='val_acc')])\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_inception(X_train, X_test, y_train, y_test):\n",
    "    #inception architecture\n",
    "    \n",
    "    target_class = y_train.shape[1]\n",
    "    # Model \n",
    "    model = Sequential()\n",
    "    model.add(keras.layers. conv2d_bn(32, 3,3, stridea =(2,2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Conv2D_bn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-9b741c842153>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn_inception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-101-3700bb2c8be9>\u001b[0m in \u001b[0;36mcnn_inception\u001b[0;34m(X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstridea\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Conv2D_bn' is not defined"
     ]
    }
   ],
   "source": [
    "cnn_inception(X_cache, X_cache, Y_cache, Y_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.argmax(y_train, axis = 1)\n",
    "Y_val = np.argmax(y_val, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.62      0.53        73\n",
      "           1       0.48      0.36      0.41        86\n",
      "           2       0.46      0.36      0.41        77\n",
      "           3       0.38      0.39      0.39        79\n",
      "           4       0.45      0.48      0.46        69\n",
      "           5       0.44      0.53      0.48        70\n",
      "           6       0.65      0.76      0.70       110\n",
      "           7       0.58      0.51      0.54       111\n",
      "           8       0.70      0.45      0.55       110\n",
      "           9       0.62      0.64      0.63        99\n",
      "          10       0.65      0.68      0.66       107\n",
      "          11       0.62      0.70      0.66       113\n",
      "\n",
      "    accuracy                           0.55      1104\n",
      "   macro avg       0.54      0.54      0.53      1104\n",
      "weighted avg       0.56      0.55      0.55      1104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = sklearn.svm.NuSVC(gamma = 'auto' )\n",
    "model.fit(X_train, Y_train)\n",
    "predictions = model.predict(X_val)\n",
    "print(classification_report(Y_val, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.63      0.56        73\n",
      "           1       0.49      0.36      0.42        86\n",
      "           2       0.48      0.32      0.39        77\n",
      "           3       0.43      0.47      0.45        79\n",
      "           4       0.44      0.52      0.48        69\n",
      "           5       0.44      0.53      0.48        70\n",
      "           6       0.71      0.79      0.75       110\n",
      "           7       0.61      0.55      0.58       111\n",
      "           8       0.66      0.52      0.58       110\n",
      "           9       0.64      0.64      0.64        99\n",
      "          10       0.65      0.72      0.68       107\n",
      "          11       0.63      0.67      0.65       113\n",
      "\n",
      "    accuracy                           0.57      1104\n",
      "   macro avg       0.56      0.56      0.55      1104\n",
      "weighted avg       0.57      0.57      0.57      1104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = sklearn.svm.SVC(gamma = 'scale')\n",
    "model.fit(X_train, Y_train)\n",
    "predictions = model.predict(X_val)\n",
    "print(classification_report(Y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.56      0.55        73\n",
      "           1       0.35      0.30      0.32        86\n",
      "           2       0.37      0.31      0.34        77\n",
      "           3       0.24      0.25      0.25        79\n",
      "           4       0.45      0.48      0.46        69\n",
      "           5       0.41      0.29      0.34        70\n",
      "           6       0.68      0.71      0.70       110\n",
      "           7       0.56      0.45      0.50       111\n",
      "           8       0.54      0.37      0.44       110\n",
      "           9       0.58      0.41      0.48        99\n",
      "          10       0.51      0.64      0.57       107\n",
      "          11       0.39      0.68      0.49       113\n",
      "\n",
      "    accuracy                           0.47      1104\n",
      "   macro avg       0.47      0.46      0.45      1104\n",
      "weighted avg       0.48      0.47      0.47      1104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = sklearn.linear_model.SGDClassifier()\n",
    "model.fit(X_train, Y_train)\n",
    "predictions = model.predict(X_val)\n",
    "print(classification_report(Y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 131, 1)\n",
      "(None, 55, 96)\n",
      "(None, 27, 96)\n",
      "(None, 27, 256)\n",
      "(None, 13, 256)\n",
      "(None, 13, 384)\n",
      "(None, 13, 384)\n",
      "(None, 13, 256)\n",
      "(None, 6, 256)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 8829 arrays: [array([[ 0.60435303,  1.65671061,  1.22716584, -0.28892201,  1.54634199,\n         0.65472625, -1.4454241 , -0.46602093, -3.23933811, -0.96958441,\n         1.89615119, -0.25373911, -0.43849836,  0.621...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-44be3b798d34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m nn_history = model.fit([[x] for x in X_train], y_train, batch_size=size_batch, epochs=num_iterations, \n\u001b[0;32m---> 63\u001b[0;31m                      validation_data=(X_val, y_val),callbacks=[es])\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 8829 arrays: [array([[ 0.60435303,  1.65671061,  1.22716584, -0.28892201,  1.54634199,\n         0.65472625, -1.4454241 , -0.46602093, -3.23933811, -0.96958441,\n         1.89615119, -0.25373911, -0.43849836,  0.621..."
     ]
    }
   ],
   "source": [
    "regularization = 0.1\n",
    "drop_likely = 0.1\n",
    "learning_rate = 0.001\n",
    "beta1 = 0.9 \n",
    "beta2 = 0.999,\n",
    "num_iterations = 40\n",
    "size_batch=256,\n",
    "activation = 'relu'\n",
    "target_class = y_train.shape[1]\n",
    "# Model ALEX NEt\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(96, 22, padding='valid',strides=2,input_shape=(X_train.shape[1], 1))) \n",
    "print(model.input_shape)\n",
    "print(model.output_shape)\n",
    "model.add(MaxPooling1D(pool_size = (3), strides=(2)))\n",
    "print(model.output_shape)\n",
    "model.add(Conv1D(256, 5, padding='same')) \n",
    "\n",
    "print(model.output_shape)\n",
    "model.add(MaxPooling1D(pool_size = (3), strides=(2)))\n",
    "print(model.output_shape)\n",
    "\n",
    "model.add(Conv1D(384, 3, padding='same')) \n",
    "print(model.output_shape)\n",
    "\n",
    "model.add(Conv1D(384, 3, padding='same')) \n",
    "print(model.output_shape)\n",
    "\n",
    "model.add(Conv1D(256, 3, padding='same')) \n",
    "print(model.output_shape)\n",
    "\n",
    "model.add(MaxPooling1D(pool_size = (3), strides=(2)))\n",
    "print(model.output_shape)\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(4096, input_dim=9216,  activation=activation,\n",
    "               kernel_regularizer=regularizers.l2(regularization)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(Dense(4096, activation=activation,\n",
    "               kernel_regularizer=regularizers.l2(regularization)))\n",
    "model.add(Dropout(drop_likely))\n",
    "\n",
    "model.add(Dense(2048, activation=activation,\n",
    "               kernel_regularizer=regularizers.l2(regularization)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(Dropout(drop_likely))\n",
    "\n",
    "model.add(Dense(1024, activation=activation,\n",
    "               kernel_regularizer=regularizers.l2(regularization)))\n",
    "\n",
    "model.add(Dense(512, activation=activation,\n",
    "               kernel_regularizer=regularizers.l2(regularization)))\n",
    "\n",
    "model.add(Dense(target_class, activation='softmax'))\n",
    "\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_acc', mode='max',  patience=10, min_delta=0)\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=learning_rate, beta_1=beta1, beta_2=beta2, epsilon=10e-8)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "nn_history = model.fit(X_train, y_train, batch_size=size_batch, epochs=num_iterations, \n",
    "                     validation_data=(X_val, y_val),callbacks=[es])\n",
    "model, nn_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_cnn(X_train, X_test, y_train, y_test):\n",
    "    # Set the target class number\n",
    "    target_class = y_train.shape[1]\n",
    "    # Model \n",
    "    model = Sequential()\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1], 1))) #1\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Conv1D(256, 8, padding='same')) #2\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(128, 8, padding='same')) #3\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Activation('tanh')) \n",
    "    model.add(Conv1D(128, 8, padding='same')) #4\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Conv1D(128, 8, padding='same')) #5\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Conv1D(128, 8, padding='same')) #6\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(64, 8, padding='same')) #7\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Conv1D(64, 8, padding='same')) #8\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(target_class)) #9\n",
    "    model.add(Activation('softmax'))\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=10e-8, decay=0.0, amsgrad=False)\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=opt,metrics=[keras.metrics.categorical_accuracy])\n",
    "    cnnhistory = model.fit(X_train, y_train, batch_size=128, epochs=40, \n",
    "                         validation_data=(X_test, y_test))\n",
    "    a = model.predict(X_train)\n",
    "    predictions = np.zeros_like(a)\n",
    "    predictions[np.arange(len(a)), a.argmax(1)] = 1\n",
    "    print(predictions)\n",
    "    print(y_train)\n",
    "    print(classification_report(y_train,predictions))\n",
    "    a = model.predict(X_test)\n",
    "    predictions = np.zeros_like(a)\n",
    "    predictions[np.arange(len(a)), a.argmax(1)] = 1\n",
    "    print(classification_report(y_test,predictions))\n",
    "    return model, cnnhistory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
