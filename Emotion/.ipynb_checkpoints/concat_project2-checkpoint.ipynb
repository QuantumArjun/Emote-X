{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import osascript\n",
    "from gtts import gTTS \n",
    "import os \n",
    "import pyaudio\n",
    "import wave\n",
    "import keyboard  # using module keyboard\n",
    "import soundfile as sf\n",
    "import math\n",
    "import pyloudnorm as pyln\n",
    "from sys import byteorder\n",
    "from array import array\n",
    "from struct import pack\n",
    "import librosa\n",
    "from scipy.signal import butter, sosfiltfilt\n",
    "import python_speech_features\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "import pysptk\n",
    "from  conch.analysis.formants import lpc\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BANDPASS_FREQ = [300, 3400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get and Process Sound Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    sos = butter(order, [low, high], btype='band', analog=False, output='sos')\n",
    "    return sos\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    sos = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = sosfiltfilt(sos, data)\n",
    "    return y\n",
    "def normalize(snd_data):\n",
    "    \"Average the volume out\"\n",
    "    MAXIMUM = 16384\n",
    "    times = float(MAXIMUM)/max(abs(i) for i in snd_data)\n",
    "\n",
    "    r = array('h')\n",
    "    for i in snd_data:\n",
    "        r.append(int(i*times))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_silence_from(amplitudes, threshold):\n",
    "    silenced = []\n",
    "    for x in amplitudes:\n",
    "        if x >= threshold:\n",
    "            silenced.append(x)\n",
    "    return silenced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(file_location):\n",
    "    BANDPASS_FREQ = [300, 3400]\n",
    "    fs, data = wavfile.read(file_location)\n",
    "    number_of_samples = data.shape[0]\n",
    "    meta_data = open(r\"LDC2002S28band-txt.txt\")\n",
    "    meta_data = pd.read_csv(\"LDC2002S28-txt.txt\", sep=\"A:\", header=None, engine='python')\n",
    "    meta_data.columns = [\"sound limits\",\"description\"]\n",
    "    \n",
    "    #dual channel to one channel\n",
    "    data = np.average(data, axis = 1)\n",
    "    #remove noise\n",
    "    data = butter_bandpass_filter(data, BANDPASS_FREQ[0], BANDPASS_FREQ[1], fs)\n",
    "    \n",
    "    # removee extra data pointts\n",
    "    meta_data = meta_data[meta_data.description != ' [MISC]']\n",
    "    meta_data = meta_data[~meta_data['description'].astype(str).str.startswith(' (')]\n",
    "    meta_data = meta_data[~meta_data['description'].astype(str).str.startswith(' Emotion category elation')]\n",
    "    meta_data = meta_data[~meta_data['description'].astype(str).str.startswith('  [MISC]')]\n",
    "\n",
    "    # description and time limits \n",
    "    voice_time_limits = meta_data[\"sound limits\"]\n",
    "    voice_time_limits = [i.split(\" \")[0:2] for i in voice_time_limits]\n",
    "    voice_time_limits = np.array(voice_time_limits)\n",
    "    voice_time_limits = voice_time_limits.astype(np.float)\n",
    "    description = meta_data[\"description\"]\n",
    "    description = [i.split(\",\")[0].strip() for i in description]\n",
    "\n",
    "    #divide the dataa set\n",
    "    divided_data = []\n",
    "    for i in voice_time_limits:\n",
    "        startingpoint = int(i[0]*fs)\n",
    "        endingpoint = int(i[1]*fs)\n",
    "        divided_data.append(data[startingpoint:endingpoint])\n",
    "    np_data = np.asarray(divided_data)\n",
    "    return np_data, description, len(divided_data), fs\n",
    "\n",
    "def data_extraction_RAVDESS(folder_location):\n",
    "    divided_data = []\n",
    "    directory = os.fsencode(folder_location)\n",
    "    for file_name in os.listdir(directory):\n",
    "        if  str(file_name) != \"b'.DS_Store'\":\n",
    "            (sig, rate) = librosa.load(folder_location+'/'+str(file_name)[2:-1], sr=None)\n",
    "            data = butter_bandpass_filter(sig, BANDPASS_FREQ[0], BANDPASS_FREQ[1], rate)\n",
    "            data = normalize(data)\n",
    "            data = np.asarray(data)\n",
    "            divided_data.append(data)\n",
    "    np_data = np.asarray(divided_data)\n",
    "    number_examples = np_data.shape[0]\n",
    "    return divided_data,rate, number_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MFCC_algorithm(np_data, fs):\n",
    "    MFCC2 = []\n",
    "\n",
    "    for one_sound in np_data:\n",
    "        one_sound = np.asarray(one_sound)\n",
    "        MFCC2.append(python_speech_features.base.mfcc(one_sound, samplerate=fs, \n",
    "                                     winlen=0.025, winstep=0.01, numcep=13, \n",
    "                                     nfilt=26, nfft=1200).T)\n",
    "    MFCC3 = []\n",
    "    cached_variables = []\n",
    "    for one_point in MFCC2:\n",
    "        cache_grad = (np.gradient(one_point, axis = 1))\n",
    "        cached_variables.append(np.asarray([np.mean(one_point, axis = 1), np.median(one_point, axis = 1),\n",
    "                                 np.var(one_point, axis = 1), \n",
    "                           np.min(one_point, axis = 1), np.max(one_point, axis = 1), \n",
    "                                 np.mean(cache_grad, axis = 1), np.var(cache_grad, axis = 1)]).flatten()\n",
    "                               )\n",
    "    return cached_variables\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pitch_vector(data, fs):\n",
    "    data = np.float32(data)\n",
    "    pitch = pysptk.sptk.rapt(data, fs, hopsize = 50)\n",
    "    silenced = remove_silence_from(pitch, np.mean(pitch))\n",
    "    return silenced\n",
    "\n",
    "def get_pitch_stats(np_array, fs):\n",
    "    stats_matrix = []\n",
    "    for data in np_array:\n",
    "        pitch_vector = get_pitch_vector(data, fs)\n",
    "        stats = get_stats(pitch_vector)\n",
    "        stats_matrix.append(stats)\n",
    "    return stats_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectral_vector(data, fs):\n",
    "    data = np.float32(data)\n",
    "    cent = librosa.feature.spectral_centroid(y=data, sr=fs)\n",
    "    return cent\n",
    "def get_spectral_stats(np_array, fs):\n",
    "    stats_matrix = []\n",
    "    for data in np_array:\n",
    "        spectral_vector = get_spectral_vector(data, fs)\n",
    "        stats = get_stats(spectral_vector)\n",
    "        stats_matrix.append(stats)\n",
    "    return stats_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lpc_vector(data):\n",
    "    vec = lpc.lpc_ref(data, 12)\n",
    "    return vec\n",
    "def get_lpc_stats(np_array):\n",
    "    stats_matrix = []\n",
    "    for data in np_array:\n",
    "        lpc_vector = get_lpc_vector(data)\n",
    "        stats_matrix.append(lpc_vector[1:])  #remove the first number, it's not useful\n",
    "    return stats_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rms_vector(data):\n",
    "    temp_data = np.float32(data)\n",
    "    cent = librosa.feature.rms(y=temp_data)\n",
    "    return cent\n",
    "def get_rms_stats(np_array):\n",
    "    stats_matrix = []\n",
    "    for data in np_array:\n",
    "        rms_vector = get_rms_vector(data)\n",
    "        stats = get_stats(rms_vector)\n",
    "        stats_matrix.append(stats)\n",
    "    return stats_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zero_vector(data):\n",
    "    temp_data = np.float32(data)\n",
    "    cent = librosa.feature.zero_crossing_rate(y=temp_data)\n",
    "    return cent\n",
    "def get_zero_stats(np_array):\n",
    "    stats_matrix = []\n",
    "    for data in np_array:\n",
    "        zero_vector = get_zero_vector(data)\n",
    "        stats = get_stats(zero_vector)\n",
    "        stats_matrix.append(stats)\n",
    "    return stats_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sr_vector(data):\n",
    "    temp_data = np.float32(data)\n",
    "    cent = librosa.feature.spectral_rolloff(y=temp_data)\n",
    "    return cent\n",
    "def get_sr_stats(np_array):\n",
    "    stats_matrix = []\n",
    "    for data in np_array:\n",
    "        sr_vector = get_sr_vector(data)\n",
    "        stats = get_stats(sr_vector)\n",
    "        stats_matrix.append(stats)\n",
    "    return stats_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(pitch_vector):\n",
    "    mean = np.mean(pitch_vector)\n",
    "    median = np.median(pitch_vector)\n",
    "    low = np.min(pitch_vector)\n",
    "    high = np.max(pitch_vector)\n",
    "    variance = np.var(pitch_vector)\n",
    "    \n",
    "    #derivative\n",
    "    derivative = np.diff(pitch_vector)\n",
    "    d_mean = np.mean(derivative)\n",
    "    d_min = np.min(derivative)\n",
    "    d_max = np.max(derivative)\n",
    "    return [mean, median, low, high, variance, d_mean, d_min, d_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# emotional extraction\n",
    "given an array of the emotions it converts the array to a number, if an emotion is not there it will print it out and break the loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_extraction(description, number_examples):\n",
    "    nu_emotion = 15\n",
    "    y = np.zeros(shape=(nu_emotion, number_examples))\n",
    "    counter = 0\n",
    "    for i in description:\n",
    "        X0 = np.zeros((number_examples,1))\n",
    "        if i == 'neutral':\n",
    "            y[0][counter] = 1\n",
    "        elif i == 'disgust':\n",
    "             y[1][counter] = 1\n",
    "        elif i == 'panic':\n",
    "             y[2][counter] = 1\n",
    "        elif i == 'anxiety':\n",
    "             y[3][counter] = 1\n",
    "        elif i == 'hot anger':\n",
    "             y[4][counter] = 1\n",
    "        elif i == 'cold anger':\n",
    "             y[5][counter] = 1\n",
    "        elif i == 'despair':\n",
    "             y[6][counter] = 1\n",
    "        elif i == 'sadness':\n",
    "             y[7][counter] = 1\n",
    "        elif i == 'elation':\n",
    "             y[8][counter] = 1\n",
    "        elif i == 'happy':\n",
    "             y[9][counter] = 1\n",
    "        elif i == 'interest':\n",
    "             y[10][counter] = 1\n",
    "        elif i == 'boredom':\n",
    "             y[11][counter] = 1\n",
    "        elif i == 'shame':\n",
    "             y[12][counter] = 1\n",
    "        elif i == 'pride':\n",
    "             y[13][counter] = 1\n",
    "        elif i == 'contempt':\n",
    "             y[14][counter] = 1\n",
    "        else:\n",
    "            print(i)\n",
    "            break\n",
    "        counter +=1\n",
    "    y = np.transpose(y)\n",
    "    return y\n",
    "def emotion_extraction_RAVDESS(folder_location, number_examples):\n",
    "    nu_emotion = 8\n",
    "    y = np.zeros(shape=(nu_emotion*2, number_examples))\n",
    "    counter = 0\n",
    "    directory = os.fsencode(folder_location)\n",
    "    for file_name in os.listdir(directory):\n",
    "        if  str(file_name) != \"b'.DS_Store'\":\n",
    "            emotion = str(file_name)[2:-1]\n",
    "            i = (emotion.split('-')[2])\n",
    "            male = int(emotion.split('-')[-1].split('.')[0])%2\n",
    "            if i == '01': # neutral \n",
    "                if male:\n",
    "                    y[0][counter] = 1 # neutral male\n",
    "                else:\n",
    "                    y[1][counter] = 1\n",
    "            elif i == '02': #calm\n",
    "                if male:\n",
    "                    y[2][counter] = 1 # neutral male\n",
    "                else:\n",
    "                    y[3][counter] = 1\n",
    "            elif i == '03': #hahppy \n",
    "                if male:\n",
    "                    y[4][counter] = 1 # neutral male\n",
    "                else:\n",
    "                    y[5][counter] = 1\n",
    "            elif i == '04': #sad\n",
    "                if male:\n",
    "                    y[6][counter] = 1 # neutral male\n",
    "                else:\n",
    "                    y[7][counter] = 1\n",
    "            elif i == '05': #angry\n",
    "                if male:\n",
    "                    y[8][counter] = 1 # neutral male\n",
    "                else:\n",
    "                    y[9][counter] = 1\n",
    "            elif i == '06': #fearful\n",
    "                if male:\n",
    "                    y[10][counter] = 1 # neutral male\n",
    "                else:\n",
    "                    y[11][counter] = 1\n",
    "            elif i == '07': #disgust\n",
    "                if male:\n",
    "                    y[12][counter] = 1 # neutral male\n",
    "                else:\n",
    "                    y[13][counter] = 1\n",
    "            elif i == '08': #surprised\n",
    "                if male:\n",
    "                    y[14][counter] = 1 # neutral male\n",
    "                else:\n",
    "                    y[15][counter] = 1\n",
    "            else:\n",
    "                print(i)\n",
    "                break\n",
    "            counter +=1\n",
    "    y = np.transpose(y)\n",
    "    print(counter)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extract_CREMA():\n",
    "EMOTIONS = {\n",
    "    \"ANG\": 0,\n",
    "    \"DIS\": 1,\n",
    "    \"FEA\": 2, ad\n",
    "    \"HAP\": 3,\n",
    "    \"NEU\": 4,\n",
    "    \"SAD\": 5,\n",
    "}\n",
    "LOCAL_PATH = \"../../CREMA/\"\n",
    "WAV_PATH = LOCAL_PATH + \"AudioWAV/\"\n",
    "demographics = pd.read_csv(LOCAL_PATH + \"VideoDemographics.csv\")\n",
    "\n",
    "NUM_SAMPLES = 7442\n",
    "NUM_EMOTIONS = 6\n",
    "fs = 0\n",
    "dataset = []\n",
    "y = np.zeros((NUM_SAMPLES, NUM_EMOTIONS*2))\n",
    "\n",
    "counter = 0\n",
    "for file in os.listdir(WAV_PATH):\n",
    "    if file.endswith('.wav'):\n",
    "        data, fs = librosa.load(WAV_PATH + file, sr=None)\n",
    "        data = butter_bandpass_filter(data, BANDPASS_FREQ[0], BANDPASS_FREQ[1], fs)\n",
    "        data = normalize(data)\n",
    "        data = np.asarray(data)\n",
    "        dataset.append(data)\n",
    "        #Get actor ID from filename\n",
    "        actor_id = int(file[0:4])\n",
    "        #Get the emotion which is in the filename\n",
    "        emotion = file[9:12]\n",
    "        #get the gender from demographics pd dataframe. 0 for Male, 1 for female\n",
    "        gender = 0\n",
    "        gender = 0 if demographics[\"Sex\"][actor_id - 1001] == \"Male\" else 1\n",
    "        emotion_index = EMOTIONS[emotion] + gender*NUM_EMOTIONS\n",
    "\n",
    "        y[counter][emotion_index] = 1\n",
    "        counter+=1\n",
    "    return dataset, fs, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extract_tess():\n",
    "    EMOTIONS = {\n",
    "        \"angry\": 0,\n",
    "        \"disgust\": 1,\n",
    "        \"fear\": 2,\n",
    "        \"happy\": 3,\n",
    "        \"neutral\": 4,\n",
    "        \"ps\": 5,       #Pleasant surprise\n",
    "        \"sad\": 6,\n",
    "    }\n",
    "    LOCAL_PATH = \"../../TESS/\"\n",
    "    NUM_SAMPLES = 2800\n",
    "    NUM_EMOTIONS = 7\n",
    "    fs = 0\n",
    "    dataset = []\n",
    "    y = np.zeros((NUM_SAMPLES, NUM_EMOTIONS))\n",
    "\n",
    "    counter = 0\n",
    "    for file in os.listdir(LOCAL_PATH):\n",
    "        if file.endswith('.wav'):\n",
    "            data, fs = librosa.load(LOCAL_PATH + file, sr=None)\n",
    "            data = butter_bandpass_filter(data, BANDPASS_FREQ[0], BANDPASS_FREQ[1], fs)\n",
    "            data = normalize(data)\n",
    "            data = np.asarray(data)\n",
    "            dataset.append(data)\n",
    "            #Get the emotion by looking at the word in between the second _ and . in the filename\n",
    "            emotion = file[file.index('_', 4) + 1 : file.index('.')]\n",
    "            emotion_index = EMOTIONS[emotion]\n",
    "            y[counter][emotion_index] = 1\n",
    "            counter+=1\n",
    "    return dataset, fs, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ready Dataset and output\n",
    "Put all of the extracted features into X and the classifications into y and split into training and testing group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_y_split(data, fs, y):\n",
    "    x = MFCC_algorithm(data, fs)\n",
    "    print(\"MFCC DONE\")\n",
    "    print(\"dimensions are \"+str([len(x), len(x[0])]))\n",
    "    x1 = get_pitch_stats(data, fs)\n",
    "    print(\"pitch DONE\")\n",
    "    print(\"dimensions are \"+str([len(x1), len(x1[0])]))\n",
    "    x2 = get_spectral_stats(data, fs)\n",
    "    print(\"spectral DONE\")\n",
    "    print(\"dimensions are \"+str([len(x2), len(x2[0])]))\n",
    "#     x3 = get_lpc_stats(data)\n",
    "#     print(\"lpc DONE\")\n",
    "#     print(\"dimensions are \"+str([len(x3), len(x3[0])]))\n",
    "    x4 = get_rms_stats(data)\n",
    "    print(\"rms DONE\")\n",
    "    print(\"dimensions are \"+str([len(x4), len(x4[0])]))\n",
    "    x5 = get_sr_stats(data)\n",
    "    print(\"sr DONE\")\n",
    "    print(\"dimensions are \"+str([len(x5), len(x5[0])]))\n",
    "    x6 = get_zero_stats(data)\n",
    "    print(\"zero DONE\")\n",
    "    print(\"dimensions are \"+str([len(x6), len(x6[0])]))\n",
    "    x = np.concatenate((x,x1,x2,x4,x5,x6), axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "    num_labels = y_train.shape[1]\n",
    "    num_features = X_train.shape[1]\n",
    "    print(\"x train shape: \" +str(X_train.shape))\n",
    "    print(\"y train shape: \" +str(y_train.shape))\n",
    "    print(\"x test shape: \" +str(X_test.shape))\n",
    "    print(\"y test shape: \" +str(y_test.shape))\n",
    "    for i in range(num_labels):\n",
    "        print(\"y_train for emotion \"+str(i)+\": \"+ str(np.sum(y_train[:,i])))\n",
    "    for i in range(num_labels): \n",
    "        print(\"y_test for emotion \"+str(i)+\": \"+ str(np.sum(y_test[:,i])))\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
