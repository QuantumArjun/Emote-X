{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import osascript\n",
    "from gtts import gTTS \n",
    "import os \n",
    "import pyaudio\n",
    "import wave\n",
    "import keyboard  # using module keyboard\n",
    "import soundfile as sf\n",
    "import math\n",
    "import pyloudnorm as pyln\n",
    "import sys\n",
    "from sys import byteorder\n",
    "from array import array\n",
    "from struct import pack\n",
    "import librosa\n",
    "from scipy.signal import butter, sosfiltfilt\n",
    "import python_speech_features\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "import pysptk\n",
    "from  conch.analysis.formants import lpc\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BANDPASS_FREQ = [300, 3400]\n",
    "NUM_EMOTIONS = 6\n",
    "NUM_FEATURES = 131\n",
    "NUM_SAMPLES_CREMA = 7441\n",
    "NUM_SAMPLES_TESS = 2400\n",
    "NUM_SAMPLES_RAVDESS = 1196\n",
    "NUM_SAMPLES_LDC = 1379\n",
    "NUM_SAMPLES = NUM_SAMPLES_CREMA + NUM_SAMPLES_TESS + NUM_SAMPLES_RAVDESS + NUM_SAMPLES_LDC\n",
    "#CHUNK_LENGTH = 512 #ms\n",
    "FS = 16000\n",
    "\n",
    "#global variables\n",
    "mean_vector = []\n",
    "std_vector = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress(count, total, status=''):\n",
    "    bar_len = 60\n",
    "    filled_len = int(round(bar_len * count / float(total)))\n",
    "\n",
    "    percents = round(100.0 * count / float(total), 1)\n",
    "    bar = '=' * filled_len + '-' * (bar_len - filled_len)\n",
    "\n",
    "    sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', status))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTIONS = {\n",
    "    #CREMA\n",
    "    \"ANG\": 0,\n",
    "    \"DIS\": 1,\n",
    "    \"FEA\": 2,\n",
    "    \"HAP\": 3,\n",
    "    \"NEU\": 4,\n",
    "    \"SAD\": 5,\n",
    "    #TESS\n",
    "    \"angry\": 0,\n",
    "    \"disgust\": 1,\n",
    "    \"fear\": 2,\n",
    "    \"happy\": 3,\n",
    "    \"neutral\": 4,\n",
    "    \"sad\": 5,\n",
    "    #RAVDESS\n",
    "    \"05\": 0, #angry ravdess\n",
    "    \"07\": 1, #disgust ravdess\n",
    "    \"06\": 2, #fear ravdess\n",
    "    \"03\": 3, #happy ravdess\n",
    "    \"01\": 4, #neutral ravdess\n",
    "    \"02\": 4, #calm ravdess (neutral)\n",
    "    \"04\": 5, #sad ravdess\n",
    "    \n",
    "    #LDC\n",
    "    \"panic\": 2,   #fear LDC\n",
    "    \"hot anger\": 0,\n",
    "    \"cold anger\": 0,\n",
    "    \"despair\": 5,\n",
    "    \"sadness\": 5,\n",
    "    \"elation\": 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotion_vector(emotion_index, gender):\n",
    "    vec = np.zeros((1, NUM_EMOTIONS*2))\n",
    "    vec[0][emotion_index + gender*NUM_EMOTIONS] = 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    sos = butter(order, [low, high], btype='band', analog=False, output='sos')\n",
    "    return sos\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    sos = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = sosfiltfilt(sos, data)\n",
    "    return y\n",
    "def normalize(snd_data):\n",
    "    \"Average the volume out\"\n",
    "    MAXIMUM = 16384\n",
    "    times = float(MAXIMUM)/max(abs(i) for i in snd_data)\n",
    "\n",
    "    r = array('h')\n",
    "    for i in snd_data:\n",
    "        r.append(int(i*times))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_silence_from(amplitudes, threshold):\n",
    "    silenced = []\n",
    "    for x in amplitudes:\n",
    "        if x >= threshold:\n",
    "            silenced.append(x)\n",
    "    return silenced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sound(data, fs):\n",
    "    data = butter_bandpass_filter(data, BANDPASS_FREQ[0], BANDPASS_FREQ[1], fs)\n",
    "    data = normalize(data)\n",
    "    data = np.asarray(data)\n",
    "    return data, fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Individual Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extract_LDC(npload=False, CHUNK_LENGTH=512):\n",
    "    LOCAL_PATH = \"../../LDC/\"\n",
    "    \n",
    "    dataset = []\n",
    "    y = np.empty((0, NUM_EMOTIONS*2), int)\n",
    "    \n",
    "    if not npload:\n",
    "        prog = 0 #for the progress bar\n",
    "        for file in os.listdir(LOCAL_PATH + \"transcr/\"): #loop through each text file\n",
    "            if file.endswith(\".txt\"):\n",
    "                metadata = pd.read_csv(LOCAL_PATH + \"transcr/\" + file, sep=\"A:\", skiprows=1, header=None, engine=\"python\")\n",
    "                #remove non-data entries\n",
    "                metadata = metadata[metadata[1] != ' [MISC]']\n",
    "                metadata = metadata[metadata[1].str.startswith(' (') == False]\n",
    "\n",
    "                #split into start, stop, and emotion\n",
    "                metadata[2] = metadata[0].str.split(' ', expand=True)[0]\n",
    "                metadata[3] = metadata[0].str.split(' ', expand=True)[1]\n",
    "                metadata[4] = metadata[1].str.split(',', expand=True)[0]\n",
    "                metadata[4] = metadata[4].str[1:]\n",
    "                metadata = metadata.drop([0,1], axis=1)\n",
    "                metadata.columns = [\"Start\", \"Stop\", \"Emotion\"]\n",
    "\n",
    "                #load sound file\n",
    "                soundfile_name = file[:-3] + \"wav\"\n",
    "                soundfile, fs = librosa.load(LOCAL_PATH + \"speech/\" + soundfile_name)\n",
    "\n",
    "                for index, row in metadata.iterrows(): #loop through each sound clip\n",
    "                    emotion = row.Emotion\n",
    "                    #Check if we're using this emotion\n",
    "                    e = EMOTIONS.get(emotion, None)\n",
    "                    if(e != None):\n",
    "                        progress(prog, NUM_SAMPLES_LDC, status=\"Reading files\")\n",
    "                        #male = 0, female  = 1\n",
    "                        gender = 0 if (file.startswith(\"cc\") or\n",
    "                                       file.startswith(\"mf\") or\n",
    "                                       file.startswith(\"cl\")) else 1\n",
    "                                              \n",
    "                        #process sound\n",
    "                        data = soundfile[int(float(metadata.Start[index])*fs):int(float(metadata.Stop[index])*fs)]\n",
    "                        data = librosa.resample(data, fs, FS)\n",
    "                        fs = FS\n",
    "                        data, fs = clean_sound(data, fs)\n",
    "\n",
    "                        chunk_size = int(CHUNK_LENGTH * fs / 1000)\n",
    "                        num_chunks = int(len(data) // chunk_size)\n",
    "                        for i in range(num_chunks):\n",
    "                            snippet = data[i*chunk_size:i*chunk_size+chunk_size]\n",
    "                            y = np.append(y, e+ gender*NUM_EMOTIONS)\n",
    "                            dataset.append(snippet)\n",
    "                        if (len(data) % chunk_size) > 0:\n",
    "                            snippet = data[num_chunks*chunk_size:]\n",
    "                            snippet = np.resize(snippet, chunk_size)\n",
    "                            y = np.append(y, e + gender*NUM_EMOTIONS)\n",
    "                            dataset.append(snippet)\n",
    "                        prog+=1 #for progress bar\n",
    "        dataset = np.array(dataset)\n",
    "        print(\"dataset shape: \" + str(dataset.shape))\n",
    "        np.save(LOCAL_PATH + \"LDCdatafsy\", (dataset, y, 0))\n",
    "    else:\n",
    "        dataset, y, useless = np.load(LOCAL_PATH + \"LDCdatafsy.npy\", allow_pickle=True)\n",
    "    #get feature vector\n",
    "    print(\"Getting all features...\")\n",
    "    #features = get_all_features(np.array(dataset), fs)\n",
    "    features = np.array(dataset)\n",
    "    print(\"Feature Vector Shape: \" + str(features.shape))\n",
    "    return features, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extract_RAVDESS(npload=False, CHUNK_LENGTH=512):\n",
    "    LOCAL_PATH = '../../RAVDESS/'\n",
    "    \n",
    "    dataset = []\n",
    "    y = np.empty((0, NUM_EMOTIONS*2), int)\n",
    "    \n",
    "    if not npload:\n",
    "        prog = 0 # for progress bar\n",
    "        for file in os.listdir(LOCAL_PATH):\n",
    "            if  file.endswith('.wav'):\n",
    "                emotion = (file.split('-')[2])\n",
    "                #Check if we're using this emotion\n",
    "                e = EMOTIONS.get(emotion, None)\n",
    "                if(e != None):\n",
    "                    progress(prog, NUM_SAMPLES_RAVDESS, status=\"Reading files\")\n",
    "                    #male = 0, female  = 1\n",
    "                    gender = (int(file.split('-')[-1].split('.')[0]) + 1) % 2\n",
    "                    \n",
    "                    #process sound\n",
    "                    data, fs = librosa.load(LOCAL_PATH + file, sr=None)\n",
    "                    data = librosa.resample(data, fs, FS)\n",
    "                    fs = FS\n",
    "                    data, fs = clean_sound(data, fs)\n",
    "                    \n",
    "                    chunk_size = int(CHUNK_LENGTH * fs / 1000)\n",
    "                    num_chunks = int(len(data) // chunk_size)\n",
    "                    for i in range(num_chunks):\n",
    "                        snippet = data[i*chunk_size:i*chunk_size+chunk_size]\n",
    "                        y = np.append(y, e+ gender*NUM_EMOTIONS)\n",
    "                        dataset.append(snippet)\n",
    "                    if (len(data) % chunk_size) > 0:\n",
    "                        snippet = data[num_chunks*chunk_size:]\n",
    "                        snippet = np.resize(snippet, chunk_size)\n",
    "                        y = np.append(y, e + gender*NUM_EMOTIONS)\n",
    "                        dataset.append(snippet)\n",
    "                    prog+=1 #for progress bar\n",
    "        np.save(LOCAL_PATH + \"RAVDESSdatafsy\", (dataset, y, 0))\n",
    "    else:\n",
    "        dataset, y, useless= np.load(LOCAL_PATH + \"RAVDESSdatafsy.npy\", allow_pickle=True)\n",
    "    #get feature vector\n",
    "    print(\"Getting all features...\")\n",
    "    #features = get_all_features(np.array(dataset), fs)\n",
    "    features = np.array(dataset)\n",
    "    print(\"Feature Vector Shape: \" + str(features.shape))\n",
    "    return features, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extract_CREMA(npload=False, CHUNK_LENGTH=512):\n",
    "    LOCAL_PATH = \"../../CREMA/\"\n",
    "    WAV_PATH = LOCAL_PATH + \"AudioWAV/\"\n",
    "    demographics = pd.read_csv(LOCAL_PATH + \"VideoDemographics.csv\")\n",
    "    \n",
    "    dataset = []\n",
    "    y = np.empty((0, 1), int)\n",
    "    \n",
    "    if not npload:\n",
    "        prog = 0 # for progress bar\n",
    "        for file in os.listdir(WAV_PATH):\n",
    "            if file.endswith('.wav'):\n",
    "                emotion = file[9:12]\n",
    "                #Check if we're using this emotion\n",
    "                e = EMOTIONS.get(emotion, None)\n",
    "                if(e != None):\n",
    "                    progress(prog, NUM_SAMPLES_CREMA, status=\"Reading files\")\n",
    "                     #Get actor ID from filename\n",
    "                    actor_id = int(file[0:4])\n",
    "                    #get the gender from demographics pd dataframe. 0 for Male, 1 for female\n",
    "                    gender = 0 if demographics[\"Sex\"][actor_id - 1001] == \"Male\" else 1\n",
    "                    \n",
    "                    #process sound\n",
    "                    data, fs = librosa.load(WAV_PATH + file, sr=None)\n",
    "                    data = librosa.resample(data, fs, FS)\n",
    "                    fs = FS\n",
    "                    data, fs = clean_sound(data, fs)\n",
    "                    \n",
    "                    chunk_size = int(CHUNK_LENGTH * fs / 1000)\n",
    "                    num_chunks = int(len(data) // chunk_size)\n",
    "                    for i in range(num_chunks):\n",
    "                        snippet = data[i*chunk_size:i*chunk_size+chunk_size]\n",
    "                        y = np.append(y, e+ gender*NUM_EMOTIONS)\n",
    "                        dataset.append(snippet)\n",
    "                    if (len(data) % chunk_size) > 0:\n",
    "                        snippet = data[num_chunks*chunk_size:]\n",
    "                        snippet = np.resize(snippet, chunk_size)\n",
    "                        y = np.append(y, e + gender*NUM_EMOTIONS)\n",
    "                        dataset.append(snippet)\n",
    "                    prog+=1 #for progress bar\n",
    "        np.save(LOCAL_PATH + \"CREMAdatafsy\", (dataset, y, 0))\n",
    "    else:\n",
    "        dataset, y, useless = np.load(LOCAL_PATH + \"CREMAdatafsy.npy\", allow_pickle=True)\n",
    "    #get feature vector\n",
    "    print(\"Getting all features...\")\n",
    "    #features = get_all_features(np.array(dataset), fs)\n",
    "    features = np.array(dataset)\n",
    "    print(\"Feature Vector Shape: \" + str(features.shape))\n",
    "    return features, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extract_TESS(npload=False, CHUNK_LENGTH=512):\n",
    "    LOCAL_PATH = \"../../TESS/\"\n",
    "    \n",
    "    dataset = []\n",
    "    y = np.empty((0, NUM_EMOTIONS*2), int)\n",
    "    \n",
    "    if not npload:\n",
    "        prog = 0 # for progress bar\n",
    "        for file in os.listdir(LOCAL_PATH):\n",
    "            if file.endswith('.wav'):\n",
    "                emotion = file[file.index('_', 4) + 1 : file.index('.')]\n",
    "                #Check if we're using this emotion\n",
    "                e = EMOTIONS.get(emotion, None)\n",
    "                if(e != None):\n",
    "                    progress(prog, NUM_SAMPLES_TESS, status=\"Reading files\")\n",
    "                    #TESS is all female so gender = 1\n",
    "                    gender = 1\n",
    "                    \n",
    "                    #process sound\n",
    "                    data, fs = librosa.load(LOCAL_PATH + file, sr=None)\n",
    "                    data = librosa.resample(data, fs, FS)\n",
    "                    fs = FS\n",
    "                    data, fs = clean_sound(data, fs)\n",
    "                    \n",
    "                    chunk_size = int(CHUNK_LENGTH * fs / 1000)\n",
    "                    num_chunks = int(len(data) // chunk_size)\n",
    "                    for i in range(num_chunks):\n",
    "                        snippet = data[i*chunk_size:i*chunk_size+chunk_size]\n",
    "                        y = np.append(y, e+ gender*NUM_EMOTIONS)\n",
    "                        dataset.append(snippet)\n",
    "                    if (len(data) % chunk_size) > 0:\n",
    "                        snippet = data[num_chunks*chunk_size:]\n",
    "                        snippet = np.resize(snippet, chunk_size)\n",
    "                        y = np.append(y, e + gender*NUM_EMOTIONS)\n",
    "                        dataset.append(snippet)\n",
    "                    prog+=1 #for progress bar\n",
    "        np.save(LOCAL_PATH + \"TESSdatafsy\", (dataset, y, 0))\n",
    "    else:\n",
    "        dataset, y, useless = np.load(LOCAL_PATH + \"TESSdatafsy.npy\", allow_pickle=True)\n",
    "    #get feature vector\n",
    "    print(\"Getting all features...\")\n",
    "    #features = get_all_features(np.array(dataset), fs)\n",
    "    features = np.array(dataset)\n",
    "    print(\"Feature Vector Shape: \" + str(features.shape))\n",
    "    return features, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-13-dfd0c8fef4bd>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-dfd0c8fef4bd>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    chunk_arr, y_arr = data_extract_LDC(npload=False, CHUNK_LENGTH)\u001b[0m\n\u001b[0m                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "def data_extract_all(CHUNK_LENGTH=512):\n",
    "    y = np.empty((0, 1), int)\n",
    "    chunked = np.empty((0, int(CHUNK_LENGTH * FS / 1000)))\n",
    "    print(\"----------------------------------------------\\n\")\n",
    "    print(\"Extracting LDC....\")\n",
    "    chunk_arr, y_arr = data_extract_LDC(npload=False, CHUNK_LENGTH)\n",
    "    print(chunked.shape)\n",
    "    print(chunk_arr.shape)\n",
    "    chunked = np.append(chunked, chunk_arr, axis=0)\n",
    "    y = np.append(y, y_arr)\n",
    "    print(\"Done extracting LDC\")\n",
    "    print(\"----------------------------------------------\\n\")\n",
    "    print(\"Extracting CREMA....\")\n",
    "    chunk_arr, y_arr = data_extract_CREMA(npload=False, CHUNK_LENGTH)\n",
    "    chunked = np.append(chunked, chunk_arr, axis=0)\n",
    "    y = np.append(y, y_arr)\n",
    "    print(\"Done extracting CREMA\")\n",
    "    print(\"\\n----------------------------------------------\\n\")\n",
    "    print(\"Extracting TESS....\")\n",
    "    chunk_arr, y_arr = data_extract_TESS(npload=False, CHUNK_LENGTH)\n",
    "    chunked = np.append(chunked, chunk_arr, axis=0)\n",
    "    y = np.append(y, y_arr)\n",
    "    print(\"Done extracting TESS\")\n",
    "    print(\"\\n----------------------------------------------\\n\")\n",
    "    print(\"Extracting RAVDESS....\")\n",
    "    chunk_arr, y_arr = data_extract_RAVDESS(npload=False, CHUNK_LENGTH=1000)\n",
    "    chunked = np.append(chunked, chunk_arr, axis=0)\n",
    "    y = np.append(y, y_arr)\n",
    "    print(\"Done extracting RAVDESS\")\n",
    "    print(\"\\n----------------------------------------------\")\n",
    "#     mean_vector = np.mean(features, axis=0)\n",
    "#     std_vector = np.std(features, axis=0)\n",
    "    \n",
    "    #normalize the data\n",
    "    #features = (features - mean_vector) / std_vector\n",
    "    return chunked, y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MFCC_algorithm(np_data, fs):\n",
    "    MFCC2 = []\n",
    "    #for progess bar\n",
    "    i = 0\n",
    "    prog = np_data.shape[0]\n",
    "    for one_sound in np_data:\n",
    "        progress(i, prog, \"Calculating MFCC's\")\n",
    "        one_sound = np.asarray(one_sound)\n",
    "        MFCC2.append(python_speech_features.base.mfcc(one_sound, samplerate=fs, \n",
    "                                     winlen=0.025, winstep=0.01, numcep=13, \n",
    "                                     nfilt=26, nfft=1200).T)\n",
    "        i+=1\n",
    "    MFCC3 = []\n",
    "    cached_variables = []\n",
    "    for one_point in MFCC2:\n",
    "        cache_grad = (np.gradient(one_point, axis = 1))\n",
    "        cached_variables.append(np.asarray([np.mean(one_point, axis = 1), np.median(one_point, axis = 1),\n",
    "                                 np.var(one_point, axis = 1), \n",
    "                           np.min(one_point, axis = 1), np.max(one_point, axis = 1), \n",
    "                                 np.mean(cache_grad, axis = 1), np.var(cache_grad, axis = 1)]).flatten()\n",
    "                               )\n",
    "    return np.array(cached_variables)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pitch_vector(data, fs):\n",
    "    data = np.float32(data)\n",
    "    pitch = pysptk.sptk.rapt(data, fs, hopsize = 50)\n",
    "    silenced = remove_silence_from(pitch, np.mean(pitch))\n",
    "    return silenced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectral_vector(data, fs):\n",
    "    data = np.float32(data)\n",
    "    cent = librosa.feature.spectral_centroid(y=data, sr=fs)\n",
    "    return cent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lpc_vector(data):\n",
    "    vec = lpc.lpc_ref(data, 12)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rms_vector(data):\n",
    "    temp_data = np.float32(data)\n",
    "    cent = librosa.feature.rms(y=temp_data)\n",
    "    return cent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zero_vector(data):\n",
    "    temp_data = np.float32(data)\n",
    "    cent = librosa.feature.zero_crossing_rate(y=temp_data)\n",
    "    return cent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sr_vector(data):\n",
    "    temp_data = np.float32(data)\n",
    "    cent = librosa.feature.spectral_rolloff(y=temp_data)\n",
    "    return cent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(pitch_vector):\n",
    "    mean = np.mean(pitch_vector)\n",
    "    median = np.median(pitch_vector)\n",
    "    low = np.min(pitch_vector)\n",
    "    high = np.max(pitch_vector)\n",
    "    variance = np.var(pitch_vector)\n",
    "    \n",
    "    #derivative\n",
    "    derivative = np.diff(pitch_vector)\n",
    "    d_mean = np.mean(derivative)\n",
    "    d_min = np.min(derivative)\n",
    "    d_max = np.max(derivative)\n",
    "    return [mean, median, low, high, variance, d_mean, d_min, d_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_features(np_array, fs):\n",
    "    x = MFCC_algorithm(np_array, fs)\n",
    "    print(\"MFCC DONE\", end=\" \")\n",
    "    print(\"dimensions are \"+str([len(x), len(x[0])]))\n",
    "    x1 = []\n",
    "    x2 = []\n",
    "    x3 = []\n",
    "    x4 = []\n",
    "    x5 = []\n",
    "    #for progress bar\n",
    "    size = np_array.shape[0]\n",
    "    i = 0\n",
    "    for data in np_array:\n",
    "        progress(i, size, status=\"Calculating stats\")\n",
    "        pitch_vector = get_pitch_vector(data, fs)\n",
    "        stats = get_stats(pitch_vector)\n",
    "        x1.append(stats)\n",
    "        \n",
    "        spectral_vector = get_spectral_vector(data, fs)\n",
    "        stats = get_stats(spectral_vector)\n",
    "        x2.append(stats)\n",
    "        \n",
    "        rms_vector = get_rms_vector(data)\n",
    "        stats = get_stats(rms_vector)\n",
    "        x3.append(stats)\n",
    "        \n",
    "        sr_vector = get_sr_vector(data)\n",
    "        stats = get_stats(sr_vector)\n",
    "        x4.append(stats)\n",
    "    \n",
    "        zero_vector = get_zero_vector(data)\n",
    "        stats = get_stats(zero_vector)\n",
    "        x5.append(stats)\n",
    "        \n",
    "        i+=1\n",
    "    print(\"MFCC dimensions:\" + str([len(x), len(x[0])]))\n",
    "    print(\"Pitch dimensions:\" + str([len(x1), len(x1[0])]))\n",
    "    print(\"Spectral dimensions:\" + str([len(x2), len(x2[0])]))\n",
    "    print(\"RMS dimensions:\" + str([len(x3), len(x3[0])]))\n",
    "    print(\"SR dimensions:\" + str([len(x4), len(x4[0])]))\n",
    "    print(\"Zero dimensions:\" + str([len(x5), len(x5[0])]))\n",
    "    x = np.concatenate((x,x1,x2,x3,x4,x5), axis=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ready Dataset and output\n",
    "Put all of the extracted features into X and the classifications into y and split into training and testing group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_y_split(x, y):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5)\n",
    "    \n",
    "    num_labels = y_train.shape[1]\n",
    "    num_features = X_train.shape[1]\n",
    "    print(\"x train shape: \" +str(X_train.shape))\n",
    "    print(\"y train shape: \" +str(y_train.shape))\n",
    "    print(\"x test shape: \" +str(X_test.shape))\n",
    "    print(\"y test shape: \" +str(y_test.shape))\n",
    "    print(\"x validation shape: \" +str(X_val.shape))\n",
    "    print(\"y validation shape: \" +str(y_val.shape))\n",
    "    for i in range(num_labels):\n",
    "        print(\"y_train for emotion \"+str(i)+\": \"+ str(np.sum(y_train[:,i])))\n",
    "    for i in range(num_labels): \n",
    "        print(\"y_test for emotion \"+str(i)+\": \"+ str(np.sum(y_test[:,i])))\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_extract_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-91c7d779cf92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_extract_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHUNK_LENGTH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data_extract_all' is not defined"
     ]
    }
   ],
   "source": [
    "features, y = data_extract_all(CHUNK_LENGTH=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting all features...======================================] 100.0% ...Reading files\n",
      "Feature Vector Shape: (40624, 8192)\n"
     ]
    }
   ],
   "source": [
    "chunk_arr, y_arr = data_extract_CREMA(npload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[    7,    -3,    -7, ...,    47,   -71,  -113],\n",
       "        [  -86,   -24,    67, ...,   845,   170,  -124],\n",
       "        [  186,   604,   624, ..., -1955, -1108,  -103],\n",
       "        ...,\n",
       "        [-1610,  -631,   288, ...,    -3,    44,   144],\n",
       "        [  229,   232,   127, ...,   -26,   -50,   -81],\n",
       "        [  -88,   -61,   -18, ...,   -76,   -75,   -73]], dtype=int16),\n",
       " array([ 0,  0,  0, ..., 10, 10, 10]),\n",
       " array([16000., 16000., 16000., ..., 16000., 16000., 16000.]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_arr, y_arr, fs_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65504, 8192)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.append(y, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../../CREMA_chunked\", (features, y, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, y, useless_number = np.load(\"../../CREMA_chunked.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0][2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.zeros((1,4))\n",
    "y[0][1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 2])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(\"../../most_of_the_data\", (y,features, 0))\n",
    "\n",
    "#X_train, X_val, X_test, y_train, y_val, y_test = x_y_split(features, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(\"../../splitdata\", (X_train, X_val, X_test, y_train, y_val, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_val, X_test, y_train, y_val, y_test = np.load(\"../../splitdata.npy\", allow_pickle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
