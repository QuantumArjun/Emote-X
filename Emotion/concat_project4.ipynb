{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import osascript\n",
    "from gtts import gTTS \n",
    "import os \n",
    "import pyaudio\n",
    "import wave\n",
    "import keyboard  # using module keyboard\n",
    "import soundfile as sf\n",
    "import math\n",
    "import pyloudnorm as pyln\n",
    "import sys\n",
    "from sys import byteorder\n",
    "from array import array\n",
    "from struct import pack\n",
    "import librosa\n",
    "from scipy.signal import butter, sosfiltfilt\n",
    "import python_speech_features\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "import pysptk\n",
    "from  conch.analysis.formants import lpc\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "BANDPASS_FREQ = [300, 3400]\n",
    "NUM_EMOTIONS = 6\n",
    "NUM_SAMPLES_CREMA = 7441\n",
    "NUM_SAMPLES_TESS = 2400\n",
    "NUM_SAMPLES_RAVDESS = 1196\n",
    "NUM_SAMPLES_LDC = 1379\n",
    "NUM_SAMPLES = NUM_SAMPLES_CREMA + NUM_SAMPLES_TESS + NUM_SAMPLES_RAVDESS + NUM_SAMPLES_LDC\n",
    "\n",
    "EMOTIONS = {\n",
    "    #CREMA\n",
    "    \"ANG\": 0,\n",
    "    \"DIS\": 1,\n",
    "    \"FEA\": 2,\n",
    "    \"HAP\": 3,\n",
    "    \"NEU\": 4,\n",
    "    \"SAD\": 5,\n",
    "    #TESS\n",
    "    \"angry\": 0,\n",
    "    \"disgust\": 1,\n",
    "    \"fear\": 2,\n",
    "    \"happy\": 3,\n",
    "    \"neutral\": 4,\n",
    "    \"sad\": 5,\n",
    "    #RAVDESS\n",
    "    \"05\": 0, #angry ravdess\n",
    "    \"07\": 1, #disgust ravdess\n",
    "    \"06\": 2, #fear ravdess\n",
    "    \"03\": 3, #happy ravdess\n",
    "    \"01\": 4, #neutral ravdess\n",
    "    \"02\": 4, #calm ravdess (neutral)\n",
    "    \"04\": 5, #sad ravdess\n",
    "    \n",
    "    #LDC\n",
    "    \"panic\": 2,   #fear LDC\n",
    "    \"hot anger\": 0,\n",
    "    \"cold anger\": 0,\n",
    "    \"despair\": 5,\n",
    "    \"sadness\": 5,\n",
    "    \"elation\": 3\n",
    "}\n",
    "\n",
    "#global variables\n",
    "mean_vector = []\n",
    "std_vector = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress(count, total, status=''):\n",
    "    bar_len = 60\n",
    "    filled_len = int(round(bar_len * count / float(total)))\n",
    "\n",
    "    percents = round(100.0 * count / float(total), 1)\n",
    "    bar = '=' * filled_len + '-' * (bar_len - filled_len)\n",
    "\n",
    "    sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', status))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    sos = butter(order, [low, high], btype='band', analog=False, output='sos')\n",
    "    return sos\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    sos = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = sosfiltfilt(sos, data)\n",
    "    return y\n",
    "def normalize(snd_data):\n",
    "    \"Average the volume out\"\n",
    "    MAXIMUM = 16384\n",
    "    times = float(MAXIMUM)/max(abs(i) for i in snd_data)\n",
    "\n",
    "    r = array('h')\n",
    "    for i in snd_data:\n",
    "        r.append(int(i*times))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_silence_from(amplitudes, threshold):\n",
    "    silenced = []\n",
    "    for x in amplitudes:\n",
    "        if x >= threshold:\n",
    "            silenced.append(x)\n",
    "    return silenced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sound(data, fs):\n",
    "    data = butter_bandpass_filter(data, BANDPASS_FREQ[0], BANDPASS_FREQ[1], fs)\n",
    "    data = normalize(data)\n",
    "    data = np.asarray(data)\n",
    "    return data, fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Individual Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(file_location):\n",
    "    BANDPASS_FREQ = [300, 3400]\n",
    "    fs, data = wavfile.read(file_location)\n",
    "    number_of_samples = data.shape[0]\n",
    "    meta_data = open(r\"LDC2002S28band-txt.txt\")\n",
    "    meta_data = pd.read_csv(\"LDC2002S28-txt.txt\", sep=\"A:\", header=None, engine='python')\n",
    "    meta_data.columns = [\"sound limits\",\"description\"]\n",
    "    \n",
    "    #dual channel to one channel\n",
    "    data = np.average(data, axis = 1)\n",
    "    #remove noise\n",
    "    data = butter_bandpass_filter(data, BANDPASS_FREQ[0], BANDPASS_FREQ[1], fs)\n",
    "    \n",
    "    # removee extra data pointts\n",
    "    meta_data = meta_data[meta_data.description != ' [MISC]']\n",
    "    meta_data = meta_data[~meta_data['description'].astype(str).str.startswith(' (')]\n",
    "    meta_data = meta_data[~meta_data['description'].astype(str).str.startswith(' Emotion category elation')]\n",
    "    meta_data = meta_data[~meta_data['description'].astype(str).str.startswith('  [MISC]')]\n",
    "\n",
    "    # description and time limits \n",
    "    voice_time_limits = meta_data[\"sound limits\"]\n",
    "    voice_time_limits = [i.split(\" \")[0:2] for i in voice_time_limits]\n",
    "    voice_time_limits = np.array(voice_time_limits)\n",
    "    voice_time_limits = voice_time_limits.astype(np.float)\n",
    "    description = meta_data[\"description\"]\n",
    "    description = [i.split(\",\")[0].strip() for i in description]\n",
    "\n",
    "    #divide the dataa set\n",
    "    divided_data = []\n",
    "    for i in voice_time_limits:\n",
    "        startingpoint = int(i[0]*fs)\n",
    "        endingpoint = int(i[1]*fs)\n",
    "        divided_data.append(data[startingpoint:endingpoint])\n",
    "    np_data = np.asarray(divided_data)\n",
    "    return np_data, description, len(divided_data), fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extract_LDC(features, y, counter, npload=False):\n",
    "    LOCAL_PATH = \"../../LDC/\"\n",
    "    dataset = []\n",
    "    fs = 0\n",
    "    if not npload:\n",
    "        i = 0 #for the progress bar\n",
    "        for file in os.listdir(LOCAL_PATH + \"transcr/\"): #loop through each text file\n",
    "            if file.endswith(\".txt\"):\n",
    "                metadata = pd.read_csv(LOCAL_PATH + \"transcr/\" + file, sep=\"A:\", skiprows=1, header=None, engine=\"python\")\n",
    "                #remove non-data entries\n",
    "                metadata = metadata[metadata[1] != ' [MISC]']\n",
    "                metadata = metadata[metadata[1].str.startswith(' (') == False]\n",
    "\n",
    "                #split into start, stop, and emotion\n",
    "                metadata[2] = metadata[0].str.split(' ', expand=True)[0]\n",
    "                metadata[3] = metadata[0].str.split(' ', expand=True)[1]\n",
    "                metadata[4] = metadata[1].str.split(',', expand=True)[0]\n",
    "                metadata[4] = metadata[4].str[1:]\n",
    "                metadata = metadata.drop([0,1], axis=1)\n",
    "                metadata.columns = [\"Start\", \"Stop\", \"Emotion\"]\n",
    "\n",
    "                #load sound file\n",
    "                soundfile_name = file[:-3] + \"wav\"\n",
    "                soundfile, freq = librosa.load(LOCAL_PATH + \"speech/\" + soundfile_name)\n",
    "\n",
    "                for index, row in metadata.iterrows(): #loop through each sound clip\n",
    "                    emotion = row.Emotion\n",
    "                    #Check if we're using this emotion\n",
    "                    e = EMOTIONS.get(emotion, None)\n",
    "                    if(e != None):\n",
    "                        progress(i, NUM_SAMPLES_LDC, status=\"Reading files\")\n",
    "                        #male = 0, female  = 1\n",
    "                        gender = 0 if (file.startswith(\"cc\") or\n",
    "                                       file.startswith(\"mf\") or\n",
    "                                       file.startswith(\"cl\")) else 1\n",
    "                        emotion_index = e + gender*NUM_EMOTIONS\n",
    "                        y[counter][emotion_index] = 1\n",
    "                        i+=1\n",
    "                        counter+=1\n",
    "\n",
    "                        #process sound\n",
    "                        data = soundfile[int(float(metadata.Start[index])*freq):int(float(metadata.Stop[index])*freq)]\n",
    "                        data, fs = clean_sound(data, freq)\n",
    "                        dataset.append(data)\n",
    "        np.save(LOCAL_PATH + \"LDCdatafsy\", (dataset, fs, y))\n",
    "        \n",
    "    else:\n",
    "        dataset, fs, y = np.load(LOCAL_PATH + \"LDCdatafsy.npy\", allow_pickle=True)\n",
    "    #get feature vector\n",
    "    print(\"Getting all features...\")\n",
    "    feature_vector = get_all_features(np.array(dataset), fs)\n",
    "    print(\"Feature Vector Shape: \" + str(feature_vector.shape))\n",
    "    \n",
    "    if features.shape[0] == 0:\n",
    "        features = feature_vector\n",
    "    else:\n",
    "        features = np.concatenate((np.array(features), np.array(feature_vector)))\n",
    "    print(\"Features shape: \" + str(features.shape))\n",
    "    return features, y, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../../LDC/speech/\" + \"cc_001.wav\"\n",
    "\n",
    "LOCAL_PATH = \"../../LDC/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "soundfile, fs = librosa.load(LOCAL_PATH + \"speech/\" + PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extract_RAVDESS(features, y, counter, npload=False):\n",
    "    LOCAL_PATH = '../../RAVDESS/'\n",
    "    dataset = []\n",
    "    fs = 0\n",
    "    if not npload:\n",
    "        i = 0 # for progress bar\n",
    "        for file in os.listdir(LOCAL_PATH):\n",
    "            if  file.endswith('.wav'):\n",
    "                emotion = (file.split('-')[2])\n",
    "                #Check if we're using this emotion\n",
    "                e = EMOTIONS.get(emotion, None)\n",
    "                if(e != None):\n",
    "                    progress(i, NUM_SAMPLES_RAVDESS, status=\"Reading files\")\n",
    "                    #male = 0, female  = 1\n",
    "                    gender = (int(file.split('-')[-1].split('.')[0]) + 1) % 2\n",
    "                    emotion_index = e + gender*NUM_EMOTIONS\n",
    "                    y[counter][emotion_index] = 1\n",
    "                    i+=1\n",
    "                    counter+=1\n",
    "                    #process sound\n",
    "                    data, fs = librosa.load(LOCAL_PATH + file, sr=None)\n",
    "                    data, fs = clean_sound(data, fs)\n",
    "                    dataset.append(data)\n",
    "        np.save(LOCAL_PATH + \"RAVDESSdatafsy\", (dataset, fs, y))\n",
    "    else:\n",
    "        dataset, fs, y = np.load(LOCAL_PATH + \"RAVDESSdatafsy.npy\", allow_pickle=True)\n",
    "    #get feature vector\n",
    "    print(\"Getting all features...\")\n",
    "    feature_vector = get_all_features(np.array(dataset), fs)\n",
    "    print(\"Feature Vector Shape: \" + str(feature_vector.shape))\n",
    "    \n",
    "    if features.shape[0] == 0:\n",
    "        features = feature_vector\n",
    "    else:\n",
    "        features = np.concatenate((np.array(features), np.array(feature_vector)))\n",
    "    print(\"Features shape: \" + str(features.shape))\n",
    "    return features, y, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extract_CREMA(features, y, counter, npload=False):\n",
    "    LOCAL_PATH = \"../../CREMA/\"\n",
    "    WAV_PATH = LOCAL_PATH + \"AudioWAV/\"\n",
    "    demographics = pd.read_csv(LOCAL_PATH + \"VideoDemographics.csv\")\n",
    "    dataset = []\n",
    "    fs = 0\n",
    "    if not npload:\n",
    "        i = 0 # for progress bar\n",
    "        for file in os.listdir(WAV_PATH):\n",
    "            if file.endswith('.wav'):\n",
    "                emotion = file[9:12]\n",
    "                #Check if we're using this emotion\n",
    "                e = EMOTIONS.get(emotion, None)\n",
    "                if(e != None):\n",
    "                    progress(i, NUM_SAMPLES_CREMA, status=\"Reading files\")\n",
    "                     #Get actor ID from filename\n",
    "                    actor_id = int(file[0:4])\n",
    "                    #get the gender from demographics pd dataframe. 0 for Male, 1 for female\n",
    "                    gender = 0 if demographics[\"Sex\"][actor_id - 1001] == \"Male\" else 1\n",
    "                    emotion_index = e + gender*NUM_EMOTIONS\n",
    "                    y[counter][emotion_index] = 1\n",
    "                    i+=1\n",
    "                    counter+=1 \n",
    "                    #process sound\n",
    "                    data, fs = librosa.load(WAV_PATH + file, sr=None)\n",
    "                    data, fs = clean_sound(data, fs)\n",
    "                    dataset.append(data)\n",
    "        np.save(LOCAL_PATH + \"CREMAdatafsy\", (dataset, fs, y))\n",
    "    else:\n",
    "        dataset, fs, y = np.load(LOCAL_PATH + \"CREMAdatafsy.npy\", allow_pickle=True)\n",
    "    #get feature vector\n",
    "    print(\"Getting all features...\")\n",
    "    feature_vector = get_all_features(np.array(dataset), fs)\n",
    "    print(\"Feature Vector Shape: \" + str(feature_vector.shape))\n",
    "    print(\"Features shape: \" + str(len(features)))\n",
    "    if features.shape[0] == 0:\n",
    "        features = feature_vector\n",
    "    else:\n",
    "        features = np.concatenate((np.array(features), np.array(feature_vector)))\n",
    "    return features, y, counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extract_TESS(features, y, counter, npload=False):\n",
    "    LOCAL_PATH = \"../../TESS/\"\n",
    "    fs = 0\n",
    "    dataset = []\n",
    "    if not npload:\n",
    "        i = 0 # for progress bar\n",
    "        for file in os.listdir(LOCAL_PATH):\n",
    "            if file.endswith('.wav'):\n",
    "                emotion = file[file.index('_', 4) + 1 : file.index('.')]\n",
    "                #Check if we're using this emotion\n",
    "                e = EMOTIONS.get(emotion, None)\n",
    "                if(e != None):\n",
    "                    progress(i, NUM_SAMPLES_TESS, status=\"Reading files\")\n",
    "                    #TESS is all female so gender = 1\n",
    "                    gender = 1\n",
    "                    emotion_index = e + gender*NUM_EMOTIONS\n",
    "                    y[counter][emotion_index] = 1\n",
    "                    i+=1\n",
    "                    counter+=1\n",
    "                    \n",
    "                    #process sound\n",
    "                    data, fs = librosa.load(LOCAL_PATH + file, sr=None)\n",
    "                    data, fs = clean_sound(data, fs)\n",
    "                    dataset.append(data)\n",
    "        np.save(LOCAL_PATH + \"TESSdatafsy\", (dataset, fs, y))\n",
    "    else:\n",
    "        dataset, fs, y = np.load(LOCAL_PATH + \"TESSdatafsy.npy\", allow_pickle=True)\n",
    "    #get feature vector\n",
    "    print(\"Getting all features...\")\n",
    "    feature_vector = get_all_features(np.array(dataset), fs)\n",
    "    print(\"Feature Vector Shape: \" + str(feature_vector.shape))\n",
    "    print(\"Features shape: \" + str(len(features)))\n",
    "    if features.shape[0] == 0:\n",
    "        features = feature_vector\n",
    "    else:\n",
    "        features = np.concatenate((np.array(features), np.array(feature_vector)))\n",
    "    return features, y, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extract_all():\n",
    "    y_index = 0\n",
    "    y = np.zeros((NUM_SAMPLES, NUM_EMOTIONS*2))\n",
    "    print(y.shape)\n",
    "    features = np.array([])\n",
    "    counter = 0\n",
    "    print(\"----------------------------------------------\\n\")\n",
    "    print(\"Extracting LDC....\")\n",
    "    features, y, counter = data_extract_LDC(features, y, counter, npload=False)\n",
    "    print(\"Done extracting LDC\")\n",
    "    print(\"----------------------------------------------\\n\")\n",
    "    print(\"Extracting CREMA....\")\n",
    "    features, y, counter = data_extract_CREMA(features, y, counter, npload=False)\n",
    "    print(\"Done extracting CREMA\")\n",
    "    print(\"\\n----------------------------------------------\\n\")\n",
    "    print(\"Extracting TESS....\")\n",
    "    features, y, counter = data_extract_TESS(features, y, counter, npload=False)\n",
    "    print(\"Done extracting TESS\")\n",
    "    print(\"\\n----------------------------------------------\\n\")\n",
    "    print(\"Extracting RAVDESS....\")\n",
    "    features, y, counter = data_extract_RAVDESS(features, y, counter, npload=False)\n",
    "    print(\"Done extracting RAVDESS\")\n",
    "    print(\"\\n----------------------------------------------\")\n",
    "    mean_vector = np.mean(features, axis=0)\n",
    "    std_vector = np.std(features, axis=0)\n",
    "    \n",
    "    #normalize the data\n",
    "    features = (features - mean_vector) / std_vector\n",
    "    return features, y\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MFCC_algorithm(np_data, fs):\n",
    "    MFCC2 = []\n",
    "    #for progess bar\n",
    "    i = 0\n",
    "    prog = np_data.shape[0]\n",
    "    for one_sound in np_data:\n",
    "        progress(i, prog, \"Calculating MFCC's\")\n",
    "        one_sound = np.asarray(one_sound)\n",
    "        MFCC2.append(python_speech_features.base.mfcc(one_sound, samplerate=fs, \n",
    "                                     winlen=0.025, winstep=0.01, numcep=13, \n",
    "                                     nfilt=26, nfft=1200).T)\n",
    "        i+=1\n",
    "    MFCC3 = []\n",
    "    cached_variables = []\n",
    "    for one_point in MFCC2:\n",
    "        cache_grad = (np.gradient(one_point, axis = 1))\n",
    "        cached_variables.append(np.asarray([np.mean(one_point, axis = 1), np.median(one_point, axis = 1),\n",
    "                                 np.var(one_point, axis = 1), \n",
    "                           np.min(one_point, axis = 1), np.max(one_point, axis = 1), \n",
    "                                 np.mean(cache_grad, axis = 1), np.var(cache_grad, axis = 1)]).flatten()\n",
    "                               )\n",
    "    return np.array(cached_variables)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pitch_vector(data, fs):\n",
    "    data = np.float32(data)\n",
    "    pitch = pysptk.sptk.rapt(data, fs, hopsize = 50)\n",
    "    silenced = remove_silence_from(pitch, np.mean(pitch))\n",
    "    return silenced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectral_vector(data, fs):\n",
    "    data = np.float32(data)\n",
    "    cent = librosa.feature.spectral_centroid(y=data, sr=fs)\n",
    "    return cent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lpc_vector(data):\n",
    "    vec = lpc.lpc_ref(data, 12)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rms_vector(data):\n",
    "    temp_data = np.float32(data)\n",
    "    cent = librosa.feature.rms(y=temp_data)\n",
    "    return cent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zero_vector(data):\n",
    "    temp_data = np.float32(data)\n",
    "    cent = librosa.feature.zero_crossing_rate(y=temp_data)\n",
    "    return cent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sr_vector(data):\n",
    "    temp_data = np.float32(data)\n",
    "    cent = librosa.feature.spectral_rolloff(y=temp_data)\n",
    "    return cent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(pitch_vector):\n",
    "    mean = np.mean(pitch_vector)\n",
    "    median = np.median(pitch_vector)\n",
    "    low = np.min(pitch_vector)\n",
    "    high = np.max(pitch_vector)\n",
    "    variance = np.var(pitch_vector)\n",
    "    \n",
    "    #derivative\n",
    "    derivative = np.diff(pitch_vector)\n",
    "    d_mean = np.mean(derivative)\n",
    "    d_min = np.min(derivative)\n",
    "    d_max = np.max(derivative)\n",
    "    return [mean, median, low, high, variance, d_mean, d_min, d_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_features(np_array, fs):\n",
    "    x = MFCC_algorithm(np_array, fs)\n",
    "    print(\"MFCC DONE\", end=\" \")\n",
    "    print(\"dimensions are \"+str([len(x), len(x[0])]))\n",
    "    x1 = []\n",
    "    x2 = []\n",
    "    x3 = []\n",
    "    x4 = []\n",
    "    x5 = []\n",
    "    #for progress bar\n",
    "    size = np_array.shape[0]\n",
    "    i = 0\n",
    "    for data in np_array:\n",
    "        progress(i, size, status=\"Calculating stats\")\n",
    "        pitch_vector = get_pitch_vector(data, fs)\n",
    "        stats = get_stats(pitch_vector)\n",
    "        x1.append(stats)\n",
    "        \n",
    "        spectral_vector = get_spectral_vector(data, fs)\n",
    "        stats = get_stats(spectral_vector)\n",
    "        x2.append(stats)\n",
    "        \n",
    "        rms_vector = get_rms_vector(data)\n",
    "        stats = get_stats(rms_vector)\n",
    "        x3.append(stats)\n",
    "        \n",
    "        sr_vector = get_sr_vector(data)\n",
    "        stats = get_stats(sr_vector)\n",
    "        x4.append(stats)\n",
    "    \n",
    "        zero_vector = get_zero_vector(data)\n",
    "        stats = get_stats(zero_vector)\n",
    "        x5.append(stats)\n",
    "        \n",
    "        i+=1\n",
    "    print(\"MFCC dimensions:\" + str([len(x), len(x[0])]))\n",
    "    print(\"Pitch dimensions:\" + str([len(x1), len(x1[0])]))\n",
    "    print(\"Spectral dimensions:\" + str([len(x2), len(x2[0])]))\n",
    "    print(\"RMS dimensions:\" + str([len(x3), len(x3[0])]))\n",
    "    print(\"SR dimensions:\" + str([len(x4), len(x4[0])]))\n",
    "    print(\"Zero dimensions:\" + str([len(x5), len(x5[0])]))\n",
    "    x = np.concatenate((x,x1,x2,x3,x4,x5), axis=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ready Dataset and output\n",
    "Put all of the extracted features into X and the classifications into y and split into training and testing group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_y_split(x, y):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5)\n",
    "    \n",
    "    num_labels = y_train.shape[1]\n",
    "    num_features = X_train.shape[1]\n",
    "    print(\"x train shape: \" +str(X_train.shape))\n",
    "    print(\"y train shape: \" +str(y_train.shape))\n",
    "    print(\"x test shape: \" +str(X_test.shape))\n",
    "    print(\"y test shape: \" +str(y_test.shape))\n",
    "    print(\"x validation shape: \" +str(X_val.shape))\n",
    "    print(\"y validation shape: \" +str(y_val.shape))\n",
    "    for i in range(num_labels):\n",
    "        print(\"y_train for emotion \"+str(i)+\": \"+ str(np.sum(y_train[:,i])))\n",
    "    for i in range(num_labels): \n",
    "        print(\"y_test for emotion \"+str(i)+\": \"+ str(np.sum(y_test[:,i])))\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12416, 12)\n",
      "----------------------------------------------\n",
      "\n",
      "Extracting LDC....\n",
      "Getting all features...======================================] 99.9% ...Reading files\n",
      "MFCC DONE dimensions are [1379, 91]==========================] 99.9% ...Calculating MFCC's\n",
      "MFCC dimensions:[1379, 91]===================================] 99.9% ...Calculating stats\n",
      "Pitch dimensions:[1379, 8]\n",
      "Spectral dimensions:[1379, 8]\n",
      "RMS dimensions:[1379, 8]\n",
      "SR dimensions:[1379, 8]\n",
      "Zero dimensions:[1379, 8]\n",
      "Feature Vector Shape: (1379, 131)\n",
      "Features shape: (1379, 131)\n",
      "Done extracting LDC\n",
      "----------------------------------------------\n",
      "\n",
      "Extracting CREMA....\n",
      "Getting all features...======================================] 100.0% ...Reading files\n",
      "MFCC DONE dimensions are [7441, 91]==========================] 100.0% ...Calculating MFCC's\n",
      "MFCC dimensions:[7441, 91]===================================] 100.0% ...Calculating stats\n",
      "Pitch dimensions:[7441, 8]\n",
      "Spectral dimensions:[7441, 8]\n",
      "RMS dimensions:[7441, 8]\n",
      "SR dimensions:[7441, 8]\n",
      "Zero dimensions:[7441, 8]\n",
      "Feature Vector Shape: (7441, 131)\n",
      "Features shape: 1379\n",
      "Done extracting CREMA\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "Extracting TESS....\n",
      "Getting all features...======================================] 100.0% ...Reading files\n",
      "MFCC DONE dimensions are [2400, 91]==========================] 100.0% ...Calculating MFCC's\n",
      "MFCC dimensions:[2400, 91]===================================] 100.0% ...Calculating stats\n",
      "Pitch dimensions:[2400, 8]\n",
      "Spectral dimensions:[2400, 8]\n",
      "RMS dimensions:[2400, 8]\n",
      "SR dimensions:[2400, 8]\n",
      "Zero dimensions:[2400, 8]\n",
      "Feature Vector Shape: (2400, 131)\n",
      "Features shape: 8820\n",
      "Done extracting TESS\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "Extracting RAVDESS....\n",
      "Getting all features...======================================] 99.9% ...Reading files\n",
      "MFCC DONE dimensions are [1196, 91]==========================] 99.9% ...Calculating MFCC's\n",
      "MFCC dimensions:[1196, 91]===================================] 99.9% ...Calculating stats\n",
      "Pitch dimensions:[1196, 8]\n",
      "Spectral dimensions:[1196, 8]\n",
      "RMS dimensions:[1196, 8]\n",
      "SR dimensions:[1196, 8]\n",
      "Zero dimensions:[1196, 8]\n",
      "Feature Vector Shape: (1196, 131)\n",
      "Features shape: (12416, 131)\n",
      "Done extracting RAVDESS\n",
      "\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#features, y = data_extract_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12416, 131), (12416, 12))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#features.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train shape: (9932, 131)\n",
      "y train shape: (9932, 12)\n",
      "x test shape: (1242, 131)\n",
      "y test shape: (1242, 12)\n",
      "x validation shape: (1242, 131)\n",
      "y validation shape: (1242, 12)\n",
      "y_train for emotion 0: 711.0\n",
      "y_train for emotion 1: 667.0\n",
      "y_train for emotion 2: 665.0\n",
      "y_train for emotion 3: 718.0\n",
      "y_train for emotion 4: 615.0\n",
      "y_train for emotion 5: 723.0\n",
      "y_train for emotion 6: 1005.0\n",
      "y_train for emotion 7: 943.0\n",
      "y_train for emotion 8: 939.0\n",
      "y_train for emotion 9: 1023.0\n",
      "y_train for emotion 10: 863.0\n",
      "y_train for emotion 11: 1060.0\n",
      "y_test for emotion 0: 94.0\n",
      "y_test for emotion 1: 77.0\n",
      "y_test for emotion 2: 81.0\n",
      "y_test for emotion 3: 97.0\n",
      "y_test for emotion 4: 69.0\n",
      "y_test for emotion 5: 89.0\n",
      "y_test for emotion 6: 124.0\n",
      "y_test for emotion 7: 136.0\n",
      "y_test for emotion 8: 118.0\n",
      "y_test for emotion 9: 121.0\n",
      "y_test for emotion 10: 104.0\n",
      "y_test for emotion 11: 132.0\n"
     ]
    }
   ],
   "source": [
    "#np.save(\"../../most_of_the_data\", (y,features, 0))\n",
    "\n",
    "#X_train, X_val, X_test, y_train, y_val, y_test = x_y_split(features, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(\"../../splitdata\", (X_train, X_val, X_test, y_train, y_val, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_val, X_test, y_train, y_val, y_test = np.load(\"../../splitdata.npy\", allow_pickle=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
