{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "import scipy\n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import osascript\n",
    "from gtts import gTTS \n",
    "import os \n",
    "import pyaudio\n",
    "import wave\n",
    "import keyboard  # using module keyboard\n",
    "import soundfile as sf\n",
    "import math\n",
    "import pyloudnorm as pyln\n",
    "from sys import byteorder\n",
    "from array import array\n",
    "from struct import pack\n",
    "import librosa\n",
    "from scipy.signal import butter, sosfiltfilt\n",
    "import python_speech_features\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "import  ipynb.fs.full.concat_project2 as emotex_lib\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "import keras as keras\n",
    "from  conch.analysis.formants import lpc\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras import regularizers\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = np.load(\"../../splitdata.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LeakyReLU\n",
    "def simple_NN(X_train, X_test, y_train, y_test, regularization = 0.1,drop_likely = 0.1,\n",
    "              learning_rate = 0.001, beta1 = 0.9, beta2 = 0.999,\n",
    "             num_iterations = 1000, size_batch=256,\n",
    "             activation = 'tanh'):\n",
    "    target_class = y_train.shape[1]\n",
    "    # Model \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(1024, input_dim=X_train.shape[1],  activation=activation,\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(2048, activation=activation,\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(Dropout(drop_likely))\n",
    "    \n",
    "    model.add(Dense(2048, activation=activation,\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Dropout(drop_likely))\n",
    "    \n",
    "    model.add(Dense(1024, activation=activation,\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "\n",
    "    model.add(Dense(512, activation=activation,\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "\n",
    "    model.add(Dense(target_class, activation='softmax'))\n",
    "    \n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_acc', mode='max',  patience=10, min_delta=0)\n",
    "    \n",
    "    opt = keras.optimizers.Adam(lr=learning_rate, beta_1=beta1, beta_2=beta2, epsilon=10e-8)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    nn_history = model.fit(X_train, y_train, batch_size=size_batch, epochs=num_iterations, \n",
    "                         validation_data=(X_test, y_test),callbacks=[es])\n",
    "    return model, nn_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8829 samples, validate on 1104 samples\n",
      "Epoch 1/40\n",
      "8829/8829 [==============================] - 58s 7ms/step - loss: 220.6880 - acc: 0.4056 - val_loss: 31.3624 - val_acc: 0.3216\n",
      "Epoch 2/40\n",
      "8829/8829 [==============================] - 38s 4ms/step - loss: 10.9706 - acc: 0.3991 - val_loss: 3.9433 - val_acc: 0.2319\n",
      "Epoch 3/40\n",
      "8829/8829 [==============================] - 39s 4ms/step - loss: 2.5527 - acc: 0.3673 - val_loss: 2.5133 - val_acc: 0.2591\n",
      "Epoch 4/40\n",
      "8829/8829 [==============================] - 35s 4ms/step - loss: 2.0770 - acc: 0.3756 - val_loss: 2.3553 - val_acc: 0.2962\n",
      "Epoch 5/40\n",
      "8829/8829 [==============================] - 34s 4ms/step - loss: 2.0453 - acc: 0.3783 - val_loss: 2.2460 - val_acc: 0.2953\n",
      "Epoch 6/40\n",
      "8829/8829 [==============================] - 28s 3ms/step - loss: 2.0342 - acc: 0.3846 - val_loss: 2.1353 - val_acc: 0.3433\n",
      "Epoch 7/40\n",
      "8829/8829 [==============================] - 38s 4ms/step - loss: 2.0155 - acc: 0.3971 - val_loss: 2.1216 - val_acc: 0.3587\n",
      "Epoch 8/40\n",
      "8829/8829 [==============================] - 37s 4ms/step - loss: 2.0683 - acc: 0.3858 - val_loss: 2.1465 - val_acc: 0.3188\n",
      "Epoch 9/40\n",
      "8829/8829 [==============================] - 39s 4ms/step - loss: 2.1642 - acc: 0.3580 - val_loss: 2.1038 - val_acc: 0.3433\n",
      "Epoch 10/40\n",
      "8829/8829 [==============================] - 39s 4ms/step - loss: 2.0457 - acc: 0.3835 - val_loss: 2.0428 - val_acc: 0.3687\n",
      "Epoch 11/40\n",
      "8829/8829 [==============================] - 43s 5ms/step - loss: 2.0272 - acc: 0.3926 - val_loss: 2.1403 - val_acc: 0.3560\n",
      "Epoch 12/40\n",
      "8829/8829 [==============================] - 46s 5ms/step - loss: 2.1489 - acc: 0.3638 - val_loss: 2.1044 - val_acc: 0.3804\n",
      "Epoch 13/40\n",
      "8829/8829 [==============================] - 44s 5ms/step - loss: 2.1610 - acc: 0.3621 - val_loss: 2.1752 - val_acc: 0.3433\n",
      "Epoch 14/40\n",
      "8829/8829 [==============================] - 40s 5ms/step - loss: 2.1159 - acc: 0.3863 - val_loss: 1.9929 - val_acc: 0.3859\n",
      "Epoch 15/40\n",
      "8829/8829 [==============================] - 39s 4ms/step - loss: 2.0187 - acc: 0.3931 - val_loss: 2.2031 - val_acc: 0.2880\n",
      "Epoch 16/40\n",
      "8829/8829 [==============================] - 38s 4ms/step - loss: 2.1088 - acc: 0.3746 - val_loss: 2.2215 - val_acc: 0.3134\n",
      "Epoch 17/40\n",
      "8829/8829 [==============================] - 38s 4ms/step - loss: 2.1124 - acc: 0.3823 - val_loss: 2.1320 - val_acc: 0.3931\n",
      "Epoch 18/40\n",
      "8829/8829 [==============================] - 37s 4ms/step - loss: 2.0283 - acc: 0.3982 - val_loss: 2.1296 - val_acc: 0.3623\n",
      "Epoch 19/40\n",
      "8829/8829 [==============================] - 38s 4ms/step - loss: 2.0472 - acc: 0.3851 - val_loss: 2.1607 - val_acc: 0.3668\n",
      "Epoch 20/40\n",
      "8829/8829 [==============================] - 38s 4ms/step - loss: 2.2045 - acc: 0.3554 - val_loss: 2.4444 - val_acc: 0.2908\n",
      "Epoch 21/40\n",
      "8829/8829 [==============================] - 38s 4ms/step - loss: 2.2212 - acc: 0.3607 - val_loss: 2.3389 - val_acc: 0.3813\n",
      "Epoch 22/40\n",
      "8829/8829 [==============================] - 38s 4ms/step - loss: 2.1828 - acc: 0.3773 - val_loss: 2.1338 - val_acc: 0.4139\n",
      "Epoch 23/40\n",
      "8829/8829 [==============================] - 38s 4ms/step - loss: 2.0997 - acc: 0.3874 - val_loss: 2.4385 - val_acc: 0.3161\n",
      "Epoch 24/40\n",
      "8829/8829 [==============================] - 37s 4ms/step - loss: 2.2120 - acc: 0.3797 - val_loss: 2.2020 - val_acc: 0.3451\n",
      "Epoch 25/40\n",
      "8829/8829 [==============================] - 37s 4ms/step - loss: 2.1767 - acc: 0.3750 - val_loss: 2.5632 - val_acc: 0.3053\n",
      "Epoch 26/40\n",
      "8829/8829 [==============================] - 41s 5ms/step - loss: 2.1769 - acc: 0.3871 - val_loss: 2.2179 - val_acc: 0.3560\n",
      "Epoch 27/40\n",
      "8829/8829 [==============================] - 40s 4ms/step - loss: 2.1843 - acc: 0.3712 - val_loss: 2.1798 - val_acc: 0.3333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras.engine.sequential.Sequential at 0x146aee780>,\n",
       " <keras.callbacks.History at 0x14e6da4e0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_NN(X_train, X_test, y_train, y_test, regularization = 0.1,drop_likely = 0.1,\n",
    "              learning_rate = 0.001, beta1 = 0.9, beta2 = 0.999,\n",
    "             num_iterations = 40, size_batch=256,\n",
    "             activation = 'tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_TESS, fs_TESS, y_TESS = emotex_lib.data_extract_tess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCC DONE\n",
      "dimensions are [2800, 91]\n",
      "pitch DONE\n",
      "dimensions are [2800, 8]\n",
      "spectral DONE\n",
      "dimensions are [2800, 8]\n",
      "rms DONE\n",
      "dimensions are [2800, 8]\n",
      "sr DONE\n",
      "dimensions are [2800, 8]\n",
      "zero DONE\n",
      "dimensions are [2800, 8]\n",
      "x train shape: (2240, 131)\n",
      "y train shape: (2240, 7)\n",
      "x test shape: (560, 131)\n",
      "y test shape: (560, 7)\n",
      "y_train for emotion 0: 312.0\n",
      "y_train for emotion 1: 323.0\n",
      "y_train for emotion 2: 320.0\n",
      "y_train for emotion 3: 312.0\n",
      "y_train for emotion 4: 334.0\n",
      "y_train for emotion 5: 316.0\n",
      "y_train for emotion 6: 323.0\n",
      "y_test for emotion 0: 88.0\n",
      "y_test for emotion 1: 77.0\n",
      "y_test for emotion 2: 80.0\n",
      "y_test for emotion 3: 88.0\n",
      "y_test for emotion 4: 66.0\n",
      "y_test for emotion 5: 84.0\n",
      "y_test for emotion 6: 77.0\n"
     ]
    }
   ],
   "source": [
    "X_train_TESS, X_test_TESS, y_train_TESS, y_test_TESS = emotex_lib.x_y_split(data_TESS, fs_TESS, y_TESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2240, 131), (2240, 7))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_TESS.shape , y_train_TESS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " def oneVSREST(X_train, X_test, y_train, y_test):\n",
    "    y_train = [np.where(r==1)[0][0] for r in y_train]\n",
    "    y_test = [np.where(r==1)[0][0] for r in y_test]\n",
    "    mlp = OneVsRestClassifier(estimator=sklearn.svm.SVC(gamma='scale',random_state=0, max_iter=100), n_jobs=100)\n",
    "    print(mlp)\n",
    "    mlp.fit(X_train,y_train)\n",
    "    predictions = mlp.predict(X_train)\n",
    "    print(classification_report(y_train,predictions))\n",
    "    predictions = mlp.predict(X_test)\n",
    "    print(classification_report(y_test,predictions))\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneVsRestClassifier(estimator=SVC(C=1.0, cache_size=200, class_weight=None,\n",
      "                                  coef0=0.0, decision_function_shape='ovr',\n",
      "                                  degree=3, gamma='scale', kernel='rbf',\n",
      "                                  max_iter=-1, probability=False,\n",
      "                                  random_state=0, shrinking=True, tol=0.001,\n",
      "                                  verbose=False),\n",
      "                    n_jobs=100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.29      0.27       312\n",
      "           1       0.25      0.17      0.21       323\n",
      "           2       0.17      0.37      0.24       320\n",
      "           3       0.24      0.21      0.23       312\n",
      "           4       0.36      0.53      0.43       334\n",
      "           5       0.67      0.01      0.01       316\n",
      "           6       0.22      0.13      0.16       323\n",
      "\n",
      "    accuracy                           0.25      2240\n",
      "   macro avg       0.31      0.24      0.22      2240\n",
      "weighted avg       0.31      0.25      0.22      2240\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.25      0.25        88\n",
      "           1       0.25      0.18      0.21        77\n",
      "           2       0.18      0.47      0.26        80\n",
      "           3       0.32      0.24      0.27        88\n",
      "           4       0.38      0.58      0.46        66\n",
      "           5       0.00      0.00      0.00        84\n",
      "           6       0.20      0.12      0.15        77\n",
      "\n",
      "    accuracy                           0.25       560\n",
      "   macro avg       0.23      0.26      0.23       560\n",
      "weighted avg       0.22      0.25      0.22       560\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "oVSr = oneVSREST(X_train_TESS, X_test_TESS, y_train_TESS, y_test_TESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLPalgorithm(X_train, X_test, y_train, y_test):\n",
    "    mlp = MLPClassifier(max_iter=10000)\n",
    "    mlp.fit(X_train,y_train)\n",
    "    predictions = mlp.predict(X_train)\n",
    "    print(classification_report(y_train,predictions))\n",
    "    predictions = mlp.predict(X_test)\n",
    "    print(classification_report(y_test,predictions))\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:568: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.97      0.90       312\n",
      "           1       0.98      0.61      0.75       323\n",
      "           2       0.99      0.82      0.90       320\n",
      "           3       0.94      0.63      0.75       312\n",
      "           4       1.00      0.99      0.99       334\n",
      "           5       0.57      0.93      0.71       316\n",
      "           6       0.96      0.96      0.96       323\n",
      "\n",
      "   micro avg       0.86      0.85      0.85      2240\n",
      "   macro avg       0.90      0.84      0.85      2240\n",
      "weighted avg       0.90      0.85      0.85      2240\n",
      " samples avg       0.80      0.85      0.81      2240\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.94      0.87        88\n",
      "           1       0.91      0.53      0.67        77\n",
      "           2       0.97      0.88      0.92        80\n",
      "           3       0.87      0.52      0.65        88\n",
      "           4       1.00      1.00      1.00        66\n",
      "           5       0.54      0.80      0.64        84\n",
      "           6       0.99      0.94      0.96        77\n",
      "\n",
      "   micro avg       0.83      0.79      0.81       560\n",
      "   macro avg       0.87      0.80      0.82       560\n",
      "weighted avg       0.86      0.79      0.81       560\n",
      " samples avg       0.75      0.79      0.76       560\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "mlp3 = MLPalgorithm(X_train_TESS, X_test_TESS, y_train_TESS, y_test_TESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mlp_for_demo.joblib']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(mlp3, 'mlp_for_demo.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = load('mlp_for_demo.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.94      0.87        88\n",
      "           1       0.91      0.53      0.67        77\n",
      "           2       0.97      0.88      0.92        80\n",
      "           3       0.87      0.52      0.65        88\n",
      "           4       1.00      1.00      1.00        66\n",
      "           5       0.54      0.80      0.64        84\n",
      "           6       0.99      0.94      0.96        77\n",
      "\n",
      "   micro avg       0.83      0.79      0.81       560\n",
      "   macro avg       0.87      0.80      0.82       560\n",
      "weighted avg       0.86      0.79      0.81       560\n",
      " samples avg       0.75      0.79      0.76       560\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6964285714285714"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = clf.predict(X_test_TESS)\n",
    "print(classification_report(y_test_TESS,predictions))\n",
    "sklearn.metrics.accuracy_score(y_test_TESS, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.97      0.90       312\n",
      "           1       0.98      0.61      0.75       323\n",
      "           2       0.99      0.82      0.90       320\n",
      "           3       0.94      0.63      0.75       312\n",
      "           4       1.00      0.99      0.99       334\n",
      "           5       0.57      0.93      0.71       316\n",
      "           6       0.96      0.96      0.96       323\n",
      "\n",
      "   micro avg       0.86      0.85      0.85      2240\n",
      "   macro avg       0.90      0.84      0.85      2240\n",
      "weighted avg       0.90      0.85      0.85      2240\n",
      " samples avg       0.80      0.85      0.81      2240\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7482142857142857"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = mlp3.predict(X_test_TESS)\n",
    "print(classification_report(y_test_TESS,predictions))\n",
    "sklearn.metrics.accuracy_score(y_test_TESS, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7272321428571429"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = mlp2.predict(X_train_TESS)\n",
    "print(classification_report(y_train,predictions))\n",
    "sklearn.metrics.accuracy_score(y_train_TESS, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6732142857142858"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = mlp2.predict(X_train_TESS)\n",
    "sklearn.metrics.accuracy_score(y_train_TESS, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6526785714285714"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = mlp.predict(X_train_TESS)\n",
    "sklearn.metrics.accuracy_score(y_train_TESS, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVMalgorithm(X_train, X_test, y_train, y_test):\n",
    "    y_train = [np.where(r==1)[0][0] for r in y_train]\n",
    "    y_test = [np.where(r==1)[0][0] for r in y_test]\n",
    "    \n",
    "    svm = sklearn.svm.SVC(gamma='auto')\n",
    "    svm.fit(X_train,y_train)\n",
    "    predictions = svm.predict(X_train)\n",
    "    print(classification_report(y_train,predictions))\n",
    "    predictions = svm.predict(X_test)\n",
    "    print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       322\n",
      "           1       1.00      1.00      1.00       307\n",
      "           2       1.00      1.00      1.00       316\n",
      "           3       1.00      1.00      1.00       325\n",
      "           4       1.00      1.00      1.00       335\n",
      "           5       1.00      1.00      1.00       315\n",
      "           6       1.00      1.00      1.00       320\n",
      "\n",
      "    accuracy                           1.00      2240\n",
      "   macro avg       1.00      1.00      1.00      2240\n",
      "weighted avg       1.00      1.00      1.00      2240\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        78\n",
      "           1       0.00      0.00      0.00        93\n",
      "           2       0.00      0.00      0.00        84\n",
      "           3       0.00      0.00      0.00        75\n",
      "           4       0.12      1.00      0.21        65\n",
      "           5       0.00      0.00      0.00        85\n",
      "           6       0.00      0.00      0.00        80\n",
      "\n",
      "    accuracy                           0.12       560\n",
      "   macro avg       0.02      0.14      0.03       560\n",
      "weighted avg       0.01      0.12      0.02       560\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "SVMalgorithm(X_train_TESS, X_test_TESS, y_train_TESS, y_test_TESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decsionTree(X_train, X_test, y_train, y_test):\n",
    "    svm = sklearn.tree.DecisionTreeClassifier()\n",
    "    svm.fit(X_train,y_train)\n",
    "    predictions = mlp.predict(X_train)\n",
    "    print(classification_report(y_train,predictions))\n",
    "    predictions = mlp.predict(X_test)\n",
    "    print(classification_report(y_test,predictions))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.05      0.09       322\n",
      "           1       0.65      0.62      0.63       307\n",
      "           2       0.00      0.00      0.00       316\n",
      "           3       0.18      1.00      0.30       325\n",
      "           4       0.00      0.00      0.00       335\n",
      "           5       0.75      0.01      0.02       315\n",
      "           6       0.69      0.60      0.64       320\n",
      "\n",
      "   micro avg       0.30      0.32      0.31      2240\n",
      "   macro avg       0.46      0.32      0.24      2240\n",
      "weighted avg       0.45      0.32      0.24      2240\n",
      " samples avg       0.23      0.32      0.26      2240\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.08      0.14        78\n",
      "           1       0.69      0.63      0.66        93\n",
      "           2       0.00      0.00      0.00        84\n",
      "           3       0.17      1.00      0.29        75\n",
      "           4       0.00      0.00      0.00        65\n",
      "           5       1.00      0.02      0.05        85\n",
      "           6       0.62      0.56      0.59        80\n",
      "\n",
      "   micro avg       0.31      0.33      0.32       560\n",
      "   macro avg       0.50      0.33      0.25       560\n",
      "weighted avg       0.52      0.33      0.26       560\n",
      " samples avg       0.23      0.33      0.26       560\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "decsionTree(X_train_TESS, X_test_TESS, y_train_TESS, y_test_TESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2240 samples, validate on 560 samples\n",
      "Epoch 1/1000\n",
      "2240/2240 [==============================] - 5s 2ms/step - loss: 454.0959 - acc: 0.1821 - val_loss: 314.8684 - val_acc: 0.2161\n",
      "Epoch 2/1000\n",
      "2240/2240 [==============================] - 2s 852us/step - loss: 236.7418 - acc: 0.2027 - val_loss: 152.1415 - val_acc: 0.1589\n",
      "Epoch 3/1000\n",
      "2240/2240 [==============================] - 2s 877us/step - loss: 110.3280 - acc: 0.2317 - val_loss: 67.4487 - val_acc: 0.1607\n",
      "Epoch 4/1000\n",
      "2240/2240 [==============================] - 2s 941us/step - loss: 48.1162 - acc: 0.2268 - val_loss: 29.0699 - val_acc: 0.1982\n",
      "Epoch 5/1000\n",
      "2240/2240 [==============================] - 2s 858us/step - loss: 21.0456 - acc: 0.2335 - val_loss: 13.4310 - val_acc: 0.1982\n",
      "Epoch 6/1000\n",
      "2240/2240 [==============================] - 2s 1ms/step - loss: 10.1983 - acc: 0.2460 - val_loss: 7.1319 - val_acc: 0.2232\n",
      "Epoch 7/1000\n",
      "2240/2240 [==============================] - 2s 1ms/step - loss: 5.7885 - acc: 0.2357 - val_loss: 4.6439 - val_acc: 0.2018\n",
      "Epoch 8/1000\n",
      "2240/2240 [==============================] - 3s 1ms/step - loss: 3.8093 - acc: 0.2504 - val_loss: 3.1907 - val_acc: 0.2000\n",
      "Epoch 9/1000\n",
      "2240/2240 [==============================] - 3s 1ms/step - loss: 2.8605 - acc: 0.2103 - val_loss: 2.5430 - val_acc: 0.2000\n",
      "Epoch 10/1000\n",
      "2240/2240 [==============================] - 3s 1ms/step - loss: 2.3808 - acc: 0.2152 - val_loss: 2.3240 - val_acc: 0.1571\n",
      "Epoch 11/1000\n",
      "2240/2240 [==============================] - 3s 1ms/step - loss: 2.1666 - acc: 0.2299 - val_loss: 2.2540 - val_acc: 0.1482\n",
      "Epoch 12/1000\n",
      "2240/2240 [==============================] - 3s 1ms/step - loss: 2.0615 - acc: 0.2277 - val_loss: 2.4219 - val_acc: 0.1482\n",
      "Epoch 13/1000\n",
      "2240/2240 [==============================] - 2s 1ms/step - loss: 2.0218 - acc: 0.2268 - val_loss: 3.4436 - val_acc: 0.1482\n",
      "Epoch 14/1000\n",
      "2240/2240 [==============================] - 2s 1ms/step - loss: 2.0066 - acc: 0.2277 - val_loss: 2.7888 - val_acc: 0.1482\n",
      "Epoch 15/1000\n",
      "2240/2240 [==============================] - 2s 1ms/step - loss: 2.0134 - acc: 0.1951 - val_loss: 2.3807 - val_acc: 0.1482\n",
      "Epoch 16/1000\n",
      "2240/2240 [==============================] - ETA: 0s - loss: 2.0060 - acc: 0.208 - 3s 1ms/step - loss: 2.0078 - acc: 0.2058 - val_loss: 3.0894 - val_acc: 0.1482\n"
     ]
    }
   ],
   "source": [
    "model = simple_NN(X_train_TESS, X_test_TESS, y_train_TESS, y_test_TESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREMA DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_CREMA, fs_CREMA, y_CREMA = emotex_lib.data_extract_CREMA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_CREMA, X_test_CREMA, y_train_CREMA, y_test_CREMA = emotex_lib.x_y_split(data_CREMA, fs_CREMA, y_CREMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = simple_NN(X_train_CREMA, X_test_CREMA, y_train_CREMA, y_test_CREMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, fs, x_size = emotex_lib.data_extraction_RAVDESS('../../RAVDESS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = emotex_lib.emotion_extraction_RAVDESS('../../RAVDESS',1380)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = []\n",
    "for i in range(len(X_train)):\n",
    "    X_train_final.append([[x]for x in  X_train[i]])\n",
    "X_train = np.asarray(X_train_final)\n",
    "X_test_final = []\n",
    "for i in range(len(X_test)):\n",
    "    X_test_final.append([[x]for x in  X_test[i]])\n",
    "X_test = np.asarray(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = emotex_lib.x_y_split(X, fs, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =np.load(\"RAVDESS_X_Y_train_test.npy\",  allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LeakyReLU\n",
    "def simple_NN(X_train, X_test, y_train, y_test, regularization = 0.1,drop_likely = 0.1,\n",
    "              learning_rate = 0.001, beta1 = 0.9, beta2 = 0.999,\n",
    "             num_iterations = 40, size_batch=X_train.shape[0],\n",
    "             activation = 'tanh'):\n",
    "    target_class = y_train.shape[1]\n",
    "    # Model \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(1024, input_dim=X_train.shape[1], activation='relu',\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "    \n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(2048, activation=activation,\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(Dropout(drop_likely))\n",
    "    \n",
    "    model.add(Dense(2048, activation=activation,\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Dropout(drop_likely))\n",
    "    \n",
    "    model.add(Dense(1024, activation=activation,\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "\n",
    "    model.add(Dense(512, activation=activation,\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "\n",
    "    model.add(Dense(target_class, activation='softmax'))\n",
    "    opt = keras.optimizers.Adam(lr=learning_rate, beta_1=beta1, beta_2=beta2, epsilon=10e-8)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    nn_history = model.fit(X_train, y_train, batch_size=size_batch, epochs=num_iterations, \n",
    "                         validation_data=(X_test, y_test))\n",
    "    return model, nn_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1104 samples, validate on 276 samples\n",
      "Epoch 1/40\n",
      "1104/1104 [==============================] - 7s 7ms/step - loss: 572.9418 - acc: 0.0607 - val_loss: 538.4969 - val_acc: 0.1522\n",
      "Epoch 2/40\n",
      "1104/1104 [==============================] - 2s 2ms/step - loss: 538.3372 - acc: 0.1531 - val_loss: 504.3840 - val_acc: 0.1159\n",
      "Epoch 3/40\n",
      "1104/1104 [==============================] - 2s 1ms/step - loss: 504.2911 - acc: 0.1087 - val_loss: 471.6053 - val_acc: 0.1304\n",
      "Epoch 4/40\n",
      "1104/1104 [==============================] - 2s 2ms/step - loss: 471.5056 - acc: 0.1377 - val_loss: 440.4761 - val_acc: 0.1341\n",
      "Epoch 5/40\n",
      "1104/1104 [==============================] - 2s 2ms/step - loss: 440.3344 - acc: 0.1223 - val_loss: 410.6561 - val_acc: 0.1341\n",
      "Epoch 6/40\n",
      "1104/1104 [==============================] - 2s 2ms/step - loss: 410.5734 - acc: 0.1350 - val_loss: 382.0634 - val_acc: 0.1558\n",
      "Epoch 7/40\n",
      "1104/1104 [==============================] - 2s 1ms/step - loss: 382.0470 - acc: 0.1531 - val_loss: 355.0507 - val_acc: 0.1594\n",
      "Epoch 8/40\n",
      "1104/1104 [==============================] - 2s 2ms/step - loss: 354.8978 - acc: 0.1630 - val_loss: 329.1614 - val_acc: 0.1667\n",
      "Epoch 9/40\n",
      "1104/1104 [==============================] - 2s 2ms/step - loss: 329.1182 - acc: 0.1703 - val_loss: 304.8999 - val_acc: 0.1196\n",
      "Epoch 10/40\n",
      "1104/1104 [==============================] - 2s 2ms/step - loss: 304.6956 - acc: 0.1884 - val_loss: 281.7748 - val_acc: 0.1703\n",
      "Epoch 11/40\n",
      "1104/1104 [==============================] - 2s 1ms/step - loss: 281.6227 - acc: 0.1975 - val_loss: 260.0403 - val_acc: 0.1449\n",
      "Epoch 12/40\n",
      "1104/1104 [==============================] - 2s 2ms/step - loss: 259.8715 - acc: 0.2065 - val_loss: 239.6497 - val_acc: 0.1449\n",
      "Epoch 13/40\n",
      "1104/1104 [==============================] - 2s 1ms/step - loss: 239.4802 - acc: 0.2092 - val_loss: 220.5130 - val_acc: 0.1377\n",
      "Epoch 14/40\n",
      "1104/1104 [==============================] - 1s 1ms/step - loss: 220.3868 - acc: 0.2065 - val_loss: 202.6540 - val_acc: 0.1304\n",
      "Epoch 15/40\n",
      "1104/1104 [==============================] - 2s 2ms/step - loss: 202.5123 - acc: 0.2056 - val_loss: 186.0092 - val_acc: 0.1630\n",
      "Epoch 16/40\n",
      "1104/1104 [==============================] - 2s 2ms/step - loss: 185.8418 - acc: 0.2192 - val_loss: 170.6437 - val_acc: 0.1159\n",
      "Epoch 17/40\n",
      "1104/1104 [==============================] - 2s 2ms/step - loss: 170.3753 - acc: 0.2038 - val_loss: 156.2003 - val_acc: 0.1304\n",
      "Epoch 18/40\n",
      "1104/1104 [==============================] - 2s 1ms/step - loss: 155.9769 - acc: 0.2183 - val_loss: 142.8707 - val_acc: 0.1123\n",
      "Epoch 19/40\n",
      "1104/1104 [==============================] - 2s 1ms/step - loss: 142.6595 - acc: 0.2029 - val_loss: 130.5570 - val_acc: 0.1413\n",
      "Epoch 20/40\n",
      "1104/1104 [==============================] - 2s 1ms/step - loss: 130.3322 - acc: 0.2192 - val_loss: 119.1807 - val_acc: 0.1377\n",
      "Epoch 21/40\n",
      "1104/1104 [==============================] - 2s 1ms/step - loss: 118.9517 - acc: 0.2129 - val_loss: 108.7053 - val_acc: 0.1486\n",
      "Epoch 22/40\n",
      "1104/1104 [==============================] - 2s 1ms/step - loss: 108.4946 - acc: 0.2201 - val_loss: 99.1267 - val_acc: 0.1377\n",
      "Epoch 23/40\n",
      "1104/1104 [==============================] - 2s 1ms/step - loss: 98.8668 - acc: 0.2011 - val_loss: 90.2517 - val_acc: 0.1522\n",
      "Epoch 24/40\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9f578b6e997a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RAVDESS_X_Y_train_test.npy\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimple_NN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-b357665e6c45>\u001b[0m in \u001b[0;36msimple_NN\u001b[0;34m(X_train, X_test, y_train, y_test, regularization, drop_likely, learning_rate, beta1, beta2, num_iterations, size_batch, activation)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     nn_history = model.fit(X_train, y_train, batch_size=size_batch, epochs=num_iterations, \n\u001b[0;32m---> 34\u001b[0;31m                          validation_data=(X_test, y_test))\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0719 17:43:20.047148 140735512335232 alias.py:221] Invalid alias: The name clear can't be aliased because it is another magic command.\n",
      "E0719 17:43:20.047904 140735512335232 alias.py:221] Invalid alias: The name more can't be aliased because it is another magic command.\n",
      "E0719 17:43:20.048449 140735512335232 alias.py:221] Invalid alias: The name less can't be aliased because it is another magic command.\n",
      "E0719 17:43:20.049008 140735512335232 alias.py:221] Invalid alias: The name man can't be aliased because it is another magic command.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test =np.load(\"RAVDESS_X_Y_train_test.npy\",  allow_pickle=True)\n",
    "model, nn_history = simple_NN(X_train, X_test, y_train, y_test, num_iterations=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =np.load(\"RAVDESS_X_Y_train_test.npy\",  allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_features , all_y = np.load('../../../most_of_the_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_NN(X_train, X_test, y_train, y_test, regularization = 0.1,drop_likely = 0.1,\n",
    "              learning_rate = 0.001, beta1 = 0.9, beta2 = 0.999\n",
    "             num_iterations = 40, size_batch=X_train.shape[0],\n",
    "             activation = 'tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_iterations(best_reg):\n",
    "    iterations = np.arange(2, 200, 5)\n",
    "    validation_history = []\n",
    "    train_history = []\n",
    "    for one_reg in regularization:\n",
    "        model, nn_history = simple_NN(X_train, X_test, y_train, y_test, num_iterations=iterations,\n",
    "                                      regularization=best_reg)\n",
    "        validation_history.append(nn_history.history['val_acc'])\n",
    "        train_history.append(nn_history.history['acc'])\n",
    "    plt.plot(regularization, validation_history, label='validation')\n",
    "    plt.plot(regularization, train_history, label='training')\n",
    "    plt.show()\n",
    "    return iterations[np.argmex(validation_history)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "def graph_regularization(num_iterations, ranges):\n",
    "    regularization = [10**c for c in -4.0 * np.arange(0.,1.0,ranges,dtype=float)]\n",
    "    validation_history = []\n",
    "    best_iteration = []\n",
    "    train_history = []\n",
    "    for one_reg in regularization:\n",
    "        \n",
    "        model, nn_history = simple_NN(X_train, X_test, y_train, y_test, num_iterations=num_iterations,\n",
    "                                      regularization=one_reg)\n",
    "        \n",
    "        validation_history.append(np.amax(nn_history.history['val_acc']))\n",
    "        train_history.append(np.amax(nn_history.history['acc']))\n",
    "        best_iteration.append(np.argmax(nn_history.history['val_acc']))\n",
    "                                  \n",
    "    print(train_history, validation_history, regularization)\n",
    "    plt.plot(regularization, validation_history, label='validation')\n",
    "    plt.plot(regularization, train_history, label='training')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    index = np.argmax(validation_history)\n",
    "    return regularization[index], best_iteration[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_regularization(65, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sherpa\n",
    "import sherpa.algorithms.bayesian_optimization as bayesian_optimization\n",
    "import time\n",
    "\n",
    "parameters = [sherpa.Continuous(name='lr', range=[0.0001, 0.1], scale='log'),\n",
    "              sherpa.Continuous(name='beta1', range=[0.8, 1.0], scale='log'),\n",
    "              sherpa.Continuous(name=\"regularization\", range=[0.0001, 3], scale='log'),\n",
    "              sherpa.Continuous(name='dropout', range=[0.0, 0.5]),\n",
    "              sherpa.Ordinal(name='batch_size', range=[16, 32, 64,128,256,512]),\n",
    "              sherpa.Choice(name='activation', range=['relu', 'elu', 'prelu', 'tanh'])]\n",
    "\n",
    "alg =bayesian_optimization.GPyOpt()\n",
    "\n",
    "study = sherpa.Study(parameters=parameters,\n",
    "                     algorithm=alg,dashboard_port=9999,disable_dashboard=False,\n",
    "                     lower_is_better=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for trial in study:\n",
    "    simple_NN(study, trial, X_train, X_test, y_train, y_test, regularization =trial.parameters['regularization'],\n",
    "              drop_likely = trial.parameters['dropout'],\n",
    "              learning_rate = trial.parameters['lr'],\n",
    "              beta1 = trial.parameters['beta1'], beta2 = 0.999,\n",
    "             num_iterations = 150, size_batch=trial.parameters[\"batch_size\"])\n",
    "    study.finalize(trial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "\n",
    "space  = [Integer(1, 5, name='max_depth'),\n",
    "          Real(10**-5, 10**0, \"log-uniform\", name='learning_rate'),\n",
    "          Integer(1, n_features, name='max_features'),\n",
    "          Integer(2, 100, name='min_samples_split'),\n",
    "          Integer(1, 100, name='min_samples_leaf')]\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    reg.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(reg, X, y, cv=5, n_jobs=-1,\n",
    "                                    scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "\n",
    "study = sherpa.Study(parameters=parameters,\n",
    "                     algorithm=alg,\n",
    "                     lower_is_better=True)\n",
    "\n",
    "for trial in study:\n",
    "    simple_NN(study, trial, X_train, X_test, y_train, y_test, regularization =trial.parameters['regularization'],\n",
    "              drop_likely = trial.parameters['dropout'],\n",
    "              learning_rate = trial.parameters['lr'],\n",
    "              beta1 = trial.parameters['beta1'], beta2 = 0.999,\n",
    "             num_iterations = 5, size_batch=trial.parameters[\"batch_size\"])\n",
    "    study.finalize(trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LeakyReLU\n",
    "def simple_NN(X_train, X_test, y_train, y_test, regularization = 0.0,drop_likely = 0.0,\n",
    "              learning_rate = 0.01, beta1 = 0.9, beta2 = 0.999,\n",
    "             num_iterations = 40, size_batch=X_train.shape[0],\n",
    "             activation = 'tanh'):\n",
    "    target_class = y_train.shape[1]\n",
    "    # Model \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(1024, input_dim=X_train.shape[1], activation=activation,\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(2048, activation=activation,\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(Dropout(drop_likely))\n",
    "    \n",
    "    model.add(Dense(4096, activation=activation,\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(Dropout(drop_likely))\n",
    "    \n",
    "    model.add(Dense(2048, activation=activation,\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Dropout(drop_likely))\n",
    "    \n",
    "    model.add(Dense(1024, activation=activation,\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "\n",
    "    model.add(Dense(512, activation=activation,\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(256, activation=activation,\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "    \n",
    "    model.add(Dense(128, activation=activation,\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "    \n",
    "    model.add(Dense(64, activation=activation,\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "    \n",
    "    model.add(Dense(32, activation=activation,\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "        \n",
    "    model.add(Dense(target_class, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    nn_history = model.fit(X_train, y_train, batch_size=size_batch, epochs=num_iterations, \n",
    "                         validation_data=(X_test, y_test))\n",
    "    \n",
    "    \n",
    "    return model, nn_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1104 samples, validate on 276 samples\n",
      "Epoch 1/40\n",
      "1104/1104 [==============================] - 10s 9ms/step - loss: 2.9012 - acc: 0.0788 - val_loss: 2.9279 - val_acc: 0.0978\n",
      "Epoch 2/40\n",
      "1104/1104 [==============================] - 3s 3ms/step - loss: 2.9113 - acc: 0.1168 - val_loss: 2.7629 - val_acc: 0.1087\n",
      "Epoch 3/40\n",
      "1104/1104 [==============================] - 3s 3ms/step - loss: 2.7208 - acc: 0.1422 - val_loss: 2.6815 - val_acc: 0.1196\n",
      "Epoch 4/40\n",
      "1104/1104 [==============================] - 3s 3ms/step - loss: 2.6619 - acc: 0.1322 - val_loss: 2.6755 - val_acc: 0.1449\n",
      "Epoch 5/40\n",
      "1104/1104 [==============================] - 3s 3ms/step - loss: 2.6190 - acc: 0.1413 - val_loss: 2.6416 - val_acc: 0.1304\n",
      "Epoch 6/40\n",
      "1104/1104 [==============================] - 4s 3ms/step - loss: 2.5629 - acc: 0.1730 - val_loss: 2.6198 - val_acc: 0.1196\n",
      "Epoch 7/40\n",
      "1104/1104 [==============================] - 3s 3ms/step - loss: 2.5321 - acc: 0.1766 - val_loss: 2.6029 - val_acc: 0.1232\n",
      "Epoch 8/40\n",
      "1104/1104 [==============================] - 3s 3ms/step - loss: 2.4967 - acc: 0.1821 - val_loss: 2.5950 - val_acc: 0.1304\n",
      "Epoch 9/40\n",
      "1104/1104 [==============================] - 3s 3ms/step - loss: 2.4676 - acc: 0.1966 - val_loss: 2.5999 - val_acc: 0.1268\n",
      "Epoch 10/40\n",
      "1104/1104 [==============================] - 3s 3ms/step - loss: 2.4394 - acc: 0.2083 - val_loss: 2.6054 - val_acc: 0.1051\n",
      "Epoch 11/40\n",
      "1104/1104 [==============================] - 3s 3ms/step - loss: 2.4181 - acc: 0.2074 - val_loss: 2.6473 - val_acc: 0.1087\n",
      "Epoch 12/40\n",
      "1104/1104 [==============================] - 3s 3ms/step - loss: 2.4268 - acc: 0.2156 - val_loss: 2.6202 - val_acc: 0.1232\n",
      "Epoch 13/40\n",
      "1104/1104 [==============================] - 3s 3ms/step - loss: 2.4019 - acc: 0.2156 - val_loss: 2.6162 - val_acc: 0.1159\n",
      "Epoch 14/40\n",
      "1104/1104 [==============================] - 3s 3ms/step - loss: 2.3767 - acc: 0.2255 - val_loss: 2.6549 - val_acc: 0.1051\n",
      "Epoch 15/40\n",
      "1104/1104 [==============================] - 4s 4ms/step - loss: 2.3818 - acc: 0.2391 - val_loss: 2.6677 - val_acc: 0.1123\n",
      "Epoch 16/40\n",
      "1104/1104 [==============================] - 4s 4ms/step - loss: 2.3449 - acc: 0.2337 - val_loss: 2.6506 - val_acc: 0.1051\n",
      "Epoch 17/40\n",
      "1104/1104 [==============================] - 4s 4ms/step - loss: 2.3212 - acc: 0.2518 - val_loss: 2.6459 - val_acc: 0.1232\n",
      "Epoch 18/40\n",
      "1104/1104 [==============================] - 4s 4ms/step - loss: 2.2939 - acc: 0.2554 - val_loss: 2.6657 - val_acc: 0.1123\n",
      "Epoch 19/40\n",
      "1104/1104 [==============================] - 5s 4ms/step - loss: 2.2807 - acc: 0.2627 - val_loss: 2.6405 - val_acc: 0.1232\n",
      "Epoch 20/40\n",
      "1104/1104 [==============================] - 4s 4ms/step - loss: 2.2372 - acc: 0.2745 - val_loss: 2.6296 - val_acc: 0.0942\n",
      "Epoch 21/40\n",
      "1104/1104 [==============================] - 4s 4ms/step - loss: 2.2285 - acc: 0.2745 - val_loss: 2.6368 - val_acc: 0.1123\n",
      "Epoch 22/40\n",
      "1104/1104 [==============================] - 4s 4ms/step - loss: 2.2024 - acc: 0.2835 - val_loss: 2.6822 - val_acc: 0.1014\n",
      "Epoch 23/40\n",
      "1104/1104 [==============================] - 3s 3ms/step - loss: 2.1979 - acc: 0.2998 - val_loss: 2.6533 - val_acc: 0.1123\n",
      "Epoch 24/40\n",
      "1104/1104 [==============================] - 4s 3ms/step - loss: 2.1859 - acc: 0.2908 - val_loss: 2.6881 - val_acc: 0.1087\n",
      "Epoch 25/40\n",
      "1104/1104 [==============================] - 3s 3ms/step - loss: 2.2292 - acc: 0.2781 - val_loss: 2.7128 - val_acc: 0.1159\n",
      "Epoch 26/40\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-fc7d8181812f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimple_NN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-3b11c8313fb4>\u001b[0m in \u001b[0;36msimple_NN\u001b[0;34m(X_train, X_test, y_train, y_test, regularization, drop_likely, learning_rate, beta1, beta2, num_iterations, size_batch, activation)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     nn_history = model.fit(X_train, y_train, batch_size=size_batch, epochs=num_iterations, \n\u001b[0;32m---> 51\u001b[0;31m                          validation_data=(X_test, y_test))\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, nn_history = simple_NN(X_train, X_test, y_train, y_test, num_iterations=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X_train, X_test, y_train, y_test):\n",
    "    a = model.predict(X_train)\n",
    "    predictions = np.zeros_like(a)\n",
    "    predictions[np.arange(len(a)), a.argmax(1)] = 1\n",
    "    print(predictions)\n",
    "    print(y_train)\n",
    "    print(classification_report(y_train,predictions))\n",
    "    a = model.predict(X_test)\n",
    "    predictions = np.zeros_like(a)\n",
    "    predictions[np.arange(len(a)), a.argmax(1)] = 1\n",
    "    print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_cnn(X_train, X_test, y_train, y_test):\n",
    "    # Set the target class number\n",
    "    target_class = y_train.shape[1]\n",
    "    # Model \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv1D(96, 55, padding='valid',input_shape=(X_train.shape[1], 1))) #1\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Conv1D(256, 8, padding='same')) #2\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(128, 8, padding='same')) #3\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Activation('tanh')) \n",
    "    model.add(Conv1D(128, 8, padding='same')) #4\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Conv1D(128, 8, padding='same')) #5\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Conv1D(128, 8, padding='same')) #6\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(64, 8, padding='same')) #7\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Conv1D(64, 8, padding='same')) #8\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(target_class)) #9\n",
    "    model.add(Activation('softmax'))\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=10e-8, decay=0.0, amsgrad=False)\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=opt,metrics=[keras.metrics.categorical_accuracy])\n",
    "    cnnhistory = model.fit(X_train, y_train, batch_size=128, epochs=40, \n",
    "                         validation_data=(X_test, y_test))\n",
    "    a = model.predict(X_train)\n",
    "    predictions = np.zeros_like(a)\n",
    "    predictions[np.arange(len(a)), a.argmax(1)] = 1\n",
    "    print(predictions)\n",
    "    print(y_train)\n",
    "    print(classification_report(y_train,predictions))\n",
    "    a = model.predict(X_test)\n",
    "    predictions = np.zeros_like(a)\n",
    "    predictions[np.arange(len(a)), a.argmax(1)] = 1\n",
    "    print(classification_report(y_test,predictions))\n",
    "    return model, cnnhistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " model, cnnhistory = complex_cnn(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = complex_cnn(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xz = model.predict(X_train)\n",
    "xz[np.where(xz==np.max(xz))] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = complex_cnn(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LeakyReLU\n",
    "def simple_NN(X_train, X_test, y_train, y_test):\n",
    "    target_class = y_train.shape[1]\n",
    "    regularization = 0.01\n",
    "    dropout= 0.25\n",
    "    # Model \n",
    "    model = Sequential()\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(1024, input_dim=X_train.shape[1], \n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(LeakyReLU(alpha=0.3))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(2048, activation='tanh',\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(4096, activation='tanh',\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(2048, activation='tanh',\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "    model.add(Dense(1024, activation='tanh',\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(512, activation='tanh',\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(256, activation='tanh',\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(128, activation='tanh',\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Dense(target_class, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, batch_size=X_train.shape[0], epochs=60, \n",
    "                         validation_data=(X_test, y_test))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = np.load(\"../../splitdata.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0719 17:56:43.868160 140735512335232 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0719 17:56:43.920475 140735512335232 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0719 17:56:44.115530 140735512335232 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0719 17:56:44.163760 140735512335232 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0719 17:56:44.217421 140735512335232 deprecation.py:506] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0719 17:56:46.867654 140735512335232 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0719 17:56:47.413426 140735512335232 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8829 samples, validate on 1104 samples\n",
      "Epoch 1/60\n",
      "8829/8829 [==============================] - 42s 5ms/step - loss: 99.3895 - acc: 0.0853 - val_loss: 96.2678 - val_acc: 0.3161\n",
      "Epoch 2/60\n",
      "8829/8829 [==============================] - 28s 3ms/step - loss: 96.0110 - acc: 0.2963 - val_loss: 92.1451 - val_acc: 0.3678\n",
      "Epoch 3/60\n",
      "8829/8829 [==============================] - 18s 2ms/step - loss: 92.1597 - acc: 0.3290 - val_loss: 87.6857 - val_acc: 0.4357\n",
      "Epoch 4/60\n",
      "8829/8829 [==============================] - 19s 2ms/step - loss: 87.8337 - acc: 0.3772 - val_loss: 83.2669 - val_acc: 0.4438\n",
      "Epoch 5/60\n",
      "8829/8829 [==============================] - 27s 3ms/step - loss: 83.4055 - acc: 0.3991 - val_loss: 78.7043 - val_acc: 0.4737\n",
      "Epoch 6/60\n",
      "8829/8829 [==============================] - 31s 4ms/step - loss: 78.8728 - acc: 0.4215 - val_loss: 74.2302 - val_acc: 0.4837\n",
      "Epoch 7/60\n",
      "8829/8829 [==============================] - 34s 4ms/step - loss: 74.3588 - acc: 0.4343 - val_loss: 69.7662 - val_acc: 0.4991\n",
      "Epoch 8/60\n",
      "8829/8829 [==============================] - 51s 6ms/step - loss: 69.8963 - acc: 0.4466 - val_loss: 65.3742 - val_acc: 0.4991\n",
      "Epoch 9/60\n",
      "8829/8829 [==============================] - 57s 6ms/step - loss: 65.5253 - acc: 0.4527 - val_loss: 61.1302 - val_acc: 0.5072\n",
      "Epoch 10/60\n",
      "8829/8829 [==============================] - 52s 6ms/step - loss: 61.2866 - acc: 0.4594 - val_loss: 57.0533 - val_acc: 0.5045\n",
      "Epoch 11/60\n",
      "8829/8829 [==============================] - 56s 6ms/step - loss: 57.1714 - acc: 0.4737 - val_loss: 53.1425 - val_acc: 0.5118\n",
      "Epoch 12/60\n",
      "8829/8829 [==============================] - 56s 6ms/step - loss: 53.2211 - acc: 0.4891 - val_loss: 49.4120 - val_acc: 0.5127\n",
      "Epoch 13/60\n",
      "8829/8829 [==============================] - 55s 6ms/step - loss: 49.4616 - acc: 0.4978 - val_loss: 45.8349 - val_acc: 0.5109\n",
      "Epoch 14/60\n",
      "8829/8829 [==============================] - 56s 6ms/step - loss: 45.8826 - acc: 0.5066 - val_loss: 42.4157 - val_acc: 0.5181\n",
      "Epoch 15/60\n",
      "8829/8829 [==============================] - 56s 6ms/step - loss: 42.4750 - acc: 0.5109 - val_loss: 39.2359 - val_acc: 0.5272\n",
      "Epoch 16/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0719 18:08:25.992053 140735512335232 ultratb.py:149] Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-e72f85718282>\", line 1, in <module>\n",
      "    model, cnnhistory = simple_NN(X_train, X_val, y_train, y_val)\n",
      "  File \"<ipython-input-5-48c9b40662af>\", line 54, in simple_NN\n",
      "    validation_data=(X_test, y_test))\n",
      "  File \"/usr/local/lib/python3.7/site-packages/keras/engine/training.py\", line 1039, in fit\n",
      "    validation_steps=validation_steps)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/keras/engine/training_arrays.py\", line 199, in fit_loop\n",
      "    outs = f(ins_batch)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\", line 2715, in __call__\n",
      "    return self._call(inputs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\", line 2675, in _call\n",
      "    fetched = self._callable_fn(*array_vals)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1458, in __call__\n",
      "    run_metadata_ptr)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 1464, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 167, in findsource\n",
      "    file = getsourcefile(object) or getfile(object)\n",
      "  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation_wrapper.py\", line 103, in __getattr__\n",
      "    def __getattr__(self, name):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0719 18:08:27.133449 140735512335232 alias.py:221] Invalid alias: The name clear can't be aliased because it is another magic command.\n",
      "E0719 18:08:27.136071 140735512335232 alias.py:221] Invalid alias: The name more can't be aliased because it is another magic command.\n",
      "E0719 18:08:27.138089 140735512335232 alias.py:221] Invalid alias: The name less can't be aliased because it is another magic command.\n",
      "E0719 18:08:27.140185 140735512335232 alias.py:221] Invalid alias: The name man can't be aliased because it is another magic command.\n"
     ]
    }
   ],
   "source": [
    "model, cnnhistory = simple_NN(X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLPalgorithm(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"DELETE\", (X,fs,x_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X,fs,x_size) = np.load(\"DELETE.npy\",  allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = emotex_lib.emotion_extraction_RAVDESS('../../RAVDESS',x_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =np.load(\"RAVDESS_X_Y_train_test.npy\",  allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = emotex_lib.x_y_split(X, fs, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LeakyReLU\n",
    "def simple_NN(X_train, X_test, y_train, y_test, regularization = 0.1,drop_likely = 0.1,\n",
    "              learning_rate = 0.001, beta1 = 0.9, beta2 = 0.999,\n",
    "             num_iterations = 40, size_batch=256,\n",
    "             activation = 'tanh'):\n",
    "    target_class = y_train.shape[1]\n",
    "    # Model \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(1024, input_dim=X_train.shape[1],  activation=activation,\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(2048, activation=activation,\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(Dropout(drop_likely))\n",
    "    \n",
    "    model.add(Dense(2048, activation=activation,\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Dropout(drop_likely))\n",
    "    \n",
    "    model.add(Dense(1024, activation=activation,\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "\n",
    "    model.add(Dense(512, activation=activation,\n",
    "                   kernel_regularizer=regularizers.l2(regularization)))\n",
    "\n",
    "    model.add(Dense(target_class, activation='softmax'))\n",
    "    \n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_acc', mode='max',  patience=2, min_delta=0.01)\n",
    "    \n",
    "    opt = keras.optimizers.Adam(lr=learning_rate, beta_1=beta1, beta_2=beta2, epsilon=10e-8)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    nn_history = model.fit(X_train, y_train, batch_size=size_batch, epochs=num_iterations, \n",
    "                         validation_data=(X_test, y_test),callbacks=[es])\n",
    "    return model, nn_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"RAVDESS_X_Y_train_test\", (X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_extraction_RAVDESS(folder_location, number_examples):\n",
    "    nu_emotion = 8\n",
    "    y = np.zeros(shape=(nu_emotion, number_examples))\n",
    "    counter = 0\n",
    "    directory = os.fsencode(folder_location)\n",
    "    for file_name in os.listdir(directory):\n",
    "        if  str(file_name) != \"b'.DS_Store'\":\n",
    "            emotion = str(file_name)[2:-1]\n",
    "            i = (emotion.split('-')[2])\n",
    "            if i == '01': # neutral \n",
    "                y[0][counter] = 1\n",
    "            elif i == '02': #calm\n",
    "                 y[1][counter] = 1\n",
    "            elif i == '03': #hahppy \n",
    "                 y[2][counter] = 1\n",
    "            elif i == '04': #sad\n",
    "                 y[3][counter] = 1\n",
    "            elif i == '05': #angry\n",
    "                 y[4][counter] = 1\n",
    "            elif i == '06': #fearful\n",
    "                 y[5][counter] = 1\n",
    "            elif i == '07': #disgust\n",
    "                 y[6][counter] = 1\n",
    "            elif i == '08': #surprised\n",
    "                 y[7][counter] = 1\n",
    "            else:\n",
    "                print(i)\n",
    "                break\n",
    "            counter +=1\n",
    "    y = np.transpose(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize dataset and test set\n",
    "# dropout regularization\n",
    "# L2 regularization\n",
    "# early stopping\n",
    "#  torrent the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLPalgorithm(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLPalgorithm(X_train, X_test, y_train, y_test):\n",
    "    mlp = MLPClassifier(max_iter=15000)\n",
    "    mlp.fit(X_train,y_train)\n",
    "    predictions = mlp.predict(X_train)\n",
    "\n",
    "    print(classification_report(y_train,predictions))\n",
    "    predictions = mlp.predict(X_test)\n",
    "    print(classification_report(y_test,predictions))\n",
    "seed = 20\n",
    "num_labels =15\n",
    "num_features = X_train.shape[1]\n",
    "# build model\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=num_features, activation='softmax'))\n",
    "    model.add(Dense(30, activation='softmax'))\n",
    "    model.add(Dense(output_dim = num_labels, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adadelta(), metrics=['categorical_accuracy'])\n",
    "    return model\n",
    "def big_boy_CNN():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(15, activation='softmax'))\n",
    "    model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def complex_layers():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(256, 5,padding='same', input_shape=( 11,13, 1))) #1\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D(128, 5,padding='same')) #2\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(MaxPooling1D(pool_size=(8)))\n",
    "    model.add(Conv1D(128, 5,padding='same')) #3\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D(128, 5,padding='same')) #4\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D(128, 5,padding='same')) #5\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(128, 5,padding='same')) #6\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10)) #7\n",
    "    model.add(Activation('softmax'))\n",
    "    opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "    return model\n",
    "def easy_NN():\n",
    "    mlp = MLPClassifier(max_iter=15000)\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_data = X\n",
    "MFCC2 = []\n",
    "for one_sound in np_data:\n",
    "    one_sound = np.asarray(one_sound)\n",
    "    MFCC2.append(python_speech_features.base.mfcc(one_sound, samplerate=fs, \n",
    "                                 winlen=0.025, winstep=0.01, numcep=13, \n",
    "                                 nfilt=26, nfft=1200).T)\n",
    "MFCC3 = []\n",
    "cached_variables = []\n",
    "for one_point in MFCC2:\n",
    "    cache_grad = (np.gradient(one_point, axis = 1))\n",
    "    cached_variables.append([np.mean(one_point, axis = 1), np.median(one_point, axis = 1),\n",
    "                             np.var(one_point, axis = 1), \n",
    "                       np.min(one_point, axis = 1), np.max(one_point, axis = 1), \n",
    "                             np.mean(cache_grad, axis = 1), np.var(cache_grad, axis = 1)])\n",
    "return cached_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot([1,2,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(one_point, axis = 1).shape, np.median(one_point, axis = 1).shape,np.var(one_point, axis = 1).shape, np.min(one_point, axis = 1).shape, np.max(one_point, axis = 1).shape, np.mean(cache_grad, axis = 1).shape, np.var(cache_grad, axis = 1).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(please.T, axis = 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(MFCC3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(MFCC2[0], axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.gradient(MFCC2[0], axis = 1)), len(np.gradient(MFCC2[0], axis = 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
