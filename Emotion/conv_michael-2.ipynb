{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "import scipy\n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import osascript\n",
    "from gtts import gTTS \n",
    "import os \n",
    "import pyaudio\n",
    "import wave\n",
    "import keyboard  # using module keyboard\n",
    "import soundfile as sf\n",
    "import math\n",
    "import pyloudnorm as pyln\n",
    "from sys import byteorder\n",
    "from array import array\n",
    "from struct import pack\n",
    "import librosa\n",
    "from scipy.signal import butter, sosfiltfilt\n",
    "import pysptk\n",
    "import python_speech_features\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "import time\n",
    "import sherpa\n",
    "import sherpa.algorithms.bayesian_optimization as bayesian_optimization\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "import keras as keras\n",
    "from  conch.analysis.formants import lpc\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras import regularizers\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = np.load(\"../../splitdata.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, fs, y = np.load(\"../../CREMA/CREMAdatafsy.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = int(512*(fs/1000))\n",
    "one_point = dataset[0][0:chunk]\n",
    "one_point.shape, fs, chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_silence_from(amplitudes, threshold):\n",
    "    silenced = []\n",
    "    for x in amplitudes:\n",
    "        if x >= threshold:\n",
    "            silenced.append(x)\n",
    "    return silenced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MFCC_algorithm(np_data, fs):\n",
    "    MFCCs = []\n",
    "    print(\"running .....\")\n",
    "    #for progess bar\n",
    "    for one_sound in np_data:\n",
    "        \n",
    "        one_sound = np.asarray(one_sound).reshape(int((fs/1000)*512), 1)\n",
    "        \n",
    "        MFCCs.append(python_speech_features.base.mfcc(one_sound, samplerate=fs, \n",
    "                                     winlen=0.025, winstep=0.01, numcep=13, \n",
    "                                     nfilt=26, nfft=552))\n",
    "    return np.array(MFCCs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pitch_vector(data, fs):\n",
    "    data = np.float32(data)\n",
    "    pitch = pysptk.sptk.rapt(y=data, sr=fs, hopsize = 40)\n",
    "    silenced = remove_silence_from(pitch, np.mean(pitch))\n",
    "    return silenced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectral_vector(data, fs):\n",
    "    data = np.float32(data)\n",
    "    cent = librosa.feature.spectral_centroid(y=data, sr=fs, hop_length=165)\n",
    "    return cent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rms_vector(data):\n",
    "    temp_data = np.float32(data)\n",
    "    cent = librosa.feature.rms(y=temp_data, hop_length=165)\n",
    "    return cent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zero_vector(data):\n",
    "    temp_data = np.float32(data)\n",
    "    cent = librosa.feature.zero_crossing_rate(y=temp_data, hop_length=165)\n",
    "    return cent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sr_vector(data):\n",
    "    temp_data = np.float32(data)\n",
    "    cent = librosa.feature.spectral_rolloff(y=temp_data, hop_length=165)\n",
    "    return cent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def me_and_gradient(x):\n",
    "    return x, np.gradient(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len((get_spectral_vector(one_point, fs))[0]))\n",
    "\n",
    "print(len(get_spectral_vector(one_point, fs)[0]))\n",
    "\n",
    "print(len(get_zero_vector(one_point)[0]))\n",
    "\n",
    "print(len(get_rms_vector(one_point)[0]))\n",
    "\n",
    "print(len(get_sr_vector(one_point)[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Y_BOG_SPEC\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f9368f694a9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museless_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../CREMA_chunked.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0;32m--> 447\u001b[0;31m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    448\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m             \u001b[0mpickle_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "E0801 16:56:42.455740 140735549535104 alias.py:221] Invalid alias: The name clear can't be aliased because it is another magic command.\n",
      "E0801 16:56:42.458575 140735549535104 alias.py:221] Invalid alias: The name more can't be aliased because it is another magic command.\n",
      "E0801 16:56:42.459796 140735549535104 alias.py:221] Invalid alias: The name less can't be aliased because it is another magic command.\n",
      "E0801 16:56:42.460480 140735549535104 alias.py:221] Invalid alias: The name man can't be aliased because it is another magic command.\n"
     ]
    }
   ],
   "source": [
    "fs = 16000\n",
    "dataset, y, useless_number = np.load(\"../../CREMA_chunked.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = MFCC_algorithm(dataset, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_CREMA_MFCC\", X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(X.shape[0], 50, 13, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_Y_SPLIT.npy\", (X_train,X_val, X_test, y_train,y_val, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,X_val, X_test, y_train,y_val, y_test) = np.load(\"X_Y_SPLIT.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train shape: (32499, 50, 13, 1)\n",
      "y train shape: (32499, 12)\n",
      "x test shape: (4063, 50, 13, 1)\n",
      "y test shape: (4063, 12)\n"
     ]
    }
   ],
   "source": [
    "print(\"x train shape: \" +str(X_train.shape))\n",
    "print(\"y train shape: \" +str(y_train.shape))\n",
    "print(\"x test shape: \" +str(X_test.shape))\n",
    "print(\"y test shape: \" +str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_MFCC_LSTM(X_train, X_test, y_train, y_test, regularization = 0.1,drop_likely = 0.1,\n",
    "              learning_rate = 0.001, beta1 = 0.9, beta2 = 0.999,\n",
    "             num_iterations = 40, size_batch=64,activation = 'relu'):\n",
    "    # Set the target class number\n",
    "    \n",
    "    # VGG 19\n",
    "    \n",
    "    target_class = y_train.shape[1]\n",
    "    # Model \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(keras.layers.BatchNormalization())    \n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size = (3,3), activation=activation, input_shape = (50,13,1)))\n",
    "    model.add(Conv2D(64, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (1,1)))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(Conv2D(128, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (1,1)))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(256, kernel_size = (3,3), activation=activation))\n",
    "    model.add(Conv2D(256, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(Conv2D(256, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(Conv2D(256, kernel_size = (3,3), activation=activation))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (1,1)))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    \n",
    "    model.add(Conv2D(512, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(Conv2D(512, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(Conv2D(512, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(Conv2D(512, kernel_size = (3,3), activation=activation))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(Flatten())\n",
    "\n",
    "    \n",
    "    \n",
    "    model.add(Dense(4096, activation=activation))\n",
    "    model.add(Dense(4096, activation=activation))\n",
    "    model.add(Dense(256, activation=activation))\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(Dense(target_class, activation='softmax'))\n",
    "\n",
    "    \n",
    "    \n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_acc', mode='max',  patience=10, min_delta=0)\n",
    "    \n",
    "    opt = keras.optimizers.Adam(lr=learning_rate, beta_1=beta1, beta_2=beta2, epsilon=10e-8)\n",
    "       \n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=opt,metrics=[keras.metrics.categorical_accuracy])\n",
    "    \n",
    "    nn_history = model.fit(X_train, y_train, batch_size=size_batch, epochs=num_iterations, \n",
    "                         validation_data=(X_test, y_test),\n",
    "                          callbacks=[es])\n",
    "    \n",
    "    a = model.predict(X_train)\n",
    "    predictions = np.zeros_like(a)\n",
    "    predictions[np.arange(len(a)), a.argmax(1)] = 1\n",
    "    print(predictions)\n",
    "    print(y_train)\n",
    "    print(classification_report(y_train,predictions))\n",
    "    a = model.predict(X_test)\n",
    "    predictions = np.zeros_like(a)\n",
    "    predictions[np.arange(len(a)), a.argmax(1)] = 1\n",
    "    print(classification_report(y_test,predictions))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0802 11:17:18.590553 140735549535104 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0802 11:17:18.767642 140735549535104 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0802 11:17:18.787890 140735549535104 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0802 11:17:18.988568 140735549535104 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_cnn(X_train, X_test, y_train, y_test, regularization = 0.1,drop_likely = 0.1,\n",
    "              learning_rate = 0.001, beta1 = 0.9, beta2 = 0.999,\n",
    "             num_iterations = 40, size_batch=128,activation = 'tanh'):\n",
    "    target_class = y_train.shape[1]\n",
    "    # Model \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Conv2D(64, kernel_size = (3,3), activation=activation, input_shape = (50,13,1)))\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size = (3,3), activation=activation))\n",
    "\n",
    "    model.add(Conv2D(128, kernel_size = (3,3), activation=activation))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (1,1)))\n",
    "    \n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(2048, activation=activation))\n",
    "    model.add(Dense(1024, activation=activation))\n",
    "    model.add(Dense(512, activation=activation))\n",
    "    \n",
    "    model.add(Dense(target_class, activation='softmax'))\n",
    "\n",
    "    \n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', mode='max',  patience=10, min_delta=0)\n",
    "    \n",
    "    opt = keras.optimizers.Adam(lr=learning_rate, beta_1=beta1, beta_2=beta2, epsilon=10e-8)\n",
    "       \n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=opt,metrics=[keras.metrics.categorical_accuracy])\n",
    "    \n",
    "    nn_history = model.fit(X_train, y_train, batch_size=size_batch, epochs=num_iterations, \n",
    "                         validation_data=(X_test, y_test),\n",
    "                          callbacks=[es])\n",
    "    \n",
    "    a = model.predict(X_train)\n",
    "    predictions = np.zeros_like(a)\n",
    "    predictions[np.arange(len(a)), a.argmax(1)] = 1\n",
    "    print(predictions)\n",
    "    print(y_train)\n",
    "    print(classification_report(y_train,predictions))\n",
    "    a = model.predict(X_test)\n",
    "    predictions = np.zeros_like(a)\n",
    "    predictions[np.arange(len(a)), a.argmax(1)] = 1\n",
    "    print(classification_report(y_test,predictions))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32499 samples, validate on 4063 samples\n",
      "Epoch 1/40\n",
      "32499/32499 [==============================] - 721s 22ms/step - loss: 2.5986 - categorical_accuracy: 0.0875 - val_loss: 2.5069 - val_categorical_accuracy: 0.0925\n",
      "Epoch 2/40\n",
      "32499/32499 [==============================] - 981s 30ms/step - loss: 2.5065 - categorical_accuracy: 0.0893 - val_loss: 2.5363 - val_categorical_accuracy: 0.0817\n",
      "Epoch 3/40\n",
      "24960/32499 [======================>.......] - ETA: 3:37 - loss: 2.5147 - categorical_accuracy: 0.0875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0729 11:00:10.212630 140735522395008 ultratb.py:149] Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-07b4934d80f8>\", line 1, in <module>\n",
      "    simple_cnn(X_train, X_test, y_train, y_test)\n",
      "  File \"<ipython-input-6-d18395cd049d>\", line 40, in simple_cnn\n",
      "    callbacks=[es])\n",
      "  File \"/usr/local/lib/python3.7/site-packages/keras/engine/training.py\", line 1039, in fit\n",
      "    validation_steps=validation_steps)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/keras/engine/training_arrays.py\", line 199, in fit_loop\n",
      "    outs = f(ins_batch)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\", line 2715, in __call__\n",
      "    return self._call(inputs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\", line 2675, in _call\n",
      "    fetched = self._callable_fn(*array_vals)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1458, in __call__\n",
      "    run_metadata_ptr)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/posixpath.py\", line 415, in _joinrealpath\n",
      "    name, _, rest = rest.partition(sep)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0729 11:00:10.812683 140735522395008 alias.py:221] Invalid alias: The name clear can't be aliased because it is another magic command.\n",
      "E0729 11:00:10.813905 140735522395008 alias.py:221] Invalid alias: The name more can't be aliased because it is another magic command.\n",
      "E0729 11:00:10.814826 140735522395008 alias.py:221] Invalid alias: The name less can't be aliased because it is another magic command.\n",
      "E0729 11:00:10.815836 140735522395008 alias.py:221] Invalid alias: The name man can't be aliased because it is another magic command.\n"
     ]
    }
   ],
   "source": [
    "simple_cnn(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_MFCC_LSTM(X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.04027690e+01, -2.29216742e+00, -2.45621438e+01, ...,\n",
       "         4.88963861e+00,  1.48930418e+01,  9.20361442e+00],\n",
       "       [ 1.13726075e+01,  1.64783184e-01, -2.58792595e+01, ...,\n",
       "        -2.06376344e+00,  1.46754733e+01,  7.15964668e+00],\n",
       "       [ 1.36984766e+01, -3.23827023e+00, -2.46365405e+01, ...,\n",
       "        -9.64290945e-01,  4.47336320e+00,  8.45182667e+00],\n",
       "       ...,\n",
       "       [ 1.89949429e+01,  2.19963632e+01, -2.23273079e+01, ...,\n",
       "        -6.88303927e+00,  2.70539077e+01,  4.76705165e+00],\n",
       "       [ 1.49043370e+01,  9.61019133e+00, -3.04150089e+01, ...,\n",
       "        -2.10617805e-02, -5.44090491e+00, -1.51162092e+00],\n",
       "       [ 1.03970799e+01, -2.06840049e+00, -1.48779458e+01, ...,\n",
       "         4.57743321e+00,  2.12141676e+01,  1.02466831e+01]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(X_train, (X_train.shape[0], 50*13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_1dconv(X_train, X_test, y_train, y_test, regularization = 0.1,drop_likely = 0.1,\n",
    "              learning_rate = 0.001, beta1 = 0.9, beta2 = 0.999,\n",
    "             num_iterations = 40, size_batch=128,activation = 'relu'):\n",
    "    target_class = y_train.shape[1]\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], 50*13,1))\n",
    "    X_test =np.reshape(X_test, (X_test.shape[0], 50*13,1))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(256, 5, padding='same',input_shape=(X_train.shape[1],1))) #1\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Conv1D(256, 5, padding='same')) #2\n",
    "    model.add(keras.layers.BatchNormalization())    \n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(MaxPooling1D(pool_size=(8)))\n",
    "    model.add(Conv1D(128, 5, padding='same')) #3\n",
    "    model.add(Activation(activation)) \n",
    "    model.add(Conv1D(128, 5, padding='same')) #4\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Conv1D(128, 5, padding='same')) #5\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Conv1D(128, 5, padding='same')) #6\n",
    "    model.add(keras.layers.BatchNormalization())    \n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(MaxPooling1D(pool_size=(8)))\n",
    "    model.add(Conv1D(64, 5, padding='same')) #7\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Conv1D(64, 5, padding='same')) #8\n",
    "    model.add(Activation(activation))\n",
    "\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    \n",
    "    model.add(Dense(target_class)) #9\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    \n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', mode='max',  patience=10, min_delta=0)\n",
    "    \n",
    "    opt = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    \n",
    "#     opt = keras.optimizers.Adam(lr=learning_rate, beta_1=beta1, beta_2=beta2, epsilon=10e-8)\n",
    "       \n",
    "    lr_reduce = keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.9, patience=20, min_lr=0.000001)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=opt)\n",
    "    cnnhistory=model.fit(X_train, y_train, batch_size=16, epochs=700, validation_data=(X_test, y_test), \n",
    "                             callbacks=[lr_reduce])\n",
    "\n",
    "    \n",
    "    a = model.predict(X_train)\n",
    "    predictions = np.zeros_like(a)\n",
    "    predictions[np.arange(len(a)), a.argmax(1)] = 1\n",
    "    print(predictions)\n",
    "    print(y_train)\n",
    "    print(classification_report(y_train,predictions))\n",
    "    a = model.predict(X_test)\n",
    "    predictions = np.zeros_like(a)\n",
    "    predictions[np.arange(len(a)), a.argmax(1)] = 1\n",
    "    print(classification_report(y_test,predictions))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32499 samples, validate on 4063 samples\n",
      "Epoch 1/700\n",
      "32499/32499 [==============================] - 946s 29ms/step - loss: 2.4967 - val_loss: 2.4478\n",
      "Epoch 2/700\n",
      "32499/32499 [==============================] - 646s 20ms/step - loss: 2.3753 - val_loss: 2.4057\n",
      "Epoch 3/700\n",
      "32499/32499 [==============================] - 647s 20ms/step - loss: 2.2933 - val_loss: 2.3426\n",
      "Epoch 4/700\n",
      "32499/32499 [==============================] - 649s 20ms/step - loss: 2.2295 - val_loss: 2.3211\n",
      "Epoch 5/700\n",
      "32499/32499 [==============================] - 674s 21ms/step - loss: 2.1756 - val_loss: 2.2523\n",
      "Epoch 6/700\n",
      "32499/32499 [==============================] - 651s 20ms/step - loss: 2.1436 - val_loss: 2.2837\n",
      "Epoch 7/700\n",
      "32499/32499 [==============================] - 655s 20ms/step - loss: 2.1164 - val_loss: 2.1921\n",
      "Epoch 8/700\n",
      "32499/32499 [==============================] - 615s 19ms/step - loss: 2.0928 - val_loss: 2.2812\n",
      "Epoch 9/700\n",
      "32499/32499 [==============================] - 582s 18ms/step - loss: 2.0835 - val_loss: 2.1872\n",
      "Epoch 10/700\n",
      "32499/32499 [==============================] - 601s 19ms/step - loss: 2.0644 - val_loss: 2.2163\n",
      "Epoch 11/700\n",
      "32499/32499 [==============================] - 585s 18ms/step - loss: 2.0546 - val_loss: 2.2091\n",
      "Epoch 12/700\n",
      "32499/32499 [==============================] - 552s 17ms/step - loss: 2.0418 - val_loss: 2.2289\n",
      "Epoch 13/700\n",
      "32499/32499 [==============================] - 566s 17ms/step - loss: 2.0316 - val_loss: 2.1414\n",
      "Epoch 14/700\n",
      "32499/32499 [==============================] - 578s 18ms/step - loss: 2.0261 - val_loss: 2.1532\n",
      "Epoch 15/700\n",
      "32499/32499 [==============================] - 574s 18ms/step - loss: 2.0207 - val_loss: 2.1237\n",
      "Epoch 16/700\n",
      "32499/32499 [==============================] - 564s 17ms/step - loss: 2.0125 - val_loss: 2.1772\n",
      "Epoch 17/700\n",
      "32499/32499 [==============================] - 575s 18ms/step - loss: 2.0021 - val_loss: 2.1547\n",
      "Epoch 18/700\n",
      "32499/32499 [==============================] - 577s 18ms/step - loss: 2.0010 - val_loss: 2.1187\n",
      "Epoch 19/700\n",
      "32499/32499 [==============================] - 563s 17ms/step - loss: 1.9882 - val_loss: 2.1437\n",
      "Epoch 20/700\n",
      "32499/32499 [==============================] - 544s 17ms/step - loss: 1.9868 - val_loss: 2.1731\n",
      "Epoch 21/700\n",
      "32499/32499 [==============================] - 542s 17ms/step - loss: 1.9782 - val_loss: 2.2544\n",
      "Epoch 22/700\n",
      "32499/32499 [==============================] - 545s 17ms/step - loss: 1.9766 - val_loss: 2.1779\n",
      "Epoch 23/700\n",
      "32499/32499 [==============================] - 573s 18ms/step - loss: 1.9699 - val_loss: 2.1440\n",
      "Epoch 24/700\n",
      "32499/32499 [==============================] - 582s 18ms/step - loss: 1.9658 - val_loss: 2.0631\n",
      "Epoch 25/700\n",
      "32499/32499 [==============================] - 562s 17ms/step - loss: 1.9628 - val_loss: 2.1408\n",
      "Epoch 26/700\n",
      "32499/32499 [==============================] - 551s 17ms/step - loss: 1.9532 - val_loss: 2.1449\n",
      "Epoch 27/700\n",
      "32499/32499 [==============================] - 579s 18ms/step - loss: 1.9521 - val_loss: 2.0623\n",
      "Epoch 28/700\n",
      "32499/32499 [==============================] - 598s 18ms/step - loss: 1.9533 - val_loss: 2.0853\n",
      "Epoch 29/700\n",
      "32499/32499 [==============================] - 593s 18ms/step - loss: 1.9481 - val_loss: 2.1002\n",
      "Epoch 30/700\n",
      "32499/32499 [==============================] - 595s 18ms/step - loss: 1.9448 - val_loss: 2.1100\n",
      "Epoch 31/700\n",
      "32499/32499 [==============================] - 592s 18ms/step - loss: 1.9433 - val_loss: 2.1294\n",
      "Epoch 32/700\n",
      "32499/32499 [==============================] - 596s 18ms/step - loss: 1.9375 - val_loss: 2.1462\n",
      "Epoch 33/700\n",
      "32499/32499 [==============================] - 596s 18ms/step - loss: 1.9337 - val_loss: 2.1047\n",
      "Epoch 34/700\n",
      "32499/32499 [==============================] - 599s 18ms/step - loss: 1.9332 - val_loss: 2.0802\n",
      "Epoch 35/700\n",
      "32499/32499 [==============================] - 596s 18ms/step - loss: 1.9314 - val_loss: 2.1183\n",
      "Epoch 36/700\n",
      "32499/32499 [==============================] - 602s 19ms/step - loss: 1.9263 - val_loss: 2.1284\n",
      "Epoch 37/700\n",
      "32499/32499 [==============================] - 599s 18ms/step - loss: 1.9182 - val_loss: 2.0820\n",
      "Epoch 38/700\n",
      "32499/32499 [==============================] - 596s 18ms/step - loss: 1.9204 - val_loss: 2.0766\n",
      "Epoch 39/700\n",
      "32499/32499 [==============================] - 598s 18ms/step - loss: 1.9151 - val_loss: 2.0631\n",
      "Epoch 40/700\n",
      "32499/32499 [==============================] - 597s 18ms/step - loss: 1.9163 - val_loss: 2.1221\n",
      "Epoch 41/700\n",
      "32499/32499 [==============================] - 595s 18ms/step - loss: 1.9124 - val_loss: 2.0774\n",
      "Epoch 42/700\n",
      "32499/32499 [==============================] - 597s 18ms/step - loss: 1.9106 - val_loss: 2.0393\n",
      "Epoch 43/700\n",
      "32499/32499 [==============================] - 595s 18ms/step - loss: 1.9092 - val_loss: 2.1221\n",
      "Epoch 44/700\n",
      "32499/32499 [==============================] - 597s 18ms/step - loss: 1.9075 - val_loss: 2.0675\n",
      "Epoch 45/700\n",
      "32499/32499 [==============================] - 597s 18ms/step - loss: 1.9005 - val_loss: 2.0739\n",
      "Epoch 46/700\n",
      "32499/32499 [==============================] - 596s 18ms/step - loss: 1.8995 - val_loss: 2.0804\n",
      "Epoch 47/700\n",
      "32499/32499 [==============================] - 600s 18ms/step - loss: 1.8968 - val_loss: 2.1634\n",
      "Epoch 48/700\n",
      "32499/32499 [==============================] - 604s 19ms/step - loss: 1.8949 - val_loss: 2.1127\n",
      "Epoch 49/700\n",
      "32499/32499 [==============================] - 597s 18ms/step - loss: 1.8944 - val_loss: 2.2121\n",
      "Epoch 50/700\n",
      "32499/32499 [==============================] - 599s 18ms/step - loss: 1.8894 - val_loss: 2.3022\n",
      "Epoch 51/700\n",
      "32499/32499 [==============================] - 597s 18ms/step - loss: 1.8899 - val_loss: 2.0645\n",
      "Epoch 52/700\n",
      "32499/32499 [==============================] - 599s 18ms/step - loss: 1.8923 - val_loss: 2.1814\n",
      "Epoch 53/700\n",
      "32499/32499 [==============================] - 597s 18ms/step - loss: 1.8846 - val_loss: 2.0686\n",
      "Epoch 54/700\n",
      "32499/32499 [==============================] - 591s 18ms/step - loss: 1.8809 - val_loss: 2.0831\n",
      "Epoch 55/700\n",
      "32499/32499 [==============================] - 584s 18ms/step - loss: 1.8829 - val_loss: 2.0626\n",
      "Epoch 56/700\n",
      "32499/32499 [==============================] - 590s 18ms/step - loss: 1.8785 - val_loss: 2.1056\n",
      "Epoch 57/700\n",
      "32499/32499 [==============================] - 593s 18ms/step - loss: 1.8783 - val_loss: 2.1388\n",
      "Epoch 58/700\n",
      "32499/32499 [==============================] - 589s 18ms/step - loss: 1.8753 - val_loss: 2.0087\n",
      "Epoch 59/700\n",
      "32499/32499 [==============================] - 584s 18ms/step - loss: 1.8729 - val_loss: 2.0268\n",
      "Epoch 60/700\n",
      "32499/32499 [==============================] - 580s 18ms/step - loss: 1.8735 - val_loss: 2.0639\n",
      "Epoch 61/700\n",
      "32499/32499 [==============================] - 586s 18ms/step - loss: 1.8719 - val_loss: 2.0547\n",
      "Epoch 62/700\n",
      "32499/32499 [==============================] - 588s 18ms/step - loss: 1.8682 - val_loss: 2.0858\n",
      "Epoch 63/700\n",
      "32499/32499 [==============================] - 590s 18ms/step - loss: 1.8645 - val_loss: 2.0791\n",
      "Epoch 64/700\n",
      "32499/32499 [==============================] - 589s 18ms/step - loss: 1.8655 - val_loss: 2.0746\n",
      "Epoch 65/700\n",
      "32499/32499 [==============================] - 583s 18ms/step - loss: 1.8616 - val_loss: 2.0477\n",
      "Epoch 66/700\n",
      "32499/32499 [==============================] - 591s 18ms/step - loss: 1.8589 - val_loss: 2.0860\n",
      "Epoch 67/700\n",
      "32499/32499 [==============================] - 598s 18ms/step - loss: 1.8584 - val_loss: 2.0119\n",
      "Epoch 68/700\n",
      "32499/32499 [==============================] - 595s 18ms/step - loss: 1.8567 - val_loss: 2.0625\n",
      "Epoch 69/700\n",
      "32499/32499 [==============================] - 589s 18ms/step - loss: 1.8562 - val_loss: 2.1338\n",
      "Epoch 70/700\n",
      "32499/32499 [==============================] - 593s 18ms/step - loss: 1.8510 - val_loss: 2.0830\n",
      "Epoch 71/700\n",
      "32499/32499 [==============================] - 586s 18ms/step - loss: 1.8511 - val_loss: 2.0546\n",
      "Epoch 72/700\n",
      "32499/32499 [==============================] - 591s 18ms/step - loss: 1.8520 - val_loss: 2.0510\n",
      "Epoch 73/700\n",
      "32499/32499 [==============================] - 556s 17ms/step - loss: 1.8463 - val_loss: 2.0302\n",
      "Epoch 74/700\n",
      "32499/32499 [==============================] - 573s 18ms/step - loss: 1.8475 - val_loss: 2.0549\n",
      "Epoch 75/700\n",
      "32499/32499 [==============================] - 587s 18ms/step - loss: 1.8418 - val_loss: 2.1145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/700\n",
      "32499/32499 [==============================] - 585s 18ms/step - loss: 1.8409 - val_loss: 2.1548\n",
      "Epoch 77/700\n",
      "32499/32499 [==============================] - 583s 18ms/step - loss: 1.8383 - val_loss: 1.9976\n",
      "Epoch 78/700\n",
      "32499/32499 [==============================] - 581s 18ms/step - loss: 1.8368 - val_loss: 2.0857\n",
      "Epoch 79/700\n",
      "32499/32499 [==============================] - 578s 18ms/step - loss: 1.8394 - val_loss: 2.1315\n",
      "Epoch 80/700\n",
      "32499/32499 [==============================] - 579s 18ms/step - loss: 1.8346 - val_loss: 1.9794\n",
      "Epoch 81/700\n",
      "32499/32499 [==============================] - 581s 18ms/step - loss: 1.8347 - val_loss: 2.1766\n",
      "Epoch 82/700\n",
      "32499/32499 [==============================] - 583s 18ms/step - loss: 1.8325 - val_loss: 1.9936\n",
      "Epoch 83/700\n",
      "32499/32499 [==============================] - 589s 18ms/step - loss: 1.8320 - val_loss: 2.1384\n",
      "Epoch 84/700\n",
      "32499/32499 [==============================] - 582s 18ms/step - loss: 1.8325 - val_loss: 2.1415\n",
      "Epoch 85/700\n",
      "32499/32499 [==============================] - 583s 18ms/step - loss: 1.8286 - val_loss: 2.0653\n",
      "Epoch 86/700\n",
      "32499/32499 [==============================] - 582s 18ms/step - loss: 1.8264 - val_loss: 2.1293\n",
      "Epoch 87/700\n",
      "32499/32499 [==============================] - 574s 18ms/step - loss: 1.8250 - val_loss: 2.0036\n",
      "Epoch 88/700\n",
      "32499/32499 [==============================] - 575s 18ms/step - loss: 1.8255 - val_loss: 2.0278\n",
      "Epoch 89/700\n",
      "32499/32499 [==============================] - 575s 18ms/step - loss: 1.8157 - val_loss: 2.1516\n",
      "Epoch 90/700\n",
      "32499/32499 [==============================] - 582s 18ms/step - loss: 1.8183 - val_loss: 1.9854\n",
      "Epoch 91/700\n",
      "32499/32499 [==============================] - 584s 18ms/step - loss: 1.8159 - val_loss: 2.1270\n",
      "Epoch 92/700\n",
      "32499/32499 [==============================] - 548s 17ms/step - loss: 1.8173 - val_loss: 2.1604\n",
      "Epoch 93/700\n",
      "32499/32499 [==============================] - 559s 17ms/step - loss: 1.8121 - val_loss: 2.0609\n",
      "Epoch 94/700\n",
      "32499/32499 [==============================] - 540s 17ms/step - loss: 1.8142 - val_loss: 2.1084\n",
      "Epoch 95/700\n",
      "32499/32499 [==============================] - 540s 17ms/step - loss: 1.8117 - val_loss: 2.0590\n",
      "Epoch 96/700\n",
      "32499/32499 [==============================] - 542s 17ms/step - loss: 1.8089 - val_loss: 2.0825\n",
      "Epoch 97/700\n",
      "32499/32499 [==============================] - 542s 17ms/step - loss: 1.8083 - val_loss: 2.0482\n",
      "Epoch 98/700\n",
      "32499/32499 [==============================] - 547s 17ms/step - loss: 1.8061 - val_loss: 2.2696\n",
      "Epoch 99/700\n",
      "32499/32499 [==============================] - 562s 17ms/step - loss: 1.8083 - val_loss: 2.0403\n",
      "Epoch 100/700\n",
      "32499/32499 [==============================] - 544s 17ms/step - loss: 1.8062 - val_loss: 2.1018\n",
      "Epoch 101/700\n",
      "32499/32499 [==============================] - 546s 17ms/step - loss: 1.8055 - val_loss: 2.0312\n",
      "Epoch 102/700\n",
      "32499/32499 [==============================] - 548s 17ms/step - loss: 1.7984 - val_loss: 2.0340\n",
      "Epoch 103/700\n",
      "32499/32499 [==============================] - 550s 17ms/step - loss: 1.8024 - val_loss: 2.1014\n",
      "Epoch 104/700\n",
      "32499/32499 [==============================] - 551s 17ms/step - loss: 1.7984 - val_loss: 2.0698\n",
      "Epoch 105/700\n",
      "32499/32499 [==============================] - 577s 18ms/step - loss: 1.7943 - val_loss: 2.1344\n",
      "Epoch 106/700\n",
      "32499/32499 [==============================] - 574s 18ms/step - loss: 1.7942 - val_loss: 2.1558\n",
      "Epoch 107/700\n",
      "32499/32499 [==============================] - 569s 18ms/step - loss: 1.7933 - val_loss: 2.2152\n",
      "Epoch 108/700\n",
      "32499/32499 [==============================] - 571s 18ms/step - loss: 1.7931 - val_loss: 1.9952\n",
      "Epoch 109/700\n",
      "32499/32499 [==============================] - 610s 19ms/step - loss: 1.7888 - val_loss: 2.2267\n",
      "Epoch 110/700\n",
      "32499/32499 [==============================] - 605s 19ms/step - loss: 1.7875 - val_loss: 2.0376\n",
      "Epoch 111/700\n",
      "32499/32499 [==============================] - 634s 20ms/step - loss: 1.7893 - val_loss: 2.1553\n",
      "Epoch 112/700\n",
      "32499/32499 [==============================] - 599s 18ms/step - loss: 1.7838 - val_loss: 2.0551\n",
      "Epoch 113/700\n",
      "32499/32499 [==============================] - 612s 19ms/step - loss: 1.7844 - val_loss: 2.1468\n",
      "Epoch 114/700\n",
      "32499/32499 [==============================] - 617s 19ms/step - loss: 1.7832 - val_loss: 2.0333\n",
      "Epoch 115/700\n",
      "32499/32499 [==============================] - 594s 18ms/step - loss: 1.7830 - val_loss: 1.9963\n",
      "Epoch 116/700\n",
      "32499/32499 [==============================] - 563s 17ms/step - loss: 1.7817 - val_loss: 1.9921\n",
      "Epoch 117/700\n",
      "32499/32499 [==============================] - 544s 17ms/step - loss: 1.7727 - val_loss: 2.0255\n",
      "Epoch 118/700\n",
      "28976/32499 [=========================>....] - ETA: 57s - loss: 1.7769"
     ]
    }
   ],
   "source": [
    "simple_1dconv(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = [sherpa.Continuous(name='lr', range=[0.0001, 0.1], scale='log'),\n",
    "              sherpa.Continuous(name='beta1', range=[0.85, 1.0], scale='log'),\n",
    "              sherpa.Continuous(name=\"regularization\", range=[0.0001, 1], scale='log'),\n",
    "              sherpa.Continuous(name='dropout', range=[0.0, 0.5]),\n",
    "              sherpa.Ordinal(name='batch_size', range=[16, 32, 64,128,256,512]),\n",
    "              sherpa.Choice(name='activation', range=['relu', 'elu', 'prelu', 'tanh'])]\n",
    "\n",
    "alg =bayesian_optimization.GPyOpt()\n",
    "\n",
    "study = sherpa.Study(parameters=parameters,\n",
    "                     algorithm=alg,dashboard_port=9998,disable_dashboard=False,\n",
    "                     lower_is_better=False)\n",
    "\n",
    "for trial in study:\n",
    "    cnn_MFCC_LSTM(study, trial, X_train, X_val, y_train, y_val, regularization =trial.parameters['regularization'],\n",
    "              drop_likely = trial.parameters['dropout'],\n",
    "              learning_rate = trial.parameters['lr'],\n",
    "              beta1 = trial.parameters['beta1'], beta2 = 0.999,\n",
    "             num_iterations = 150, size_batch=trial.parameters[\"batch_size\"])\n",
    "    study.finalize(trial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_MFCC_LSTM(study, trial, X_train, X_test, y_train, y_test, regularization = 0.1,drop_likely = 0.1,\n",
    "              learning_rate = 0.001, beta1 = 0.9, beta2 = 0.999,\n",
    "             num_iterations = 40, size_batch=X_train.shape[0],activation = 'tanh'):\n",
    "    # Set the target class number\n",
    "    \n",
    "    # VGG 19\n",
    "    \n",
    "    target_class = y_train.shape[1]\n",
    "    # Model \n",
    "    model = Sequential()\n",
    "    \n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size = (3,3), activation=activation, input_shape = (50,13,1)))\n",
    "    model.add(Conv2D(64, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (1,1)))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(Conv2D(128, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (1,1)))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(256, kernel_size = (3,3), activation=activation))\n",
    "    model.add(Conv2D(256, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(Conv2D(256, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(Conv2D(256, kernel_size = (3,3), activation=activation))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (1,1)))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    \n",
    "    model.add(Conv2D(512, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(Conv2D(512, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(Conv2D(512, kernel_size = (3,3), activation=activation, padding='same'))\n",
    "    model.add(Conv2D(512, kernel_size = (3,3), activation=activation))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(Flatten())\n",
    "\n",
    "    \n",
    "    \n",
    "    model.add(Dense(4096, activation=activation))\n",
    "    model.add(Dense(4096, activation=activation))\n",
    "\n",
    "    model.add(Dense(target_class, activation='softmax'))\n",
    "\n",
    "    \n",
    "    \n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_acc', mode='max',  patience=10, min_delta=0)\n",
    "    \n",
    "    opt = keras.optimizers.Adam(lr=learning_rate, beta_1=beta1, beta_2=beta2, epsilon=10e-8)\n",
    "       \n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=opt,metrics=[keras.metrics.categorical_accuracy])\n",
    "    \n",
    "    nn_history = model.fit(X_train, y_train, batch_size=size_batch, epochs=num_iterations, \n",
    "                         validation_data=(X_test, y_test),\n",
    "                          callbacks=[es, study.keras_callback(trial, objective_name='val_categorical_accuracy')])\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_inception(X_train, X_test, y_train, y_test):\n",
    "    #inception architecture\n",
    "    \n",
    "    target_class = y_train.shape[1]\n",
    "    # Model \n",
    "    model = Sequential()\n",
    "    model.add(keras.layers. conv2d_bn(32, 3,3, stridea =(2,2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_inception(X_cache, X_cache, Y_cache, Y_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.argmax(y_train, axis = 1)\n",
    "Y_val = np.argmax(y_val, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.svm.NuSVC(gamma = 'auto' )\n",
    "model.fit(X_train, Y_train)\n",
    "predictions = model.predict(X_val)\n",
    "print(classification_report(Y_val, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.svm.SVC(gamma = 'scale')\n",
    "model.fit(X_train, Y_train)\n",
    "predictions = model.predict(X_val)\n",
    "print(classification_report(Y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.SGDClassifier()\n",
    "model.fit(X_train, Y_train)\n",
    "predictions = model.predict(X_val)\n",
    "print(classification_report(Y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization = 0.1\n",
    "drop_likely = 0.1\n",
    "learning_rate = 0.001\n",
    "beta1 = 0.9 \n",
    "beta2 = 0.999,\n",
    "num_iterations = 40\n",
    "size_batch=256,\n",
    "activation = 'relu'\n",
    "target_class = y_train.shape[1]\n",
    "# Model ALEX NEt\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(96, 22, padding='valid',strides=2,input_shape=(X_train.shape[1], 1))) \n",
    "print(model.input_shape)\n",
    "print(model.output_shape)\n",
    "model.add(MaxPooling1D(pool_size = (3), strides=(2)))\n",
    "print(model.output_shape)\n",
    "model.add(Conv1D(256, 5, padding='same')) \n",
    "\n",
    "print(model.output_shape)\n",
    "model.add(MaxPooling1D(pool_size = (3), strides=(2)))\n",
    "print(model.output_shape)\n",
    "\n",
    "model.add(Conv1D(384, 3, padding='same')) \n",
    "print(model.output_shape)\n",
    "\n",
    "model.add(Conv1D(384, 3, padding='same')) \n",
    "print(model.output_shape)\n",
    "\n",
    "model.add(Conv1D(256, 3, padding='same')) \n",
    "print(model.output_shape)\n",
    "\n",
    "model.add(MaxPooling1D(pool_size = (3), strides=(2)))\n",
    "print(model.output_shape)\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(4096, input_dim=9216,  activation=activation,\n",
    "               kernel_regularizer=regularizers.l2(regularization)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(Dense(4096, activation=activation,\n",
    "               kernel_regularizer=regularizers.l2(regularization)))\n",
    "model.add(Dropout(drop_likely))\n",
    "\n",
    "model.add(Dense(2048, activation=activation,\n",
    "               kernel_regularizer=regularizers.l2(regularization)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(Dropout(drop_likely))\n",
    "\n",
    "model.add(Dense(1024, activation=activation,\n",
    "               kernel_regularizer=regularizers.l2(regularization)))\n",
    "\n",
    "model.add(Dense(512, activation=activation,\n",
    "               kernel_regularizer=regularizers.l2(regularization)))\n",
    "\n",
    "model.add(Dense(target_class, activation='softmax'))\n",
    "\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_acc', mode='max',  patience=10, min_delta=0)\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=learning_rate, beta_1=beta1, beta_2=beta2, epsilon=10e-8)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "nn_history = model.fit(X_train, y_train, batch_size=size_batch, epochs=num_iterations, \n",
    "                     validation_data=(X_val, y_val),callbacks=[es])\n",
    "model, nn_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_cnn(X_train, X_test, y_train, y_test):\n",
    "    # Set the target class number\n",
    "    target_class = y_train.shape[1]\n",
    "    # Model \n",
    "    model = Sequential()\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1], 1))) #1\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Conv1D(256, 8, padding='same')) #2\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(128, 8, padding='same')) #3\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Activation('tanh')) \n",
    "    model.add(Conv1D(128, 8, padding='same')) #4\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Conv1D(128, 8, padding='same')) #5\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Conv1D(128, 8, padding='same')) #6\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(64, 8, padding='same')) #7\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Conv1D(64, 8, padding='same')) #8\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(target_class)) #9\n",
    "    model.add(Activation('softmax'))\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=10e-8, decay=0.0, amsgrad=False)\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=opt,metrics=[keras.metrics.categorical_accuracy])\n",
    "    cnnhistory = model.fit(X_train, y_train, batch_size=128, epochs=40, \n",
    "                         validation_data=(X_test, y_test))\n",
    "    a = model.predict(X_train)\n",
    "    predictions = np.zeros_like(a)\n",
    "    predictions[np.arange(len(a)), a.argmax(1)] = 1\n",
    "    print(predictions)\n",
    "    print(y_train)\n",
    "    print(classification_report(y_train,predictions))\n",
    "    a = model.predict(X_test)\n",
    "    predictions = np.zeros_like(a)\n",
    "    predictions[np.arange(len(a)), a.argmax(1)] = 1\n",
    "    print(classification_report(y_test,predictions))\n",
    "    return model, cnnhistory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
